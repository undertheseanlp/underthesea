"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[3768],{7748(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"rewrite-fasttext-in-rust","metadata":{"permalink":"/underthesea/blog/rewrite-fasttext-in-rust","editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/blog/2026-02-08-rewrite-fasttext-in-rust.md","source":"@site/blog/2026-02-08-rewrite-fasttext-in-rust.md","title":"Rewriting FastText in Rust","description":"In underthesea v9.2.9, we replaced the fasttext Python package (a wrapper around Facebook\'s C++ library) with a pure Rust implementation inside underthesea-core. The result: identical predictions, simpler installation, and one fewer C++ dependency in our stack.","date":"2026-02-08T00:00:00.000Z","tags":[{"inline":true,"label":"rust","permalink":"/underthesea/blog/tags/rust"},{"inline":true,"label":"performance","permalink":"/underthesea/blog/tags/performance"},{"inline":true,"label":"fasttext","permalink":"/underthesea/blog/tags/fasttext"},{"inline":true,"label":"nlp","permalink":"/underthesea/blog/tags/nlp"},{"inline":true,"label":"language-detection","permalink":"/underthesea/blog/tags/language-detection"}],"readingTime":9.74,"hasTruncateMarker":true,"authors":[{"name":"Vu Anh","title":"Creator of Underthesea","url":"https://github.com/rain1024","imageURL":"https://github.com/rain1024.png","key":"rain1024","page":null}],"frontMatter":{"slug":"rewrite-fasttext-in-rust","title":"Rewriting FastText in Rust","authors":["rain1024"],"tags":["rust","performance","fasttext","nlp","language-detection"]},"unlisted":false,"nextItem":{"title":"Rust-Powered Text Classification","permalink":"/underthesea/blog/rust-text-classifier"}},"content":"In underthesea v9.2.9, we replaced the `fasttext` Python package (a wrapper around Facebook\'s C++ library) with a **pure Rust implementation** inside `underthesea-core`. The result: identical predictions, simpler installation, and one fewer C++ dependency in our stack.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Replace FastText?\\n\\nFastText is used in underthesea for **language detection** \u2014 identifying whether input text is Vietnamese, English, French, etc. The original setup worked, but had friction:\\n\\n1. **C++ compilation required** \u2014 `pip install fasttext` needs a C++ compiler, which fails on minimal Docker images and Windows environments\\n2. **Heavy dependency** \u2014 pulls in the entire FastText C++ library (~50MB) just for inference\\n3. **Multiple language boundaries** \u2014 Python \u2192 C++ FFI \u2192 Python, with overhead at each crossing\\n4. **Maintenance burden** \u2014 the `fasttext` package has had compatibility issues with newer Python versions\\n\\nSince we already had `underthesea-core` (our Rust extension via PyO3), adding FastText inference there was a natural fit.\\n\\n## What We Built\\n\\nA **pure Rust FastText inference engine** in 1,149 lines across 6 files:\\n\\n```\\nextensions/underthesea_core/src/fasttext/\\n\u251c\u2500\u2500 mod.rs          # FastTextModel: load + predict (126 lines)\\n\u251c\u2500\u2500 args.rs         # Model hyperparameters deserialization (127 lines)\\n\u251c\u2500\u2500 dictionary.rs   # Vocabulary + n-gram hashing (285 lines)\\n\u251c\u2500\u2500 inference.rs    # Hierarchical softmax + softmax prediction (303 lines)\\n\u251c\u2500\u2500 matrix.rs       # Dense + quantized matrix operations (256 lines)\\n\u2514\u2500\u2500 hash.rs         # FNV-1a hash (52 lines)\\n```\\n\\nThis is inference-only \u2014 we load existing FastText `.bin` and `.ftz` models trained by the original C++ library. No retraining needed.\\n\\n## The Architecture Change\\n\\n### Before\\n\\n```\\nPython\\n  \u2502\\n  \u251c\u2500\u2500 import fasttext           # C++ FFI wrapper\\n  \u2502     \u2514\u2500\u2500 libfasttext.so      # Facebook\'s C++ library\\n  \u2502           \u2514\u2500\u2500 model.bin     # FastText binary model\\n  \u2502\\n  \u2514\u2500\u2500 predictions\\n```\\n\\nThree language boundaries: Python \u2192 C++ wrapper \u2192 C++ library.\\n\\n### After\\n\\n```\\nPython\\n  \u2502\\n  \u251c\u2500\u2500 from underthesea_core import FastText   # PyO3 Rust binding\\n  \u2502     \u2514\u2500\u2500 fasttext::FastTextModel           # Pure Rust\\n  \u2502           \u2514\u2500\u2500 model.bin                   # Same binary format\\n  \u2502\\n  \u2514\u2500\u2500 predictions\\n```\\n\\nOne clean boundary: Python \u2192 Rust via PyO3.\\n\\n### Code Change\\n\\nThe API is almost identical:\\n\\n**Before:**\\n```python\\nimport fasttext\\n\\nmodel = fasttext.load_model(\\"lid.176.ftz\\")\\npredictions = model.predict(\\"Xin ch\xe0o th\u1ebf gi\u1edbi\\")\\n# ((\'__label__vi\',), array([0.98]))\\n```\\n\\n**After:**\\n```python\\nfrom underthesea_core import FastText\\n\\nmodel = FastText.load(\\"lid.176.ftz\\")\\npredictions = model.predict(\\"Xin ch\xe0o th\u1ebf gi\u1edbi\\")\\n# [(\'vi\', 0.98)]\\n```\\n\\nIn underthesea\'s `lang_detect` pipeline, the switch was a one-line import change:\\n\\n```python\\n# Before: import fasttext\\n# After:\\nfrom underthesea_core import FastText\\n\\nlang_detect_model = FastText.load(str(model_path))\\npredictions = lang_detect_model.predict(text, k=1)\\n```\\n\\n## Implementation Deep Dive\\n\\n### 1. Binary Format Parsing\\n\\nFastText models use a custom binary format. We must parse it byte-for-byte to match the C++ implementation:\\n\\n```rust\\npub fn load(path: &str) -> io::Result<Self> {\\n    let file = File::open(path)?;\\n    let mut reader = BufReader::new(file);\\n\\n    let quant = path.ends_with(\\".ftz\\");\\n\\n    check_header(&mut reader)?;          // Magic number + version\\n    let args = Args::load(&mut reader)?;  // Hyperparameters\\n    let dictionary = Dictionary::load(&mut reader, &args)?;  // Vocabulary\\n\\n    let file_quant = read_bool(&mut reader)?;\\n    let input_matrix = FastTextMatrix::load(&mut reader, quant && file_quant)?;\\n\\n    let output_quant = read_bool(&mut reader)?;\\n    let output_matrix = FastTextMatrix::load(&mut reader, quant && output_quant)?;\\n\\n    // Build Huffman tree for hierarchical softmax\\n    let hs_tree = if args.loss == LossName::HierarchicalSoftmax {\\n        let counts = dictionary.get_label_counts();\\n        Some(HSTree::build(&counts))\\n    } else {\\n        None\\n    };\\n\\n    Ok(FastTextModel { args, dictionary, input_matrix, output_matrix, hs_tree })\\n}\\n```\\n\\nThe magic number `0x2F4F16BA` and version `12` must match exactly, or the model is rejected.\\n\\n### 2. The Hash Function: A Subtle Trap\\n\\nFastText uses FNV-1a hashing for word and n-gram lookups. The critical detail: **non-ASCII bytes are sign-extended**, matching C++\'s signed `char` behavior:\\n\\n```rust\\npub fn fasttext_hash(s: &[u8]) -> u32 {\\n    let mut h: u32 = 2166136261;\\n    for &byte in s {\\n        // Sign-extend: cast to i8 first (like C++ int8_t), then to u32\\n        h ^= byte as i8 as u32;\\n        h = h.wrapping_mul(16777619);\\n    }\\n    h\\n}\\n```\\n\\nWithout the `byte as i8 as u32` cast, Vietnamese characters (which use multi-byte UTF-8 with high bytes like `0xC3`, `0xE1`) would hash to different values than the C++ implementation, producing wrong predictions. This was the trickiest part to get right.\\n\\n### 3. Dictionary and N-gram Features\\n\\nThe dictionary handles vocabulary lookup with an open-addressing hash table (30M slots, matching C++):\\n\\n```rust\\npub struct Dictionary {\\n    entries: Vec<Entry>,\\n    word2int: Vec<i32>,    // 30M-slot hash table\\n    nwords: i32,\\n    nlabels: i32,\\n    pruneidx: HashMap<i32, i32>,  // For quantized models\\n    // N-gram parameters\\n    bucket: i32,\\n    minn: i32,\\n    maxn: i32,\\n    word_ngrams: i32,\\n}\\n```\\n\\nFor each input token, we generate three types of features:\\n1. **Word ID** \u2014 direct vocabulary lookup\\n2. **Character n-grams** \u2014 subword features like `<xin`, `xin>`, `<xi`, `in>` for the word \\"xin\\"\\n3. **Word n-grams** \u2014 bigrams/trigrams of word IDs\\n\\nCharacter n-grams are bounded by `<` and `>` markers and hashed into buckets:\\n\\n```rust\\nfn compute_char_ngrams(&self, word: &str, features: &mut Vec<i32>) {\\n    let bounded = format!(\\"<{}>\\", word);\\n    let bytes = bounded.as_bytes();\\n\\n    // Walk character boundaries (not bytes \u2014 Vietnamese is multi-byte UTF-8)\\n    let char_boundaries = compute_utf8_boundaries(bytes);\\n\\n    for n in self.minn..=self.maxn {\\n        for start_char in 0..=(nchars - n) {\\n            let ngram = &bytes[char_boundaries[start_char]..char_boundaries[start_char + n]];\\n            let h = fasttext_hash(ngram);\\n            let bucket_hash = (h as i64 % self.bucket as i64) as i32;\\n            self.push_hash(features, bucket_hash);\\n        }\\n    }\\n}\\n```\\n\\nThis is where FastText\'s power comes from \u2014 even unknown words get meaningful features from their character substrings.\\n\\n### 4. Two Inference Paths\\n\\nFastText models use different loss functions. We implement both:\\n\\n**Hierarchical Softmax** (for models with many labels like language detection with 176 languages):\\n\\nThe key insight: instead of computing scores for all 176 labels (O(n)), we traverse a Huffman tree via DFS, pruning branches that can\'t beat the current top-k (O(k log n)):\\n\\n```rust\\nfn dfs(&self, k: usize, threshold: f32, node: usize, score: f32,\\n       hidden: &[f32], output: &FastTextMatrix,\\n       heap: &mut BinaryHeap<Reverse<(FloatOrd, usize)>>) {\\n    // Prune: if this branch can\'t beat threshold, skip\\n    if score < std_log(threshold) { return; }\\n    // Prune: if heap is full and this can\'t beat worst result, skip\\n    if heap.len() == k && score < heap.peek().unwrap().0.0.0 { return; }\\n\\n    let n = &self.tree[node];\\n    if n.left == -1 && n.right == -1 {\\n        // Leaf node = label\\n        heap.push(Reverse((FloatOrd(score), node)));\\n        if heap.len() > k { heap.pop(); }\\n        return;\\n    }\\n\\n    let f = sigmoid(output.dot_row(hidden, node - self.osz));\\n\\n    // Recurse into both children with accumulated log-probabilities\\n    self.dfs(k, threshold, n.left as usize,  score + std_log(1.0 - f), hidden, output, heap);\\n    self.dfs(k, threshold, n.right as usize, score + std_log(f),       hidden, output, heap);\\n}\\n```\\n\\n**Standard Softmax** (for models with fewer labels):\\n\\nDirect computation \u2014 one dot product per label, then partial sort to find top-k:\\n\\n```rust\\nfn predict_softmax(k: usize, hidden: &[f32], output: &FastTextMatrix, nlabels: usize)\\n    -> Vec<(f32, usize)>\\n{\\n    let mut logits: Vec<f32> = (0..nlabels)\\n        .map(|i| output.dot_row(hidden, i))\\n        .collect();\\n    softmax(&mut logits);\\n\\n    // Partial sort: O(n) instead of O(n log n) full sort\\n    let mut indices: Vec<usize> = (0..nlabels).collect();\\n    indices.select_nth_unstable_by(k - 1, |&a, &b|\\n        logits[b].partial_cmp(&logits[a]).unwrap_or(Ordering::Equal)\\n    );\\n    indices.truncate(k);\\n    // ...\\n}\\n```\\n\\n### 5. Dense and Quantized Matrices\\n\\nFastText models come in two flavors:\\n- **`.bin`** \u2014 dense float32 matrices (large but fast)\\n- **`.ftz`** \u2014 product-quantized matrices (4-10x smaller, slightly slower)\\n\\nWe handle both through a trait:\\n\\n```rust\\npub trait Matrix {\\n    fn rows(&self) -> usize;\\n    fn cols(&self) -> usize;\\n    fn add_row_to(&self, row: usize, output: &mut [f32]);\\n    fn dot_row(&self, vec: &[f32], row: usize) -> f32;\\n}\\n```\\n\\nDense is straightforward \u2014 row-major float array with SIMD-friendly dot products:\\n\\n```rust\\nfn dot_row(&self, vec: &[f32], row: usize) -> f32 {\\n    let start = row * self.cols;\\n    let row_data = &self.data[start..start + self.cols];\\n    vec.iter().zip(row_data.iter()).map(|(&a, &b)| a * b).sum()\\n}\\n```\\n\\nQuantized uses product quantization \u2014 each vector is split into sub-spaces, each quantized to one of 256 centroids (8-bit codes):\\n\\n```rust\\nfn dot_row(&self, vec: &[f32], row: usize) -> f32 {\\n    let norm = self.get_norm(row);\\n    let mut sum = 0.0f32;\\n    let mut dim_offset = 0;\\n    for subq in 0..self.pq.nsubq {\\n        let code = self.codes[row * self.pq.nsubq + subq];\\n        let centroid = self.pq.get_centroid(subq, code);\\n        for (i, &c) in centroid.iter().enumerate() {\\n            sum += vec[dim_offset + i] * c;\\n        }\\n        dim_offset += self.pq.dsub;\\n    }\\n    sum * norm\\n}\\n```\\n\\nThis lets us load Facebook\'s compressed `lid.176.ftz` (917KB) instead of the dense `lid.176.bin` (~130MB).\\n\\n### 6. PyO3 Bindings\\n\\nThe Python interface is minimal \u2014 PyO3 makes it trivial:\\n\\n```rust\\n#[pyclass(name = \\"FastText\\")]\\npub struct PyFastText {\\n    model: fasttext::FastTextModel,\\n}\\n\\n#[pymethods]\\nimpl PyFastText {\\n    #[staticmethod]\\n    pub fn load(path: &str) -> PyResult<Self> {\\n        let model = fasttext::FastTextModel::load(path)\\n            .map_err(pyo3::exceptions::PyIOError::new_err)?;\\n        Ok(Self { model })\\n    }\\n\\n    #[pyo3(signature = (text, k=1))]\\n    pub fn predict(&self, text: &str, k: usize) -> Vec<(String, f32)> {\\n        self.model.predict(text, k)\\n    }\\n\\n    pub fn get_labels(&self) -> Vec<String> {\\n        self.model.get_labels()\\n    }\\n\\n    #[getter]\\n    pub fn dim(&self) -> i32 { self.model.dim() }\\n\\n    #[getter]\\n    pub fn nwords(&self) -> i32 { self.model.nwords() }\\n\\n    #[getter]\\n    pub fn nlabels(&self) -> i32 { self.model.nlabels() }\\n}\\n```\\n\\n## What We Gained\\n\\n### Simpler Installation\\n\\n```\\n# Before: needs C++ compiler, can fail on many platforms\\npip install fasttext  # Often fails: \\"error: command \'gcc\' failed\\"\\n\\n# After: pre-built wheels, just works\\npip install underthesea  # Includes Rust-compiled underthesea-core\\n```\\n\\n### Fewer Dependencies\\n\\n```\\n# Before: underthesea required\\nfasttext>=0.9.2      # C++ FastText wrapper\\npython-crfsuite      # C++ CRFsuite wrapper\\nscikit-learn         # For TfidfVectorizer + LinearSVC\\njoblib               # For pickle serialization\\n\\n# After: just one Rust extension\\nunderthesea-core     # Everything in Rust\\n```\\n\\nAll four dependencies \u2014 `fasttext`, `python-crfsuite`, `scikit-learn` (for classification), and `joblib` \u2014 have been replaced by a single `underthesea-core` package.\\n\\n### Model Compatibility\\n\\nThe Rust implementation reads the **exact same binary format** as Facebook\'s C++ code. Any `.bin` or `.ftz` model trained by the original FastText works unchanged:\\n\\n```python\\nfrom underthesea_core import FastText\\n\\n# Facebook\'s pre-trained language identification model \u2014 just works\\nmodel = FastText.load(\\"lid.176.ftz\\")\\nmodel.predict(\\"H\xe0 N\u1ed9i l\xe0 th\u1ee7 \u0111\xf4 c\u1ee7a Vi\u1ec7t Nam\\", k=3)\\n# [(\'vi\', 0.97), (\'id\', 0.01), (\'ms\', 0.005)]\\n```\\n\\n## The Bigger Picture: Replacing C++ with Rust\\n\\nThis FastText rewrite is part of a broader effort to consolidate underthesea\'s backend into a single Rust extension. Here\'s the full migration timeline:\\n\\n| Version | What Changed | C++ Dependency Removed |\\n|---------|-------------|----------------------|\\n| v9.2.2-v9.2.5 | CRF tagger rewritten in Rust | `python-crfsuite` |\\n| v9.2.9 | Text classifier rewritten in Rust | `scikit-learn` + `joblib` |\\n| v9.2.9 | FastText inference rewritten in Rust | `fasttext` |\\n\\n### Lines of Rust replacing each dependency:\\n\\n| Component | Rust Lines | Replaces |\\n|-----------|-----------|----------|\\n| FastText inference | 1,149 | `fasttext` (C++ FFI) |\\n| TF-IDF + Linear SVM | 2,024 | `scikit-learn` + `joblib` |\\n| CRF tagger + trainer | ~3,000 | `python-crfsuite` (C++ FFI) |\\n| Vietnamese preprocessor | 620 | Custom Python code |\\n| PyO3 bindings | 918 | N/A |\\n| **Total** | **~7,700** | **4 C/C++ dependencies** |\\n\\nUnder 8,000 lines of Rust replaced four separate C/C++ dependencies, unifying everything into a single compiled extension with:\\n- Pre-built wheels for Linux, macOS (Intel + ARM), and Windows\\n- No C/C++ compiler needed for installation\\n- One coherent codebase instead of four upstream projects\\n- Consistent binary serialization (bincode) instead of mixed pickle/joblib/custom formats\\n\\n## Lessons Learned\\n\\n### 1. Byte-level compatibility is non-negotiable\\n\\nThe hash function sign-extension bug (`byte as i8 as u32`) took the longest to find. Without it, predictions for any non-ASCII text (i.e., all Vietnamese) were wrong. When reimplementing binary formats, every byte matters.\\n\\n### 2. Inference-only is the sweet spot\\n\\nWe deliberately chose not to implement FastText training in Rust. Training happens once; inference happens millions of times. By supporting the existing `.bin`/`.ftz` format, we get the benefits of Rust for the hot path while still using Facebook\'s battle-tested training code when needed.\\n\\n### 3. Product quantization support is essential\\n\\nMany production FastText models use `.ftz` (quantized) format for 4-10x size reduction. Skipping quantization support would have made the rewrite impractical for real deployments.\\n\\n### 4. PyO3 makes Rust-Python integration painless\\n\\nThe binding layer is under 50 lines. PyO3 handles type conversion, error propagation, GIL management, and memory cleanup automatically. The cognitive overhead of maintaining a Rust extension is surprisingly low.\\n\\n## Try It Out\\n\\n```bash\\npip install underthesea>=9.2.9\\n```\\n\\n```python\\nfrom underthesea import lang_detect\\n\\n# Uses Rust FastText under the hood\\nlang_detect(\\"Xin ch\xe0o th\u1ebf gi\u1edbi\\")  # \'vi\'\\nlang_detect(\\"Hello world\\")        # \'en\'\\nlang_detect(\\"Bonjour le monde\\")   # \'fr\'\\n```\\n\\nOr use the Rust FastText directly:\\n\\n```python\\nfrom underthesea_core import FastText\\n\\nmodel = FastText.load(\\"your_model.bin\\")\\nmodel.predict(\\"your text here\\", k=5)\\nmodel.get_labels()   # All available labels\\nmodel.dim            # Embedding dimension\\nmodel.nwords         # Vocabulary size\\nmodel.nlabels        # Number of labels\\n```\\n\\n## Links\\n\\n- [PR #953](https://github.com/undertheseanlp/underthesea/pull/953) \u2014 Remove fasttext dependency\\n- [PR #947](https://github.com/undertheseanlp/underthesea/pull/947) \u2014 Add pure Rust FastText inference\\n- [underthesea-core source](https://github.com/undertheseanlp/underthesea/tree/main/extensions/underthesea_core) \u2014 Full Rust implementation\\n- [underthesea-core on PyPI](https://pypi.org/project/underthesea-core/) \u2014 Pre-built wheels"},{"id":"rust-text-classifier","metadata":{"permalink":"/underthesea/blog/rust-text-classifier","editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/blog/2026-02-03-rust-text-classifier.md","source":"@site/blog/2026-02-03-rust-text-classifier.md","title":"Rust-Powered Text Classification","description":"In underthesea v9.2.9, we\'ve completely rewritten the text classification pipeline using our Rust-based TextClassifier. This delivers up to 273x faster inference compared to the previous sklearn-based implementation.","date":"2026-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"rust","permalink":"/underthesea/blog/tags/rust"},{"inline":true,"label":"performance","permalink":"/underthesea/blog/tags/performance"},{"inline":true,"label":"classification","permalink":"/underthesea/blog/tags/classification"},{"inline":true,"label":"nlp","permalink":"/underthesea/blog/tags/nlp"}],"readingTime":3.82,"hasTruncateMarker":true,"authors":[{"name":"Vu Anh","title":"Creator of Underthesea","url":"https://github.com/rain1024","imageURL":"https://github.com/rain1024.png","key":"rain1024","page":null}],"frontMatter":{"slug":"rust-text-classifier","title":"Rust-Powered Text Classification","authors":["rain1024"],"tags":["rust","performance","classification","nlp"]},"unlisted":false,"prevItem":{"title":"Rewriting FastText in Rust","permalink":"/underthesea/blog/rewrite-fasttext-in-rust"},"nextItem":{"title":"Rewriting CRF Model in Rust","permalink":"/underthesea/blog/rewrite-rust-crf-model"}},"content":"In underthesea v9.2.9, we\'ve completely rewritten the text classification pipeline using our Rust-based `TextClassifier`. This delivers up to **273x faster inference** compared to the previous sklearn-based implementation.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nText classification in underthesea supports two domains:\\n- **General**: News categorization (10 categories)\\n- **Bank**: Banking intent classification (14 categories)\\n\\nPreviously, we used scikit-learn\'s `TfidfVectorizer` + `LinearSVC` loaded via joblib. While accurate, this approach had significant overhead.\\n\\n## The Architecture Change\\n\\n### Before (sklearn-based)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                               \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502TfidfVectorizer\u2500\u2500\u2500\u25b6\u2502  LinearSVC   \u2502  \u2502\\n\u2502  \u2502    Text      \u2502    \u2502  (sklearn)   \u2502    \u2502  (sklearn)   \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                   \u2502          \u2502\\n\u2502                      joblib.load()        joblib.load()     \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nLoading two separate pickle files, with Python-based vectorization and inference.\\n\\n### After (Rust-based)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                               \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502         TextClassifier           \u2502  \u2502\\n\u2502  \u2502    Text      \u2502    \u2502  TF-IDF + LinearSVC (Rust)       \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                              \u2502\\n\u2502                      single .bin file                       \u2502\\n\u2502                      underthesea-core                       \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nSingle binary model file, vectorization and inference fused in Rust.\\n\\n## Code Changes\\n\\nThe API remains unchanged:\\n\\n```python\\nfrom underthesea import classify\\n\\n# General classification\\nclassify(\\"Vi\u1ec7t Nam v\xf4 \u0111\u1ecbch AFF Cup\\")\\n# \\"The thao\\"\\n\\n# Bank domain\\nclassify(\\"L\xe3i su\u1ea5t ti\u1ebft ki\u1ec7m bao nhi\xeau?\\", domain=\\"bank\\")\\n# [\'INTEREST_RATE\']\\n```\\n\\nInternally, the implementation is much simpler:\\n\\n**Before:**\\n```python\\nimport joblib\\nfrom underthesea.pipeline.classification import bank\\n\\nvectorizer = joblib.load(\\"vectorizer.pkl\\")\\nclassifier = joblib.load(\\"classifier.pkl\\")\\nfeatures = vectorizer.transform([text])\\nprediction = classifier.predict(features)\\n```\\n\\n**After:**\\n```python\\nfrom underthesea_core import TextClassifier\\n\\nclassifier = TextClassifier.load(\\"model.bin\\")\\nprediction = classifier.predict(text)\\n```\\n\\n## Benchmark Results\\n\\nTested on the same hardware with batch inference:\\n\\n| Domain | sklearn | Rust | Speedup |\\n|--------|---------|------|---------|\\n| General | 1,228 samples/sec | 66,678 samples/sec | **54x** |\\n| Bank | 244 samples/sec | 66,678 samples/sec | **273x** |\\n\\nSingle sample latency: **4ms \u2192 0.465ms**\\n\\n## Why Is It Faster?\\n\\n### 1. Fused Pipeline\\n\\nTF-IDF vectorization and SVM inference run in a single Rust function call, eliminating Python overhead between stages.\\n\\n### 2. Optimized Sparse Operations\\n\\n```rust\\npub fn predict(&self, text: &str) -> String {\\n    // Tokenize and hash features in one pass\\n    let features = self.vectorizer.transform(text);\\n\\n    // Sparse dot product with pre-sorted indices\\n    let scores = self.svm.decision_function(&features);\\n\\n    self.classes[scores.argmax()].clone()\\n}\\n```\\n\\n### 3. Single File Model\\n\\nOne `.bin` file instead of multiple pickle files:\\n- Faster loading\\n- Atomic deployment\\n- Smaller size (JSON-based, not pickle)\\n\\n### 4. No Python GIL Contention\\n\\nRust code releases the GIL during computation, enabling true parallelism.\\n\\n## The Models\\n\\n### sen-classifier-general\\n\\nGeneral Vietnamese news classification model trained on VNTC dataset.\\n\\n**Training Data:** [VNTC](https://github.com/duyvuleo/VNTC) (Vietnamese News Text Classification)\\n- 33,759 training samples\\n- 50,373 test samples\\n- 10 news categories\\n\\n**Categories:**\\n\\n| Label | Vietnamese | English |\\n|-------|------------|---------|\\n| Chinh tri Xa hoi | Ch\xednh tr\u1ecb X\xe3 h\u1ed9i | Politics/Society |\\n| Doi song | \u0110\u1eddi s\u1ed1ng | Lifestyle |\\n| Khoa hoc | Khoa h\u1ecdc | Science |\\n| Kinh doanh | Kinh doanh | Business |\\n| Phap luat | Ph\xe1p lu\u1eadt | Law |\\n| Suc khoe | S\u1ee9c kh\u1ecfe | Health |\\n| The gioi | Th\u1ebf gi\u1edbi | World |\\n| The thao | Th\u1ec3 thao | Sports |\\n| Van hoa | V\u0103n h\xf3a | Culture |\\n| Vi tinh | Vi t\xednh | Technology |\\n\\n**Performance:**\\n- Accuracy: **92.49%**\\n- F1 (weighted): 92.40%\\n- Training time: 37.6s\\n\\n### sen-classifier-bank\\n\\nVietnamese banking intent classification model trained on UTS2017_Bank dataset.\\n\\n**Training Data:** [UTS2017_Bank](https://huggingface.co/datasets/undertheseanlp/UTS2017_Bank)\\n- 1,581 training samples\\n- 396 test samples\\n- 14 banking categories\\n\\n**Categories:**\\n\\n| Label | Description | Samples |\\n|-------|-------------|---------|\\n| CUSTOMER_SUPPORT | Customer support queries | 774 |\\n| TRADEMARK | Brand/trademark mentions | 697 |\\n| LOAN | Loan services | 73 |\\n| INTERNET_BANKING | Internet banking | 69 |\\n| CARD | Card services | 66 |\\n| INTEREST_RATE | Interest rates | 58 |\\n| PROMOTION | Promotions | 56 |\\n| DISCOUNT | Discounts | 40 |\\n| MONEY_TRANSFER | Money transfer | 37 |\\n| OTHER | Other queries | 70 |\\n| PAYMENT | Payment services | 17 |\\n| SAVING | Savings | 12 |\\n| ACCOUNT | Account services | 5 |\\n| SECURITY | Security | 3 |\\n\\n**Performance:**\\n- Accuracy: **75.76%** (+3.29% vs previous sonar_core_1)\\n- F1 (weighted): 72.70%\\n- Training time: 0.13s\\n\\n## Training Pipeline\\n\\nBoth models use a 3-stage TF-IDF + Linear SVM pipeline:\\n\\n```\\nInput Text\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  CountVectorizer                    \u2502\\n\u2502  - max_features: 20,000             \u2502\\n\u2502  - ngram_range: (1, 2)              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  TfidfTransformer                   \u2502\\n\u2502  - use_idf: True                    \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  LinearSVC                          \u2502\\n\u2502  - C: 1.0                           \u2502\\n\u2502  - max_iter: 2000                   \u2502\\n\u2502  - loss: squared_hinge              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\nPredicted Label + Confidence\\n```\\n\\n**Key design decisions:**\\n- **Syllable-level tokenization**: No word segmentation for speed\\n- **Character n-grams (1-2)**: Captures Vietnamese morphology\\n- **20K vocabulary**: Balances accuracy and model size\\n- **Linear SVM**: Fast training, works well with sparse high-dimensional data\\n\\nTraining code: [sen-1/src/scripts/train_vntc.py](https://github.com/undertheseanlp/sen-1)\\n\\n## Label Format Change\\n\\nLabels now use Title case with spaces:\\n\\n| Old | New |\\n|-----|-----|\\n| `the_thao` | `The thao` |\\n| `kinh_doanh` | `Kinh doanh` |\\n| `vi_tinh` | `Vi tinh` |\\n\\nBank domain labels remain uppercase: `INTEREST_RATE`, `MONEY_TRANSFER`, etc.\\n\\n## Simplified Codebase\\n\\nWe consolidated three separate modules into one:\\n\\n**Before:**\\n```\\nclassification/\\n\u251c\u2500\u2500 bank/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 sonar_core_1/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 vntc/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u2514\u2500\u2500 __init__.py\\n```\\n\\n**After:**\\n```\\nclassification/\\n\u251c\u2500\u2500 __init__.py      # Everything here\\n\u2514\u2500\u2500 classification_prompt.py\\n```\\n\\n~190 lines removed, single source of truth for model URLs and loading logic.\\n\\n## Try It Out\\n\\n```bash\\npip install underthesea==9.2.9\\n```\\n\\n```python\\nfrom underthesea import classify\\n\\n# 273x faster!\\nclassify(\\"Th\u1ecb tr\u01b0\u1eddng ch\u1ee9ng kho\xe1n t\u0103ng \u0111i\u1ec3m m\u1ea1nh\\")\\n# \\"Kinh doanh\\"\\n\\nclassify.labels\\n# [\'Chinh tri Xa hoi\', \'Doi song\', \'Khoa hoc\', \'Kinh doanh\', ...]\\n\\nclassify(\\"M\u1edf th\u1ebb t\xedn d\u1ee5ng\\", domain=\\"bank\\")\\n# [\'CARD\']\\n\\nclassify.bank.labels\\n# [\'ACCOUNT\', \'CARD\', \'CUSTOMER_SUPPORT\', ...]\\n```\\n\\n## Links\\n\\n- [PR #935](https://github.com/undertheseanlp/underthesea/pull/935) - Classification pipeline refactor\\n- [Sen-1](https://github.com/undertheseanlp/sen-1) - Training code and technical report\\n- [underthesea-core](https://pypi.org/project/underthesea-core/) - Rust extension on PyPI"},{"id":"rewrite-rust-crf-model","metadata":{"permalink":"/underthesea/blog/rewrite-rust-crf-model","editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/blog/2026-02-02-rewrite-rust-crf-model.md","source":"@site/blog/2026-02-02-rewrite-rust-crf-model.md","title":"Rewriting CRF Model in Rust","description":"In underthesea v9.2.5, we completed the migration from python-crfsuite to our native Rust implementation underthesea-core. This change resulted in a 20% performance improvement across all CRF-based NLP tasks, plus up to 10x faster training through systematic optimization.","date":"2026-02-02T00:00:00.000Z","tags":[{"inline":true,"label":"rust","permalink":"/underthesea/blog/tags/rust"},{"inline":true,"label":"performance","permalink":"/underthesea/blog/tags/performance"},{"inline":true,"label":"crf","permalink":"/underthesea/blog/tags/crf"},{"inline":true,"label":"nlp","permalink":"/underthesea/blog/tags/nlp"}],"readingTime":8.82,"hasTruncateMarker":true,"authors":[{"name":"Vu Anh","title":"Creator of Underthesea","url":"https://github.com/rain1024","imageURL":"https://github.com/rain1024.png","key":"rain1024","page":null}],"frontMatter":{"slug":"rewrite-rust-crf-model","title":"Rewriting CRF Model in Rust","authors":["rain1024"],"tags":["rust","performance","crf","nlp"]},"unlisted":false,"prevItem":{"title":"Rust-Powered Text Classification","permalink":"/underthesea/blog/rust-text-classifier"}},"content":"In underthesea v9.2.5, we completed the migration from `python-crfsuite` to our native Rust implementation `underthesea-core`. This change resulted in a **20% performance improvement** across all CRF-based NLP tasks, plus up to **10x faster training** through systematic optimization.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nUnderthesea uses Conditional Random Fields (CRF) for several core NLP tasks:\\n- Word tokenization\\n- POS tagging\\n- Named Entity Recognition (NER)\\n- Chunking\\n\\nPreviously, we relied on `python-crfsuite`, a Python wrapper for the CRFsuite C++ library. While functional, this introduced overhead from multiple language boundaries.\\n\\n## The Architecture Change\\n\\n### Before (v9.2.1)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                                \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502 CRFFeaturizer\u2502\u2500\u2500\u2500\u25b6\u2502   Python     \u2502  \u2502\\n\u2502  \u2502   Tokens     \u2502    \u2502    (Rust)    \u2502    \u2502    List      \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                                                  \u2502          \u2502\\n\u2502                                                  \u25bc          \u2502\\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502                                          \u2502  pycrfsuite  \u2502  \u2502\\n\u2502                                          \u2502    (C++)     \u2502  \u2502\\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nThe data flow crossed multiple language boundaries:\\n1. Python \u2192 Rust (CRFFeaturizer)\\n2. Rust \u2192 Python (feature list)\\n3. Python \u2192 C++ (pycrfsuite)\\n4. C++ \u2192 Python (tags)\\n\\n### After (v9.2.5)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                                \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502 CRFFeaturizer\u2502\u2500\u2500\u2500\u25b6\u2502  CRFTagger   \u2502  \u2502\\n\u2502  \u2502   Tokens     \u2502    \u2502    (Rust)    \u2502    \u2502    (Rust)    \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                   \u2502          \u2502\\n\u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\\n\u2502                               underthesea-core              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nNow both preprocessing and inference are in Rust within the same module, eliminating the C++ dependency entirely.\\n\\n## Code Changes\\n\\nThe change was minimal from the API perspective:\\n\\n**Before:**\\n```python\\nimport pycrfsuite\\nfrom underthesea_core import CRFFeaturizer\\n\\nclass FastCRFSequenceTagger:\\n    def load(self, base_path):\\n        estimator = pycrfsuite.Tagger()\\n        estimator.open(model_path)\\n        # ...\\n```\\n\\n**After:**\\n```python\\nfrom underthesea_core import CRFFeaturizer, CRFTagger\\n\\nclass FastCRFSequenceTagger:\\n    def load(self, base_path):\\n        estimator = CRFTagger()\\n        estimator.load(model_path)\\n        # ...\\n```\\n\\n## Inference Benchmark Results\\n\\nWe benchmarked both versions on the same hardware (AMD EPYC 7713, Linux, Python 3.12) with 100 iterations:\\n\\n| Function | v9.2.1 (pycrfsuite) | v9.2.5 (Rust) | Improvement |\\n|----------|---------------------|---------------|-------------|\\n| word_tokenize | 1.45ms | 1.18ms | **-19%** |\\n| pos_tag | 3.58ms | 2.93ms | **-18%** |\\n| ner | 9.61ms | 8.49ms | **-12%** |\\n| chunk | 6.19ms | 5.65ms | **-9%** |\\n\\n## Why Is Inference Faster?\\n\\n### 1. Unified Runtime\\n\\nBoth `CRFFeaturizer` and `CRFTagger` are now in the same Rust module. This allows:\\n- Shared memory management\\n- No intermediate Python object creation\\n- Potential for future optimizations (e.g., fusing operations)\\n\\n### 2. Optimized Viterbi Implementation\\n\\nOur Rust implementation uses pre-allocated vectors and cache-friendly memory layouts:\\n\\n```rust\\nfn viterbi(&self, attr_ids: &[Vec<u32>]) -> TaggingResult {\\n    let n = attr_ids.len();\\n    let num_labels = self.model.num_labels;\\n\\n    // Pre-allocated score matrix\\n    let mut score = vec![vec![f64::NEG_INFINITY; num_labels]; n];\\n    let mut back = vec![vec![0u32; num_labels]; n];\\n\\n    // Cache emission scores per position\\n    let emission_t = self.model.emission_scores(&attr_ids[t]);\\n\\n    // Direct memory access in inner loop\\n    for y in 0..num_labels {\\n        for y_prev in 0..num_labels {\\n            let trans = self.model.get_transition(y_prev as u32, y as u32);\\n            // ...\\n        }\\n    }\\n}\\n```\\n\\n### 3. Zero-Copy Where Possible\\n\\nPyO3 bindings allow efficient data transfer between Python and Rust without unnecessary copying.\\n\\n### 4. Removed Dependency\\n\\nRemoving `python-crfsuite` also means:\\n- Simpler installation (no C++ compiler needed)\\n- Smaller package size\\n- Fewer potential compatibility issues\\n\\n## Training Optimizations\\n\\nBeyond inference, we also optimized the CRF trainer in `underthesea-core`. The original Rust trainer was **7.2x slower** than python-crfsuite for word segmentation. Through four key optimizations, we made it competitive \u2014 and even faster for some tasks.\\n\\n### CRF Training Algorithm\\n\\nThe trainer uses Limited-memory BFGS (L-BFGS) optimization with Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) extension for L1 regularization:\\n\\n```\\nminimize: L(w) = -log P(y|x) + \u03bb\u2081\u2016w\u2016\u2081 + \u03bb\u2082\u2016w\u2016\u2082\xb2\\n```\\n\\nWhere `\u03bb\u2081 = 1.0` (L1 coefficient) and `\u03bb\u2082 = 0.001` (L2 coefficient).\\n\\nThe core computation is the forward-backward algorithm for computing:\\n1. **Partition function** Z(x) via forward pass\\n2. **Marginal probabilities** P(y_t | x) via forward-backward\\n3. **Gradient** \u2207L(w) = E_model[f] - E_empirical[f]\\n\\nComplexity per sequence: **O(n \xd7 L\xb2)** where n is the sequence length and L is the number of labels.\\n\\nFollowing CRFsuite\'s approach, we use **scaled probability space** instead of log-space:\\n\\n```rust\\n// Instead of: log_alpha[t][y] = logsumexp(log_alpha[t-1] + log_trans + log_state)\\n// We use:     alpha[t][y] = sum(alpha[t-1] * exp_trans) * exp_state * scale\\n```\\n\\nBenefits:\\n- No log/exp in inner loops\\n- Better numerical stability with scaling factors\\n- Matches CRFsuite\'s performance characteristics\\n\\n### Starting Point\\n\\n| Task | python-crfsuite | underthesea-core (original) | Slowdown |\\n|------|-----------------|----------------------------|----------|\\n| Word Segmentation | 2m 34s | 18m 33s | **7.2x slower** |\\n| POS Tagging | 4m 50s | 7m 21s | **1.5x slower** |\\n\\n### Optimization 1: Flat Data Structure for Feature Lookup\\n\\nThe original used nested vectors (`Vec<Vec<(u32, u32)>>`) for feature lookup \u2014 each inner `Vec` separately heap-allocated, causing cache misses for large feature sets (562k features).\\n\\nWe flattened into contiguous arrays with offset indexing:\\n\\n```rust\\n// Contiguous memory, excellent cache locality\\nattr_offsets: Vec<u32>               // attr_id -> start index\\nattr_features_flat: Vec<(u32, u32)>  // flattened (label_id, feature_id) pairs\\n\\n// Lookup: O(1) with sequential memory access\\nlet start = attr_offsets[attr_id];\\nlet end = attr_offsets[attr_id + 1];\\nfor i in start..end {\\n    let (label_id, feature_id) = attr_features_flat[i];\\n    // Process feature...\\n}\\n```\\n\\n**Result**:\\n\\n| Task | Before | After | Speedup |\\n|------|--------|-------|---------|\\n| Word Segmentation | 18m 33s | 1m 49s | **10.2x** |\\n| POS Tagging | 7m 21s | 5m 9s | **1.4x** |\\n\\nThe larger speedup for word segmentation (562k features vs 37k for POS) confirms the feature lookup was the bottleneck.\\n\\n### Optimization 2: Loop Unrolling for Auto-Vectorization\\n\\nThe forward-backward algorithm has O(n \xd7 L\xb2) inner loops. For POS tagging (16 labels = 256 transitions per timestep), we applied 4-way manual loop unrolling to enable SIMD auto-vectorization:\\n\\n```rust\\n// 4-way unrolled for instruction-level parallelism\\nlet chunks = num_labels / 4;\\nfor i in 0..chunks {\\n    let y = i * 4;\\n    let a0 = alpha[curr + y];\\n    let a1 = alpha[curr + y + 1];\\n    let a2 = alpha[curr + y + 2];\\n    let a3 = alpha[curr + y + 3];\\n\\n    let t0 = trans[trans_base + y];\\n    let t1 = trans[trans_base + y + 1];\\n    let t2 = trans[trans_base + y + 2];\\n    let t3 = trans[trans_base + y + 3];\\n\\n    alpha[curr + y]     = a0 + alpha_prev * t0;\\n    alpha[curr + y + 1] = a1 + alpha_prev * t1;\\n    alpha[curr + y + 2] = a2 + alpha_prev * t2;\\n    alpha[curr + y + 3] = a3 + alpha_prev * t3;\\n}\\n```\\n\\n**Result**: POS tagging (10 iterations) went from 25.7s to 17.58s \u2014 a **1.46x speedup**.\\n\\n### Optimization 3: Unsafe Bounds-Check Elimination\\n\\nWe used `unsafe` with `get_unchecked` for hot paths where indices are provably valid, eliminating Rust\'s bounds checks in tight loops:\\n\\n```rust\\n// Safe but slow: 2 bounds checks per iteration\\nfor y in 0..num_labels {\\n    gradient[feature_id] += state_mexp[base + y];\\n}\\n\\n// Unsafe but fast: 0 bounds checks\\nunsafe {\\n    for y in 0..num_labels {\\n        *gradient.get_unchecked_mut(feature_id) +=\\n            *state_mexp.get_unchecked(base + y);\\n    }\\n}\\n```\\n\\nAll `unsafe` blocks are guarded by loop bounds derived from array lengths, assertions, and algorithm invariants.\\n\\n### Optimization 4: Fused Operations\\n\\nSeparate loops for related operations cause redundant memory traversals. We fused them:\\n\\n```rust\\n// Before: 3 separate loops, 3 memory traversals\\nfor y in 0..L { alpha[y] *= exp_state[y]; }\\nfor y in 0..L { sum += alpha[y]; }\\nfor y in 0..L { alpha[y] *= scale; }\\n\\n// After: 1 fused loop + 1 normalization pass\\nlet mut sum = 0.0;\\nfor y in 0..L {\\n    let val = alpha[y] * exp_state[y];\\n    alpha[y] = val;\\n    sum += val;\\n}\\nlet scale = 1.0 / sum;\\nfor y in 0..L { alpha[y] *= scale; }\\n```\\n\\n### Cumulative Optimization Impact\\n\\n| Optimization | Word Seg Speedup | POS Tag Speedup |\\n|--------------|------------------|-----------------|\\n| Baseline (original) | 1.0x | 1.0x |\\n| + Flat data structure | **10.2x** | 1.4x |\\n| + Loop unrolling | 10.2x | **2.1x** |\\n| + Unsafe bounds elim | ~10.2x | ~2.3x |\\n| **Total** | **10.2x** | **2.3x** |\\n\\n### Training Benchmark Results (200 iterations)\\n\\n| Task | Features | Labels | python-crfsuite | underthesea-core | Result |\\n|------|----------|--------|-----------------|------------------|--------|\\n| Word Segmentation | 562,885 | 2 | 2m 2s | **1m 38s** | **1.24x faster** |\\n| POS Tagging | 626,723 | 16 | 4m 3s | 4m 14s | ~equal (4% slower) |\\n\\n### Accuracy Verification\\n\\n| Task | Metric | python-crfsuite | underthesea-core |\\n|------|--------|-----------------|------------------|\\n| Word Segmentation | Syllable Accuracy | 98.89% | **98.89%** |\\n| Word Segmentation | Word F1 | 98.00% | **98.00%** |\\n| POS Tagging | Accuracy | 95.98% | **95.97%** |\\n\\n**Accuracy is identical** \u2014 optimizations only affected performance, not correctness.\\n\\n## What Didn\'t Work\\n\\nWe evaluated several additional optimizations that did not provide significant improvements:\\n\\n- **Explicit SIMD Intrinsics (AVX2)**: The inner loops process only 2-16 labels, too small for explicit SIMD to outperform the compiler\'s auto-vectorization with loop unrolling.\\n- **Parallel Forward-Backward (Rayon)**: Thread-local gradient accumulation overhead from buffer allocation per sequence and gradient merging negated the parallelism benefits. Sequential processing with buffer reuse remains faster.\\n- **Memory Pool for Temporary Buffers**: Already implemented \u2014 the current implementation reuses buffers across sequences within each L-BFGS evaluation. Further pooling across evaluations showed minimal improvement.\\n- **Compressed Sparse Features**: The flat data structure with offset indexing already provides efficient sparse feature access \u2014 additional compression just adds decode overhead.\\n\\n## Key Insight: Different Tasks, Different Bottlenecks\\n\\n| Feature Set Size | Bottleneck | Best Optimization |\\n|------------------|------------|-------------------|\\n| Large (500k+) | Feature lookup (cache misses) | Flat data structure |\\n| Small (&lt;50k) | Forward-backward O(L\xb2) | Loop unrolling |\\n\\n### Why python-crfsuite Was Initially Faster\\n\\nCRFsuite (C implementation) already had:\\n1. **Hand-optimized sparse feature storage** \u2014 similar to our flat structure\\n2. **SIMD-vectorized matrix operations** \u2014 AVX/SSE intrinsics\\n3. **Cache-optimized memory layout** \u2014 column-major for transitions\\n4. **Decades of optimization** \u2014 mature codebase\\n\\nOur flat data structure and loop unrolling effectively replicated these advantages in Rust.\\n\\n### Lessons Learned\\n\\n1. **Profile first** \u2014 the bottleneck was different for each task\\n2. **Data structure matters** \u2014 flat arrays beat nested vectors by 10x\\n3. **Cache locality is critical** \u2014 sequential memory access enables hardware prefetching\\n4. **Unsafe Rust is justified** \u2014 when correctness is provable and performance is critical\\n5. **Incremental migration reduces risk** \u2014 migrating one task at a time allowed validation at each step\\n\\n## Migration Path\\n\\nThe migration was done incrementally across 4 releases:\\n\\n| Version | Changes |\\n|---------|---------|\\n| v9.2.2 | word_tokenize migrated |\\n| v9.2.3 | pos_tag, ner, chunking migrated |\\n| v9.2.4 | CRFTrainer migrated, removed unused files |\\n| v9.2.5 | Removed python-crfsuite dependency |\\n\\n## Model Compatibility\\n\\nThe Rust implementation can load existing `.crfsuite` model files trained by python-crfsuite. No retraining is required.\\n\\n```python\\n# Works with both old and new models\\nfrom underthesea import word_tokenize\\nword_tokenize(\\"H\xe0 N\u1ed9i l\xe0 th\u1ee7 \u0111\xf4 c\u1ee7a Vi\u1ec7t Nam\\")\\n# [\'H\xe0 N\u1ed9i\', \'l\xe0\', \'th\u1ee7 \u0111\xf4\', \'c\u1ee7a\', \'Vi\u1ec7t Nam\']\\n```\\n\\n## Conclusion\\n\\nBy rewriting our CRF implementation in Rust and unifying the pipeline, we achieved:\\n\\n- **12-19% faster inference** across all CRF-based tasks\\n- **1.24x faster training** for word segmentation (10x from original Rust implementation)\\n- **Identical accuracy** \u2014 no degradation from the migration\\n- **Simpler dependency tree** (no python-crfsuite / C++ compiler needed)\\n- **Better maintainability** with a single Rust codebase\\n\\nThe full implementation is available in [underthesea-core](https://github.com/undertheseanlp/underthesea/tree/main/extensions/underthesea_core).\\n\\n## Try It Out\\n\\n```bash\\npip install underthesea==9.2.5\\n```\\n\\n```python\\nfrom underthesea import word_tokenize, pos_tag, ner, chunk\\n\\nword_tokenize(\\"Vi\u1ec7t Nam\\")  # 20% faster!\\n```\\n\\n## Appendix\\n\\n### Hyperparameters\\n\\n| Parameter | Value | Description |\\n|-----------|-------|-------------|\\n| c1 (L1) | 1.0 | L1 regularization coefficient |\\n| c2 (L2) | 0.001 | L2 regularization coefficient |\\n| max_iterations | 200 | Maximum L-BFGS iterations |\\n| linesearch | Backtracking | Line search algorithm for OWL-QN |\\n| max_step_size | 1e20 | Allow large steps (critical for convergence) |\\n\\n### Hardware\\n\\n| Component | Specification |\\n|-----------|---------------|\\n| CPU | AMD EPYC 7713 64-Core Processor |\\n| Platform | Linux |\\n| Rust | 1.75+ (release mode with LTO) |\\n| Python | 3.12 |\\n\\n### Code References\\n\\n| File | Description |\\n|------|-------------|\\n| `underthesea_core/src/crf/trainer.rs` | Main CRF trainer implementation |\\n| `underthesea_core/src/crf/model.rs` | CRF model structure |\\n| `tre-1/scripts/train.py` | POS tagger training script |\\n| `tre-1/scripts/train_word_segmentation.py` | Word segmentation training script |\\n\\n### Detailed Benchmark Results (2026-01-31)\\n\\n**10-Iteration Tests:**\\n\\n| Task | Trainer | Training Time | Accuracy |\\n|------|---------|---------------|----------|\\n| POS Tagging | python-crfsuite | 12.96s | 78.37% |\\n| POS Tagging | underthesea-core | 18.51s | 75.42% |\\n| Word Segmentation | python-crfsuite | 6.78s | 81.44% F1 |\\n| Word Segmentation | underthesea-core | 11.99s | 82.81% F1 |\\n\\n**200-Iteration Tests:**\\n\\n| Task | Trainer | Training Time | Accuracy |\\n|------|---------|---------------|----------|\\n| POS Tagging | python-crfsuite | 243.22s | 95.98% |\\n| POS Tagging | underthesea-core | 254.07s | 95.97% |\\n| Word Segmentation | python-crfsuite | 121.69s | 98.89% / 98.00% F1 |\\n| Word Segmentation | underthesea-core | 98.34s | 98.89% / 98.00% F1 |"}]}}')}}]);