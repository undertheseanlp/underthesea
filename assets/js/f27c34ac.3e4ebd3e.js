"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[9380],{5165(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"api/word-tokenize","title":"word_tokenize","description":"Segment Vietnamese text into words.","source":"@site/versioned_docs/version-9.2.11/api/word-tokenize.md","sourceDirName":"api","slug":"/api/word-tokenize","permalink":"/underthesea/docs/api/word-tokenize","draft":false,"unlisted":false,"editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/versioned_docs/version-9.2.11/api/word-tokenize.md","tags":[],"version":"9.2.11","frontMatter":{},"sidebar":"apiReferenceSidebar","previous":{"title":"text_normalize","permalink":"/underthesea/docs/api/text-normalize"},"next":{"title":"pos_tag","permalink":"/underthesea/docs/api/pos-tag"}}');var r=t(4848),i=t(8453);const d={},o="word_tokenize",l={},c=[{value:"Usage",id:"usage",level:2},{value:"Function Signature",id:"function-signature",level:2},{value:"Parameters",id:"parameters",level:2},{value:"Returns",id:"returns",level:2},{value:"Examples",id:"examples",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Text Format",id:"text-format",level:3},{value:"Fixed Words",id:"fixed-words",level:3},{value:"Processing Multiple Sentences",id:"processing-multiple-sentences",level:3},{value:"Notes",id:"notes",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"word_tokenize",children:"word_tokenize"})}),"\n",(0,r.jsx)(n.p,{children:"Segment Vietnamese text into words."}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from underthesea import word_tokenize\n\ntext = "Ch\xe0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\xf2"\nwords = word_tokenize(text)\nprint(words)\n# ["Ch\xe0ng trai", "9X", "Qu\u1ea3ng Tr\u1ecb", "kh\u1edfi nghi\u1ec7p", "t\u1eeb", "n\u1ea5m", "s\xf2"]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"function-signature",children:"Function Signature"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def word_tokenize(\n    sentence: str,\n    format: str = None,\n    use_token_normalize: bool = True,\n    fixed_words: list = None\n) -> list[str] | str\n"})}),"\n",(0,r.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sentence"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:"The input text to tokenize"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"format"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"None"})}),(0,r.jsxs)(n.td,{children:["Output format: ",(0,r.jsx)(n.code,{children:"None"})," for list, ",(0,r.jsx)(n.code,{children:'"text"'})," for string"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"use_token_normalize"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"bool"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"True"})}),(0,r.jsx)(n.td,{children:"Whether to normalize tokens"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"fixed_words"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"list"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"None"})}),(0,r.jsx)(n.td,{children:"List of words that should not be split"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"list[str]"})}),(0,r.jsxs)(n.td,{children:["List of words (when ",(0,r.jsx)(n.code,{children:"format=None"}),")"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsxs)(n.td,{children:["Space-separated string with underscores (when ",(0,r.jsx)(n.code,{children:'format="text"'}),")"]})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from underthesea import word_tokenize\n\ntext = "Ch\xe0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\xf2"\nwords = word_tokenize(text)\nprint(words)\n# ["Ch\xe0ng trai", "9X", "Qu\u1ea3ng Tr\u1ecb", "kh\u1edfi nghi\u1ec7p", "t\u1eeb", "n\u1ea5m", "s\xf2"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"text-format",children:"Text Format"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'text = "Ch\xe0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\xf2"\nresult = word_tokenize(text, format="text")\nprint(result)\n# "Ch\xe0ng_trai 9X Qu\u1ea3ng_Tr\u1ecb kh\u1edfi_nghi\u1ec7p t\u1eeb n\u1ea5m s\xf2"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"fixed-words",children:"Fixed Words"}),"\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.code,{children:"fixed_words"})," to ensure certain words are kept together:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'text = "Vi\u1ec7n Nghi\xean C\u1ee9u chi\u1ebfn l\u01b0\u1ee3c qu\u1ed1c gia v\u1ec1 h\u1ecdc m\xe1y"\nfixed_words = ["Vi\u1ec7n Nghi\xean C\u1ee9u", "h\u1ecdc m\xe1y"]\nresult = word_tokenize(text, fixed_words=fixed_words, format="text")\nprint(result)\n# "Vi\u1ec7n_Nghi\xean_C\u1ee9u chi\u1ebfn_l\u01b0\u1ee3c qu\u1ed1c_gia v\u1ec1 h\u1ecdc_m\xe1y"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"processing-multiple-sentences",children:"Processing Multiple Sentences"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"sentences = [\n    \"T\xf4i y\xeau Vi\u1ec7t Nam\",\n    \"H\xe0 N\u1ed9i l\xe0 th\u1ee7 \u0111\xf4 c\u1ee7a Vi\u1ec7t Nam\"\n]\n\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    print(words)\n# ['T\xf4i', 'y\xeau', 'Vi\u1ec7t Nam']\n# ['H\xe0 N\u1ed9i', 'l\xe0', 'th\u1ee7 \u0111\xf4', 'c\u1ee7a', 'Vi\u1ec7t Nam']\n"})}),"\n",(0,r.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Vietnamese word segmentation is challenging because spaces don't always indicate word boundaries"}),"\n",(0,r.jsx)(n.li,{children:"The function uses a CRF model trained on Vietnamese text"}),"\n",(0,r.jsx)(n.li,{children:'Multi-syllable words are joined (e.g., "Vi\u1ec7t Nam" is one word, not two)'}),"\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.code,{children:"fixed_words"})," parameter for domain-specific terminology"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453(e,n,t){t.d(n,{R:()=>d,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function d(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);