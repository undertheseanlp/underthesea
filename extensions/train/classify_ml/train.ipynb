{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Install required packages\n!pip install scikit-learn joblib",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VNTC Text Classification Training\n",
    "\n",
    "This notebook trains multiple text classification models on the VNTC dataset using TF-IDF features and different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.expanduser(\"~/.underthesea/VNTC\")\n",
    "output_folder = os.path.expanduser(\"~/.underthesea/models\")\n",
    "train_file = os.path.join(dataset_folder, \"train.txt\")\n",
    "test_file = os.path.join(dataset_folder, \"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse training data\n",
    "print(\"Reading train.txt...\")\n",
    "X_train_raw = []\n",
    "y_train = []\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            label = parts[0].replace('__label__', '')\n",
    "            text = parts[1]\n",
    "            y_train.append(label)\n",
    "            X_train_raw.append(text)\n",
    "\n",
    "print(f\"Train samples: {len(X_train_raw)}\")\n",
    "print(f\"Unique labels: {len(set(y_train))}\")\n",
    "print(f\"Labels: {sorted(set(y_train))[:10]}...\")  # Show first 10 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse test data\n",
    "print(\"Reading test.txt...\")\n",
    "X_test_raw = []\n",
    "y_test = []\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            label = parts[0].replace('__label__', '')\n",
    "            text = parts[1]\n",
    "            y_test.append(label)\n",
    "            X_test_raw.append(text)\n",
    "\n",
    "print(f\"Test samples: {len(X_test_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options for experiments\n",
    "model_version = \"UTS-C1\"\n",
    "max_features_options = [10000, 20000, 30000]\n",
    "ngram_options = [(1, 2), (1, 3)]\n",
    "classifier_options = [\n",
    "    ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('SVC', SVC(kernel='linear', random_state=42, probability=True))\n",
    "]\n",
    "\n",
    "# Store results for all experiments\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with different configurations\n",
    "for max_features in max_features_options:\n",
    "    for ngram_range in ngram_options:\n",
    "        for clf_name, classifier in classifier_options:\n",
    "            config_name = f\"{model_version}_feat{max_features//1000}k_ngram{ngram_range[0]}-{ngram_range[1]}_{clf_name}\"\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"Training: {config_name}\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            # Create TF-IDF pipeline\n",
    "            print(f\"Creating pipeline with max_features={max_features}, ngram_range={ngram_range}, classifier={clf_name}\")\n",
    "            text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features=max_features, ngram_range=ngram_range)),\n",
    "                ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                ('clf', classifier)\n",
    "            ])\n",
    "\n",
    "            # Train the model\n",
    "            print(\"Training model...\")\n",
    "            start_time = time.time()\n",
    "            text_clf.fit(X_train_raw, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "            # Evaluate on training set\n",
    "            print(\"Evaluating on training set...\")\n",
    "            train_predictions = text_clf.predict(X_train_raw)\n",
    "            train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "            print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            # Evaluate on test set\n",
    "            print(\"Evaluating on test set...\")\n",
    "            start_time = time.time()\n",
    "            test_predictions = text_clf.predict(X_test_raw)\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "            prediction_time = time.time() - start_time\n",
    "            print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"Prediction time: {prediction_time:.2f} seconds\")\n",
    "\n",
    "            # Store results\n",
    "            result = {\n",
    "                'model_version': model_version,\n",
    "                'config_name': config_name,\n",
    "                'max_features': max_features,\n",
    "                'ngram_range': ngram_range,\n",
    "                'classifier': clf_name,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'train_time': train_time,\n",
    "                'prediction_time': prediction_time\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Show classification report for first 5 classes\n",
    "            print(\"\\nClassification Report (first 5 classes):\")\n",
    "            unique_labels = sorted(set(y_train))[:5]\n",
    "            report = classification_report(y_test, test_predictions, labels=unique_labels, zero_division=0, output_dict=True)\n",
    "\n",
    "            # Save the model with configuration name\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            model_filename = os.path.join(output_folder, f'{config_name}.pkl')\n",
    "            joblib.dump(text_clf, model_filename)\n",
    "            print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of all experiments\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Config':<50} {'Train Acc':<10} {'Test Acc':<10} {'Train Time':<12} {'Pred Time':<10}\")\n",
    "print(\"-\"*80)\n",
    "for result in sorted(results, key=lambda x: x['test_accuracy'], reverse=True):\n",
    "    print(f\"{result['config_name']:<50} {result['train_accuracy']:.4f}     {result['test_accuracy']:.4f}      {result['train_time']:>8.2f}s    {result['prediction_time']:>6.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as the main model\n",
    "best_result = max(results, key=lambda x: x['test_accuracy'])\n",
    "print(f\"\\nBest configuration: {best_result['config_name']} with test accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "\n",
    "# Load and save best model as main model\n",
    "best_model_path = os.path.join(output_folder, f\"{best_result['config_name']}.pkl\")\n",
    "best_model = joblib.load(best_model_path)\n",
    "main_model_path = os.path.join(output_folder, 'vntc_classifier.pkl')\n",
    "joblib.dump(best_model, main_model_path)\n",
    "print(f\"Best model saved as main model to {main_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_file = os.path.join(output_folder, f'{model_version}_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Save label mapping for reference\n",
    "label_mapping_filename = os.path.join(output_folder, 'label_mapping.txt')\n",
    "with open(label_mapping_filename, 'w', encoding='utf-8') as f:\n",
    "    for label in sorted(set(y_train)):\n",
    "        f.write(f\"{label}\\n\")\n",
    "print(f\"Label mapping saved to {label_mapping_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}