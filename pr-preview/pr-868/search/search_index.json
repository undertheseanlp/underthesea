{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Underthesea","text":""},{"location":"#open-source-vietnamese-natural-language-processing-toolkit","title":"Open-source Vietnamese Natural Language Processing Toolkit","text":"<p>Underthesea is a suite of open source Python modules, datasets, and tutorials supporting research and development in Vietnamese Natural Language Processing.</p> <p>We provide an extremely easy API to quickly apply pretrained NLP models to your Vietnamese text.</p>"},{"location":"#features","title":"Features","text":"Feature Description Model Type Sentence Segmentation Breaking text into individual sentences CRF Text Normalization Standardizing textual data representation Rule-based Word Segmentation Dividing text into individual words CRF POS Tagging Labeling words with their part-of-speech CRF Chunking Grouping words into meaningful phrases CRF Dependency Parsing Analyzing grammatical structure Deep Learning Named Entity Recognition Identifying named entities CRF / Deep Learning Text Classification Categorizing text into predefined groups CRF / Prompt Sentiment Analysis Determining text's emotional tone CRF Translation Translating Vietnamese to English Deep Learning Language Detection Identifying the language of text FastText Text-to-Speech Converting text into spoken audio Deep Learning"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from underthesea import word_tokenize, pos_tag, ner\n\n# Word Segmentation\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nprint(word_tokenize(text))\n# ['Ch\u00e0ng trai', '9X', 'Qu\u1ea3ng Tr\u1ecb', 'kh\u1edfi nghi\u1ec7p', 't\u1eeb', 'n\u1ea5m', 's\u00f2']\n\n# POS Tagging\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\nprint(pos_tag(text))\n# [('Ch\u1ee3', 'N'), ('th\u1ecbt', 'N'), ('ch\u00f3', 'N'), ('n\u1ed5i ti\u1ebfng', 'A'), ...]\n\n# Named Entity Recognition\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nprint(ner(text))\n# [('Vi\u1ec7t Nam', 'Np', 'B-NP', 'B-LOC'), ('M\u1ef9', 'Np', 'B-NP', 'B-LOC'), ...]\n</code></pre>"},{"location":"#whats-new","title":"What's New","text":"<p>New in v9.0.0</p> <p>Vietnamese-English translation is here! Use <code>translate(\"Xin ch\u00e0o Vi\u1ec7t Nam\")</code> to translate Vietnamese text to English.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>:material-download:{ .lg .middle } Installation</p> <p>Install Underthesea with pip in seconds</p> <p>:octicons-arrow-right-24: Installation Guide</p> </li> <li> <p>:material-rocket-launch:{ .lg .middle } Quick Start</p> <p>Get started with your first NLP task</p> <p>:octicons-arrow-right-24: Quick Start</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } Tutorials</p> <p>Learn with step-by-step tutorials</p> <p>:octicons-arrow-right-24: Tutorials</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Facebook Page</li> <li>YouTube Channel</li> <li>Google Colab Notebook</li> </ul>"},{"location":"#support","title":"Support","text":"<p>If you found this project helpful, please consider supporting us.</p>"},{"location":"history/","title":"Changelog","text":"<p>All notable changes to Underthesea are documented here.</p>"},{"location":"history/#912-2025-01-24","title":"9.1.2 (2025-01-24)","text":""},{"location":"history/#new-features","title":"New Features","text":"<ul> <li>Add <code>labels</code> property to <code>classify</code> and <code>sentiment</code> functions (#865)</li> <li><code>classify.labels</code> - Get all available classification labels</li> <li><code>classify.bank.labels</code> - Get bank domain classification labels</li> <li><code>sentiment.labels</code> - Get all available sentiment labels</li> <li><code>sentiment.bank.labels</code> - Get bank domain sentiment labels</li> </ul>"},{"location":"history/#900-2025-01-xx","title":"9.0.0 (2025-01-XX)","text":""},{"location":"history/#new-features_1","title":"New Features","text":"<ul> <li>Vietnamese-English translation module (#856)</li> <li><code>translate()</code> function for VI\u2192EN and EN\u2192VI translation</li> <li>Requires <code>[deep]</code> installation</li> </ul>"},{"location":"history/#improvements","title":"Improvements","text":"<ul> <li>Migrated from Flake8/Pylint to Ruff for linting (#857)</li> </ul>"},{"location":"history/#830-2025-09-28","title":"8.3.0 (2025-09-28)","text":"<ul> <li>Remove flake8 as runtime dependency by @BLKSerene in #818</li> <li>Train text classification model for dataset VNTC2017_BANK by @rain1024 in #819</li> <li>Build wheels for macOS x86-64 by @BLKSerene in #820</li> <li>Add datasets UTS2017_Bank by @rain1024 in #822</li> <li>Add bank model by @rain1024 in #824</li> </ul>"},{"location":"history/#820-2025-09-21","title":"8.2.0 (2025-09-21)","text":"<ul> <li>Update project structure, create extensions/lab folder by @rain1024 in #812</li> <li>Create Sonar Core 1 - System Card by @rain1024 in #813</li> <li>Update output format of model sonar_core_1 by @rain1024 in #815</li> </ul>"},{"location":"history/#810-2025-09-21","title":"8.1.0 (2025-09-21)","text":"<ul> <li>Fix missing .pkl files by @rain1024 in #809</li> </ul>"},{"location":"history/#801-2025-09-21","title":"8.0.1 (2025-09-21)","text":"<ul> <li>Security updates for dependencies</li> <li>Update publish distribution to Pypi workflow by @rain1024 in #805</li> <li>Fix missing .txt files by @rain1024 in #806</li> </ul>"},{"location":"history/#800-2025-09-20","title":"8.0.0 (2025-09-20)","text":""},{"location":"history/#new-features_2","title":"New Features","text":"<ul> <li>Underthesea Languages v2 by @rain1024 in #748</li> <li>Interactive Page for Most Frequently Used Vietnamese Words by @rain1024 in #756</li> </ul>"},{"location":"history/#improvements_1","title":"Improvements","text":"<ul> <li>Support Python 3.12, 3.13 by @rain1024 in #777</li> <li>Update PyO3 API usage by @trunghieu0207 in #768</li> <li>Update project structure by @rain1024 in #790</li> </ul>"},{"location":"history/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix wrong global var in sent_tokenize by @Darejkal in #764</li> <li>Minor fix (Logo in Readme.rst) by @ichxorya in #761</li> </ul>"},{"location":"history/#684-2024-06-22","title":"6.8.4 (2024-06-22)","text":"<ul> <li>Optimize imports by @rain1024 in #741</li> <li>Remove issue-manager workflow by @rain1024 in #726</li> <li>Add lang_detect module by @rain1024 in #733</li> </ul>"},{"location":"history/#680-2023-09-23","title":"6.8.0 (2023-09-23)","text":"<ul> <li>Release Source Distribution for underthesea_core by @rain1024 in #708</li> <li>Create docker image for underthesea by @rain1024 in #711</li> <li>Code refactoring by @tosemml in #713</li> <li>Fix permission errors on removing downloaded models by @BLKSerene in #715</li> </ul>"},{"location":"history/#670-2023-07-28","title":"6.7.0 (2023-07-28)","text":"<ul> <li>Zero shot classification with OpenAI API by @rain1024 in #700</li> </ul>"},{"location":"history/#660-2023-07-27","title":"6.6.0 (2023-07-27)","text":"<ul> <li>Fix bug word_tokenize by @rain1024 in #697</li> </ul>"},{"location":"history/#650-2023-07-14","title":"6.5.0 (2023-07-14)","text":"<ul> <li>Fix text_normalizer token rules</li> </ul>"},{"location":"history/#640-2023-07-14","title":"6.4.0 (2023-07-14)","text":"<ul> <li>Fix fixed_words regex</li> </ul>"},{"location":"history/#630-2023-06-28","title":"6.3.0 (2023-06-28)","text":"<ul> <li>Support MacOS ARM</li> </ul>"},{"location":"history/#620-2023-03-04","title":"6.2.0 (2023-03-04)","text":"<ul> <li>Add Text to Speech API by @rain1024 in #668</li> <li>Provide training script for word segmentation, pos tagging, and NER by @rain1024 in #666</li> <li>Create UTS_Dictionary v1.0 datasets by @rain1024 in #663</li> </ul>"},{"location":"history/#614-2023-02-26","title":"6.1.4 (2023-02-26)","text":"<ul> <li>Support underthesea_core with Python 3.11 by @rain1024 in #659</li> </ul>"},{"location":"history/#612-2023-02-15","title":"6.1.2 (2023-02-15)","text":"<ul> <li>Add option fixed_words to tokenize and word_tokenize API by @rain1024 in #649</li> </ul>"},{"location":"history/#600-2023-01-01","title":"6.0.0 (2023-01-01)","text":"<ul> <li>Happy New Year 2023! Version bump!</li> </ul>"},{"location":"history/#141-2022-12-17","title":"1.4.1 (2022-12-17)","text":"<ul> <li>Create underthesea app</li> <li>Add viet2ipa module</li> <li>Training NER model with VLSP2016 dataset using BERT</li> <li>Remove unidecode as a dependency</li> </ul>"},{"location":"history/#135-2022-10-31","title":"1.3.5 (2022-10-31)","text":"<ul> <li>Add Text Normalization module</li> <li>Release underthesea_core version 0.0.5a2</li> <li>Support GLIBC_2.17</li> <li>Update resources path</li> <li>Fix function word_tokenize</li> </ul>"},{"location":"history/#134-2022-01-08","title":"1.3.4 (2022-01-08)","text":"<ul> <li>Demo chatbot with rasa</li> <li>Lite version of underthesea</li> <li>Increase word_tokenize speed 1.5 times</li> <li>Add build for Windows</li> </ul>"},{"location":"history/#133-2021-09-02","title":"1.3.3 (2021-09-02)","text":"<ul> <li>Update torch and transformer dependency</li> </ul>"},{"location":"history/#132-2021-08-04","title":"1.3.2 (2021-08-04)","text":"<ul> <li>Publish two ABSA open datasets</li> <li>Migrate from travis-ci to github actions</li> <li>Update ParserTrainer</li> <li>Add pipeline folder</li> </ul>"},{"location":"history/#131-2021-01-11","title":"1.3.1 (2021-01-11)","text":"<ul> <li>Compatible with newer version of scikit-learn</li> <li>Retrain classification and sentiment models</li> <li>Add ClassifierTrainer</li> <li>Add 3 new datasets</li> </ul>"},{"location":"history/#130-2020-12-11","title":"1.3.0 (2020-12-11)","text":"<ul> <li>Remove languageflow dependency</li> <li>Remove tabulate dependency</li> <li>Dependency Parsing</li> </ul>"},{"location":"history/#100-2017-03-01","title":"1.0.0 (2017-03-01)","text":"<ul> <li>First release on PyPI</li> <li>First release on ReadTheDocs</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher (supports 3.9, 3.10, 3.11, 3.12, 3.13)</li> <li>pip package manager</li> </ul>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>To install Underthesea, simply run:</p> <pre><code>pip install underthesea\n</code></pre> <p>This installs the core package with basic NLP features:</p> <ul> <li>Sentence segmentation</li> <li>Text normalization</li> <li>Word segmentation</li> <li>POS tagging</li> <li>Chunking</li> <li>Named entity recognition (CRF model)</li> <li>Text classification</li> <li>Sentiment analysis</li> </ul>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Underthesea provides optional dependencies for advanced features:</p>"},{"location":"installation/#deep-learning-models","title":"Deep Learning Models","text":"<p>For deep learning-based features (dependency parsing, deep NER, translation):</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <p>This includes PyTorch and Transformers libraries.</p>"},{"location":"installation/#text-to-speech","title":"Text-to-Speech","text":"<p>For Vietnamese text-to-speech synthesis:</p> <pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre> <p>This includes JAX, Haiku, and audio processing libraries.</p>"},{"location":"installation/#prompt-based-models","title":"Prompt-based Models","text":"<p>For OpenAI-powered text classification:</p> <pre><code>pip install \"underthesea[prompt]\"\n</code></pre> <p>Note</p> <p>You need to set the <code>OPENAI_API_KEY</code> environment variable to use prompt-based models.</p>"},{"location":"installation/#language-detection","title":"Language Detection","text":"<p>For language detection using FastText:</p> <pre><code>pip install \"underthesea[langdetect]\"\n</code></pre>"},{"location":"installation/#development-tools","title":"Development Tools","text":"<p>For development and testing:</p> <pre><code>pip install \"underthesea[dev]\"\n</code></pre>"},{"location":"installation/#install-all-optional-dependencies","title":"Install All Optional Dependencies","text":"<pre><code>pip install \"underthesea[deep,voice,prompt,langdetect]\"\n</code></pre>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":""},{"location":"installation/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/undertheseanlp/underthesea.git\ncd underthesea\n</code></pre>"},{"location":"installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code># Create virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install in development mode\nuv pip install -e \".\"\n\n# With optional dependencies\nuv pip install -e \".[deep]\"\n</code></pre>"},{"location":"installation/#using-pip","title":"Using pip","text":"<pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install in development mode\npip install -e \".\"\n</code></pre>"},{"location":"installation/#macos-arm64-apple-silicon","title":"macOS ARM64 (Apple Silicon)","text":"<p>For macOS with Apple Silicon (M1/M2/M3), you need to build the Rust extension:</p> <pre><code>cd extensions/underthesea_core\nuv pip install maturin\nmaturin develop\ncd ../..\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>&gt;&gt;&gt; import underthesea\n&gt;&gt;&gt; underthesea.__version__\n'9.0.0'\n\n&gt;&gt;&gt; from underthesea import word_tokenize\n&gt;&gt;&gt; word_tokenize(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n['Xin', 'ch\u00e0o', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, make sure you have the correct optional dependencies installed:</p> <pre><code># For deep learning features\npip install \"underthesea[deep]\"\n\n# For text-to-speech\npip install \"underthesea[voice]\"\n\n# For language detection\npip install \"underthesea[langdetect]\"\n</code></pre>"},{"location":"installation/#model-download-issues","title":"Model Download Issues","text":"<p>Some models are downloaded automatically on first use. If you have network issues, you can manually download models:</p> <pre><code>underthesea download-model MODEL_NAME\n</code></pre>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> <li>Facebook Community</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get started with Underthesea in 5 minutes!</p>"},{"location":"quickstart/#step-1-install-underthesea","title":"Step 1: Install Underthesea","text":"<pre><code>pip install underthesea\n</code></pre>"},{"location":"quickstart/#step-2-try-your-first-nlp-task","title":"Step 2: Try Your First NLP Task","text":""},{"location":"quickstart/#word-segmentation","title":"Word Segmentation","text":"<p>Vietnamese text doesn't have spaces between words. Underthesea can segment text into words:</p> <pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# ['Ch\u00e0ng trai', '9X', 'Qu\u1ea3ng Tr\u1ecb', 'kh\u1edfi nghi\u1ec7p', 't\u1eeb', 'n\u1ea5m', 's\u00f2']\n</code></pre>"},{"location":"quickstart/#pos-tagging","title":"POS Tagging","text":"<p>Label each word with its part-of-speech:</p> <pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nprint(tagged)\n# [('Ch\u1ee3', 'N'), ('th\u1ecbt', 'N'), ('ch\u00f3', 'N'), ('n\u1ed5i ti\u1ebfng', 'A'),\n#  ('\u1edf', 'E'), ('S\u00e0i G\u00f2n', 'Np'), ('b\u1ecb', 'V'), ('truy qu\u00e9t', 'V')]\n</code></pre>"},{"location":"quickstart/#named-entity-recognition","title":"Named Entity Recognition","text":"<p>Identify named entities like people, locations, and organizations:</p> <pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\nfor word, pos, chunk, entity in entities:\n    if entity != 'O':\n        print(f\"{word}: {entity}\")\n# Vi\u1ec7t Nam: B-LOC\n# M\u1ef9: B-LOC\n# Donald: B-PER\n# Trump: I-PER\n</code></pre>"},{"location":"quickstart/#text-classification","title":"Text Classification","text":"<p>Categorize Vietnamese text:</p> <pre><code>from underthesea import classify\n\ntext = \"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\"\ncategory = classify(text)\nprint(category)\n# ['The thao']\n</code></pre>"},{"location":"quickstart/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>Determine the sentiment of text:</p> <pre><code>from underthesea import sentiment\n\ntext = \"S\u1ea3n ph\u1ea9m h\u01a1i nh\u1ecf so v\u1edbi t\u01b0\u1edfng t\u01b0\u1ee3ng nh\u01b0ng ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t\"\nresult = sentiment(text)\nprint(result)\n# 'positive'\n</code></pre>"},{"location":"quickstart/#step-3-explore-more-features","title":"Step 3: Explore More Features","text":""},{"location":"quickstart/#translation-requires-deep-install","title":"Translation (requires <code>[deep]</code> install)","text":"<pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>from underthesea import translate\n\ntext = \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\nenglish = translate(text)\nprint(english)\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"quickstart/#language-detection-requires-langdetect-install","title":"Language Detection (requires <code>[langdetect]</code> install)","text":"<pre><code>pip install \"underthesea[langdetect]\"\n</code></pre> <pre><code>from underthesea import lang_detect\n\ntext = \"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng\"\nlang = lang_detect(text)\nprint(lang)\n# 'vi'\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Tutorials for in-depth guides</li> <li>Explore the API Reference for complete documentation</li> <li>Learn about Optional Dependencies</li> <li>Check out the Concepts to understand how Underthesea works</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides complete API documentation for all Underthesea functions.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":"Function Description Install <code>sent_tokenize</code> Sentence segmentation Core <code>text_normalize</code> Text normalization Core <code>word_tokenize</code> Word segmentation Core <code>pos_tag</code> Part-of-speech tagging Core <code>chunk</code> Phrase chunking Core <code>ner</code> Named entity recognition Core <code>classify</code> Text classification Core <code>sentiment</code> Sentiment analysis Core"},{"location":"api/#deep-learning-functions","title":"Deep Learning Functions","text":"Function Description Install <code>dependency_parse</code> Dependency parsing <code>[deep]</code> <code>translate</code> Vietnamese-English translation <code>[deep]</code>"},{"location":"api/#additional-functions","title":"Additional Functions","text":"Function Description Install <code>lang_detect</code> Language detection <code>[langdetect]</code> <code>tts</code> Text-to-speech <code>[voice]</code>"},{"location":"api/#quick-import","title":"Quick Import","text":"<p>All main functions can be imported directly from <code>underthesea</code>:</p> <pre><code>from underthesea import (\n    sent_tokenize,\n    text_normalize,\n    word_tokenize,\n    pos_tag,\n    chunk,\n    ner,\n    classify,\n    sentiment,\n    dependency_parse,  # requires [deep]\n    translate,         # requires [deep]\n    lang_detect,       # requires [langdetect]\n)\n</code></pre>"},{"location":"api/#common-parameters","title":"Common Parameters","text":"<p>Many functions share common parameters:</p>"},{"location":"api/#format","title":"<code>format</code>","text":"<p>Controls output format:</p> <ul> <li><code>None</code> (default): Returns a list</li> <li><code>\"text\"</code>: Returns a string with underscores joining multi-word tokens</li> </ul> <pre><code>word_tokenize(\"Vi\u1ec7t Nam\", format=None)   # ['Vi\u1ec7t Nam']\nword_tokenize(\"Vi\u1ec7t Nam\", format=\"text\") # 'Vi\u1ec7t_Nam'\n</code></pre>"},{"location":"api/#model","title":"<code>model</code>","text":"<p>Specifies which model to use:</p> <pre><code># Use default model\nner(\"text\")\n\n# Use specific model\nner(\"text\", deep=True)  # Use deep learning model\nclassify(\"text\", model='prompt')  # Use OpenAI model\n</code></pre>"},{"location":"api/#domain","title":"<code>domain</code>","text":"<p>Specifies the domain for domain-specific models:</p> <pre><code>classify(\"text\", domain='bank')\nsentiment(\"text\", domain='bank')\n</code></pre>"},{"location":"api/chunk/","title":"chunk","text":"<p>Group words into meaningful phrases (chunking/shallow parsing).</p>"},{"location":"api/chunk/#usage","title":"Usage","text":"<pre><code>from underthesea import chunk\n\ntext = \"B\u00e1c s\u0129 b\u00e2y gi\u1edd c\u00f3 th\u1ec3 th\u1ea3n nhi\u00ean b\u00e1o tin b\u1ec7nh nh\u00e2n b\u1ecb ung th\u01b0?\"\nchunks = chunk(text)\nprint(chunks)\n# [('B\u00e1c s\u0129', 'N', 'B-NP'),\n#  ('b\u00e2y gi\u1edd', 'P', 'B-NP'),\n#  ('c\u00f3 th\u1ec3', 'R', 'O'),\n#  ('th\u1ea3n nhi\u00ean', 'A', 'B-AP'),\n#  ('b\u00e1o', 'V', 'B-VP'),\n#  ('tin', 'N', 'B-NP'),\n#  ('b\u1ec7nh nh\u00e2n', 'N', 'B-NP'),\n#  ('b\u1ecb', 'V', 'B-VP'),\n#  ('ung th\u01b0', 'N', 'B-NP'),\n#  ('?', 'CH', 'O')]\n</code></pre>"},{"location":"api/chunk/#function-signature","title":"Function Signature","text":"<pre><code>def chunk(\n    sentence: str,\n    format: str = None\n) -&gt; list[tuple[str, str, str]]\n</code></pre>"},{"location":"api/chunk/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to chunk <code>format</code> <code>str</code> <code>None</code> Output format (currently only <code>None</code> supported)"},{"location":"api/chunk/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, str, str]]</code> List of (word, POS tag, chunk tag) tuples"},{"location":"api/chunk/#chunk-tags","title":"Chunk Tags","text":"Tag Description <code>B-NP</code> Beginning of Noun Phrase <code>I-NP</code> Inside Noun Phrase <code>B-VP</code> Beginning of Verb Phrase <code>I-VP</code> Inside Verb Phrase <code>B-AP</code> Beginning of Adjective Phrase <code>I-AP</code> Inside Adjective Phrase <code>B-PP</code> Beginning of Prepositional Phrase <code>I-PP</code> Inside Prepositional Phrase <code>O</code> Outside any chunk"},{"location":"api/chunk/#examples","title":"Examples","text":""},{"location":"api/chunk/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import chunk\n\ntext = \"B\u00e1c s\u0129 b\u00e2y gi\u1edd c\u00f3 th\u1ec3 th\u1ea3n nhi\u00ean b\u00e1o tin b\u1ec7nh nh\u00e2n b\u1ecb ung th\u01b0?\"\nchunks = chunk(text)\nfor word, pos, chunk_tag in chunks:\n    print(f\"{word:15} {pos:5} {chunk_tag}\")\n</code></pre>"},{"location":"api/chunk/#extracting-noun-phrases","title":"Extracting Noun Phrases","text":"<pre><code>text = \"Sinh vi\u00ean \u0110\u1ea1i h\u1ecdc B\u00e1ch Khoa H\u00e0 N\u1ed9i \u0111\u1ea1t gi\u1ea3i nh\u1ea5t\"\nchunks = chunk(text)\n\n# Extract noun phrases\ncurrent_np = []\nnoun_phrases = []\n\nfor word, pos, chunk_tag in chunks:\n    if chunk_tag == 'B-NP':\n        if current_np:\n            noun_phrases.append(' '.join(current_np))\n        current_np = [word]\n    elif chunk_tag == 'I-NP':\n        current_np.append(word)\n    else:\n        if current_np:\n            noun_phrases.append(' '.join(current_np))\n            current_np = []\n\nif current_np:\n    noun_phrases.append(' '.join(current_np))\n\nprint(noun_phrases)\n</code></pre>"},{"location":"api/chunk/#extracting-verb-phrases","title":"Extracting Verb Phrases","text":"<pre><code>text = \"T\u00f4i \u0111ang h\u1ecdc ti\u1ebfng Vi\u1ec7t v\u00e0 s\u1ebd \u0111i du l\u1ecbch\"\nchunks = chunk(text)\n\nverb_phrases = [word for word, pos, tag in chunks if tag.endswith('VP')]\nprint(verb_phrases)\n</code></pre>"},{"location":"api/chunk/#notes","title":"Notes","text":"<ul> <li>Chunking is performed on top of word segmentation and POS tagging</li> <li>It provides shallow syntactic structure without full parsing</li> <li>Useful for extracting noun phrases, verb phrases, etc.</li> </ul>"},{"location":"api/classify/","title":"classify","text":"<p>Categorize text into predefined categories.</p>"},{"location":"api/classify/#usage","title":"Usage","text":"<pre><code>from underthesea import classify\n\ntext = \"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\"\ncategory = classify(text)\nprint(category)\n# ['The thao']\n</code></pre>"},{"location":"api/classify/#function-signature","title":"Function Signature","text":"<pre><code>def classify(\n    X: str,\n    domain: str = None,\n    model: str = None\n) -&gt; list[str] | str\n</code></pre>"},{"location":"api/classify/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> <code>str</code> The input text to classify <code>domain</code> <code>str</code> <code>None</code> Domain for classification (<code>'bank'</code>) <code>model</code> <code>str</code> <code>None</code> Model type (<code>'prompt'</code> for OpenAI)"},{"location":"api/classify/#returns","title":"Returns","text":"Type Description <code>list[str]</code> List of predicted categories"},{"location":"api/classify/#available-domains","title":"Available Domains","text":""},{"location":"api/classify/#general-default","title":"General (Default)","text":"<p>News topic classification:</p> Category Description <code>The thao</code> Sports <code>Kinh doanh</code> Business <code>Chinh tri Xa hoi</code> Politics &amp; Society <code>Van hoa</code> Culture <code>Khoa hoc</code> Science <code>Phap luat</code> Law <code>Suc khoe</code> Health <code>Doi song</code> Lifestyle <code>The gioi</code> World <code>Vi tinh</code> Technology"},{"location":"api/classify/#bank-domain","title":"Bank Domain","text":"<p>Bank-related topic classification:</p> <pre><code>classify(text, domain='bank')\n</code></pre> Category Description <code>INTEREST_RATE</code> Interest rate related <code>CUSTOMER_SUPPORT</code> Customer service <code>PRODUCT</code> Bank products <code>TRADEMARK</code> Brand/trademark"},{"location":"api/classify/#examples","title":"Examples","text":""},{"location":"api/classify/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import classify\n\n# Sports\nclassify(\"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\")\n# ['The thao']\n\n# Business\nclassify(\"H\u1ed9i \u0111\u1ed3ng t\u01b0 v\u1ea5n kinh doanh Asean vinh danh gi\u1ea3i th\u01b0\u1edfng qu\u1ed1c t\u1ebf\")\n# ['Kinh doanh']\n</code></pre>"},{"location":"api/classify/#bank-domain_1","title":"Bank Domain","text":"<pre><code>from underthesea import classify\n\nclassify(\"L\u00e3i su\u1ea5t t\u1eeb BIDV r\u1ea5t \u01b0u \u0111\u00e3i\", domain='bank')\n# ['INTEREST_RATE']\n\nclassify(\"Nh\u00e2n vi\u00ean h\u1ed7 tr\u1ee3 r\u1ea5t nhi\u1ec7t t\u00ecnh\", domain='bank')\n# ['CUSTOMER_SUPPORT']\n</code></pre>"},{"location":"api/classify/#prompt-based-model","title":"Prompt-based Model","text":"<p>Requires Installation</p> <pre><code>pip install \"underthesea[prompt]\"\nexport OPENAI_API_KEY=your_api_key\n</code></pre> <pre><code>from underthesea import classify\n\ntext = \"HLV ngo\u1ea1i \u0111\u00f2i g\u1ea7n t\u1ef7 m\u1ed7i th\u00e1ng d\u1eabn d\u1eaft tuy\u1ec3n Vi\u1ec7t Nam\"\nresult = classify(text, model='prompt')\nprint(result)\n# 'Th\u1ec3 thao'\n</code></pre>"},{"location":"api/classify/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>from underthesea import classify\n\ndocuments = [\n    \"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam th\u1eafng \u0111\u1eadm trong tr\u1eadn \u0111\u1ea5u\",\n    \"Gi\u00e1 v\u00e0ng t\u0103ng m\u1ea1nh trong tu\u1ea7n qua\",\n    \"Ph\u00e1t hi\u1ec7n virus m\u1edbi g\u00e2y b\u1ec7nh \u1edf ch\u00e2u Phi\"\n]\n\nfor doc in documents:\n    category = classify(doc)\n    print(f\"{doc[:30]}... -&gt; {category}\")\n</code></pre>"},{"location":"api/classify/#accessing-available-labels","title":"Accessing Available Labels","text":"<p>You can access all available category labels using the <code>labels</code> property:</p> <pre><code>from underthesea import classify\n\n# Get labels for general domain\nclassify.labels\n# ['chinh_tri_xa_hoi', 'doi_song', 'khoa_hoc', 'kinh_doanh', 'phap_luat',\n#  'suc_khoe', 'the_gioi', 'the_thao', 'van_hoa', 'vi_tinh']\n\n# Get labels for bank domain\nclassify.bank.labels\n# ['ACCOUNT', 'CARD', 'CUSTOMER_SUPPORT', 'DISCOUNT', 'INTEREST_RATE',\n#  'INTERNET_BANKING', 'LOAN', 'MONEY_TRANSFER', 'OTHER', 'PAYMENT',\n#  'PROMOTION', 'SAVING', 'SECURITY', 'TRADEMARK']\n</code></pre>"},{"location":"api/classify/#notes","title":"Notes","text":"<ul> <li>The default model is trained on Vietnamese news data</li> <li>The bank domain model is specialized for banking feedback</li> <li>Prompt-based model uses OpenAI API and requires an API key</li> <li>First call may take longer due to model loading</li> <li>Use <code>classify.labels</code> to get all available categories for the default domain</li> <li>Use <code>classify.bank.labels</code> to get all available categories for the bank domain</li> </ul>"},{"location":"api/dependency_parse/","title":"dependency_parse","text":"<p>Analyze the grammatical structure and dependencies between words.</p> <p>Requires Deep Learning</p> <p>This function requires the deep learning dependencies: <pre><code>pip install \"underthesea[deep]\"\n</code></pre></p>"},{"location":"api/dependency_parse/#usage","title":"Usage","text":"<pre><code>from underthesea import dependency_parse\n\ntext = \"T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19\"\nresult = dependency_parse(text)\nprint(result)\n# [('T\u1ed1i', 5, 'obl:tmod'),\n#  ('29/11', 1, 'flat:date'),\n#  (',', 1, 'punct'),\n#  ('Vi\u1ec7t Nam', 5, 'nsubj'),\n#  ('th\u00eam', 0, 'root'),\n#  ('2', 7, 'nummod'),\n#  ('ca', 5, 'obj'),\n#  ('m\u1eafc', 7, 'nmod'),\n#  ('Covid-19', 8, 'nummod')]\n</code></pre>"},{"location":"api/dependency_parse/#function-signature","title":"Function Signature","text":"<pre><code>def dependency_parse(sentence: str) -&gt; list[tuple[str, int, str]]\n</code></pre>"},{"location":"api/dependency_parse/#parameters","title":"Parameters","text":"Parameter Type Description <code>sentence</code> <code>str</code> The input text to parse"},{"location":"api/dependency_parse/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, int, str]]</code> List of (word, head_index, relation) tuples <p>Each tuple contains:</p> <ul> <li><code>word</code>: The word token</li> <li><code>head_index</code>: Index of the head word (0 = root)</li> <li><code>relation</code>: The dependency relation type</li> </ul>"},{"location":"api/dependency_parse/#dependency-relations","title":"Dependency Relations","text":"Relation Description <code>root</code> Root of the sentence <code>nsubj</code> Nominal subject <code>obj</code> Object <code>obl</code> Oblique nominal <code>obl:tmod</code> Temporal modifier <code>amod</code> Adjectival modifier <code>nmod</code> Nominal modifier <code>nummod</code> Numeric modifier <code>punct</code> Punctuation <code>flat:date</code> Flat date expression <code>compound</code> Compound word"},{"location":"api/dependency_parse/#examples","title":"Examples","text":""},{"location":"api/dependency_parse/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import dependency_parse\n\ntext = \"T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19\"\nresult = dependency_parse(text)\n\nfor i, (word, head, rel) in enumerate(result, 1):\n    print(f\"{i}\\t{word}\\t{head}\\t{rel}\")\n# 1    T\u1ed1i         5    obl:tmod\n# 2    29/11       1    flat:date\n# 3    ,           1    punct\n# 4    Vi\u1ec7t Nam    5    nsubj\n# 5    th\u00eam        0    root\n# 6    2           7    nummod\n# 7    ca          5    obj\n# 8    m\u1eafc         7    nmod\n# 9    Covid-19    8    nummod\n</code></pre>"},{"location":"api/dependency_parse/#finding-the-root","title":"Finding the Root","text":"<pre><code>text = \"T\u00f4i y\u00eau Vi\u1ec7t Nam\"\nresult = dependency_parse(text)\n\nroot = [(i, word) for i, (word, head, rel) in enumerate(result, 1) if rel == 'root']\nprint(f\"Root: {root}\")\n# Root: [(2, 'y\u00eau')]\n</code></pre>"},{"location":"api/dependency_parse/#finding-subjects-and-objects","title":"Finding Subjects and Objects","text":"<pre><code>text = \"Sinh vi\u00ean \u0111\u1ecdc s\u00e1ch \u1edf th\u01b0 vi\u1ec7n\"\nresult = dependency_parse(text)\n\nsubjects = [word for word, head, rel in result if rel == 'nsubj']\nobjects = [word for word, head, rel in result if rel == 'obj']\n\nprint(f\"Subjects: {subjects}\")\nprint(f\"Objects: {objects}\")\n</code></pre>"},{"location":"api/dependency_parse/#notes","title":"Notes","text":"<ul> <li>This function uses a transformer-based model</li> <li>First call may take longer due to model loading</li> <li>Requires significant memory for the deep learning model</li> </ul>"},{"location":"api/lang_detect/","title":"lang_detect","text":"<p>Identify the language of text.</p> <p>Requires Language Detection</p> <p>This function requires the langdetect dependencies: <pre><code>pip install \"underthesea[langdetect]\"\n</code></pre></p>"},{"location":"api/lang_detect/#usage","title":"Usage","text":"<pre><code>from underthesea import lang_detect\n\ntext = \"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng khi th\u1ea5y cu\u1ed9c s\u1ed1ng h\u00f2a b\u00ecnh t\u1ea1i Vi\u1ec7t Nam\"\nlang = lang_detect(text)\nprint(lang)\n# 'vi'\n</code></pre>"},{"location":"api/lang_detect/#function-signature","title":"Function Signature","text":"<pre><code>def lang_detect(text: str) -&gt; str\n</code></pre>"},{"location":"api/lang_detect/#parameters","title":"Parameters","text":"Parameter Type Description <code>text</code> <code>str</code> The input text to analyze"},{"location":"api/lang_detect/#returns","title":"Returns","text":"Type Description <code>str</code> ISO 639-1 language code"},{"location":"api/lang_detect/#supported-languages","title":"Supported Languages","text":"<p>The function can detect 176 languages. Common codes:</p> Code Language <code>vi</code> Vietnamese <code>en</code> English <code>zh</code> Chinese <code>ja</code> Japanese <code>ko</code> Korean <code>fr</code> French <code>de</code> German <code>es</code> Spanish <code>ru</code> Russian <code>th</code> Thai"},{"location":"api/lang_detect/#examples","title":"Examples","text":""},{"location":"api/lang_detect/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import lang_detect\n\n# Vietnamese\nlang_detect(\"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng\")\n# 'vi'\n\n# English\nlang_detect(\"Hello, how are you today?\")\n# 'en'\n\n# Chinese\nlang_detect(\"\u4f60\u597d\uff0c\u4eca\u5929\u600e\u4e48\u6837\uff1f\")\n# 'zh'\n\n# Japanese\nlang_detect(\"\u3053\u3093\u306b\u3061\u306f\u3001\u5143\u6c17\u3067\u3059\u304b\uff1f\")\n# 'ja'\n</code></pre>"},{"location":"api/lang_detect/#detecting-multiple-texts","title":"Detecting Multiple Texts","text":"<pre><code>from underthesea import lang_detect\n\ntexts = [\n    \"Xin ch\u00e0o Vi\u1ec7t Nam\",\n    \"Hello World\",\n    \"Bonjour le monde\",\n    \"Hallo Welt\"\n]\n\nfor text in texts:\n    lang = lang_detect(text)\n    print(f\"{text} -&gt; {lang}\")\n# Xin ch\u00e0o Vi\u1ec7t Nam -&gt; vi\n# Hello World -&gt; en\n# Bonjour le monde -&gt; fr\n# Hallo Welt -&gt; de\n</code></pre>"},{"location":"api/lang_detect/#filtering-by-language","title":"Filtering by Language","text":"<pre><code>from underthesea import lang_detect\n\ndocuments = [\n    \"Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p\",\n    \"This is an English sentence\",\n    \"H\u00f4m nay tr\u1eddi \u0111\u1eb9p qu\u00e1\",\n    \"The weather is nice today\"\n]\n\n# Filter Vietnamese documents\nvietnamese_docs = [doc for doc in documents if lang_detect(doc) == 'vi']\nprint(vietnamese_docs)\n# ['Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p', 'H\u00f4m nay tr\u1eddi \u0111\u1eb9p qu\u00e1']\n</code></pre>"},{"location":"api/lang_detect/#language-statistics","title":"Language Statistics","text":"<pre><code>from collections import Counter\nfrom underthesea import lang_detect\n\ndocuments = [\n    \"Xin ch\u00e0o\",\n    \"Hello\",\n    \"T\u1ea1m bi\u1ec7t\",\n    \"Goodbye\",\n    \"C\u1ea3m \u01a1n\",\n    \"Merci\"\n]\n\nlangs = [lang_detect(doc) for doc in documents]\ndistribution = Counter(langs)\nprint(distribution)\n# Counter({'vi': 3, 'en': 2, 'fr': 1})\n</code></pre>"},{"location":"api/lang_detect/#notes","title":"Notes","text":"<ul> <li>Uses FastText's language identification model</li> <li>Works best with longer text (at least a few words)</li> <li>Very short text may be less accurate</li> <li>First call may take longer due to model loading</li> </ul>"},{"location":"api/ner/","title":"ner","text":"<p>Identify and classify named entities in text.</p>"},{"location":"api/ner/#usage","title":"Usage","text":"<pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\nprint(entities)\n# [('Ch\u01b0a', 'R', 'O', 'O'),\n#  ('ti\u1ebft l\u1ed9', 'V', 'B-VP', 'O'),\n#  ('l\u1ecbch tr\u00ecnh', 'V', 'B-VP', 'O'),\n#  ('t\u1edbi', 'E', 'B-PP', 'O'),\n#  ('Vi\u1ec7t Nam', 'Np', 'B-NP', 'B-LOC'),\n#  ('c\u1ee7a', 'E', 'B-PP', 'O'),\n#  ('T\u1ed5ng th\u1ed1ng', 'N', 'B-NP', 'O'),\n#  ('M\u1ef9', 'Np', 'B-NP', 'B-LOC'),\n#  ('Donald', 'Np', 'B-NP', 'B-PER'),\n#  ('Trump', 'Np', 'B-NP', 'I-PER')]\n</code></pre>"},{"location":"api/ner/#function-signature","title":"Function Signature","text":"<pre><code>def ner(\n    sentence: str,\n    format: str = None,\n    deep: bool = False\n) -&gt; list[tuple] | list[dict]\n</code></pre>"},{"location":"api/ner/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text <code>format</code> <code>str</code> <code>None</code> Output format <code>deep</code> <code>bool</code> <code>False</code> Use deep learning model (requires <code>[deep]</code> install)"},{"location":"api/ner/#returns","title":"Returns","text":""},{"location":"api/ner/#crf-model-default","title":"CRF Model (default)","text":"Type Description <code>list[tuple[str, str, str, str]]</code> List of (word, POS, chunk, entity) tuples"},{"location":"api/ner/#deep-learning-model-deeptrue","title":"Deep Learning Model (<code>deep=True</code>)","text":"Type Description <code>list[dict]</code> List of dictionaries with <code>entity</code> and <code>word</code> keys"},{"location":"api/ner/#entity-types","title":"Entity Types","text":"Tag Description <code>PER</code> Person <code>LOC</code> Location <code>ORG</code> Organization <code>O</code> Not an entity <p>Tags use BIO format:</p> <ul> <li><code>B-XXX</code>: Beginning of entity</li> <li><code>I-XXX</code>: Inside entity</li> <li><code>O</code>: Outside (not an entity)</li> </ul>"},{"location":"api/ner/#examples","title":"Examples","text":""},{"location":"api/ner/#basic-usage-crf-model","title":"Basic Usage (CRF Model)","text":"<pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\n\n# Extract only named entities\nfor word, pos, chunk, entity in entities:\n    if entity != 'O':\n        print(f\"{word}: {entity}\")\n# Vi\u1ec7t Nam: B-LOC\n# M\u1ef9: B-LOC\n# Donald: B-PER\n# Trump: I-PER\n</code></pre>"},{"location":"api/ner/#deep-learning-model","title":"Deep Learning Model","text":"<p>Requires Installation</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>from underthesea import ner\n\ntext = \"B\u1ed9 C\u00f4ng Th\u01b0\u01a1ng x\u00f3a m\u1ed9t t\u1ed5ng c\u1ee5c, gi\u1ea3m nhi\u1ec1u \u0111\u1ea7u m\u1ed1i\"\nentities = ner(text, deep=True)\nprint(entities)\n# [\n#   {'entity': 'B-ORG', 'word': 'B\u1ed9'},\n#   {'entity': 'I-ORG', 'word': 'C\u00f4ng'},\n#   {'entity': 'I-ORG', 'word': 'Th\u01b0\u01a1ng'}\n# ]\n</code></pre>"},{"location":"api/ner/#extracting-entities-by-type","title":"Extracting Entities by Type","text":"<pre><code>text = \"\u00d4ng Nguy\u1ec5n V\u0103n A t\u1eeb H\u00e0 N\u1ed9i \u0111\u1ebfn c\u00f4ng ty ABC\"\nentities = ner(text)\n\npersons = []\nlocations = []\norgs = []\n\nfor word, pos, chunk, entity in entities:\n    if entity.endswith('PER'):\n        persons.append(word)\n    elif entity.endswith('LOC'):\n        locations.append(word)\n    elif entity.endswith('ORG'):\n        orgs.append(word)\n\nprint(f\"Persons: {persons}\")\nprint(f\"Locations: {locations}\")\nprint(f\"Organizations: {orgs}\")\n</code></pre>"},{"location":"api/ner/#combining-multi-word-entities","title":"Combining Multi-word Entities","text":"<pre><code>text = \"T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump th\u0103m Vi\u1ec7t Nam\"\nentities = ner(text)\n\n# Combine B-/I- tags into full entities\ncurrent_entity = []\ncurrent_type = None\nfull_entities = []\n\nfor word, pos, chunk, entity in entities:\n    if entity.startswith('B-'):\n        if current_entity:\n            full_entities.append((' '.join(current_entity), current_type))\n        current_entity = [word]\n        current_type = entity[2:]\n    elif entity.startswith('I-'):\n        current_entity.append(word)\n    else:\n        if current_entity:\n            full_entities.append((' '.join(current_entity), current_type))\n            current_entity = []\n            current_type = None\n\nif current_entity:\n    full_entities.append((' '.join(current_entity), current_type))\n\nprint(full_entities)\n# [('Donald Trump', 'PER'), ('Vi\u1ec7t Nam', 'LOC')]\n</code></pre>"},{"location":"api/ner/#notes","title":"Notes","text":"<ul> <li>The CRF model is fast and lightweight</li> <li>The deep learning model provides better accuracy but requires more resources</li> <li>First call with <code>deep=True</code> may take longer due to model loading</li> </ul>"},{"location":"api/pos_tag/","title":"pos_tag","text":"<p>Label words with their part-of-speech tags.</p>"},{"location":"api/pos_tag/#usage","title":"Usage","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nprint(tagged)\n# [('Ch\u1ee3', 'N'), ('th\u1ecbt', 'N'), ('ch\u00f3', 'N'), ('n\u1ed5i ti\u1ebfng', 'A'),\n#  ('\u1edf', 'E'), ('S\u00e0i G\u00f2n', 'Np'), ('b\u1ecb', 'V'), ('truy qu\u00e9t', 'V')]\n</code></pre>"},{"location":"api/pos_tag/#function-signature","title":"Function Signature","text":"<pre><code>def pos_tag(\n    sentence: str,\n    format: str = None,\n    model: str = None\n) -&gt; list[tuple[str, str]]\n</code></pre>"},{"location":"api/pos_tag/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to tag <code>format</code> <code>str</code> <code>None</code> Output format (currently only <code>None</code> supported) <code>model</code> <code>str</code> <code>None</code> Path to custom model"},{"location":"api/pos_tag/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, str]]</code> List of (word, POS tag) tuples"},{"location":"api/pos_tag/#pos-tags","title":"POS Tags","text":"Tag Description Example <code>N</code> Noun ch\u1ee3, th\u1ecbt, ch\u00f3 <code>Np</code> Proper noun S\u00e0i G\u00f2n, Vi\u1ec7t Nam <code>V</code> Verb b\u1ecb, truy qu\u00e9t <code>A</code> Adjective n\u1ed5i ti\u1ebfng, \u0111\u1eb9p <code>P</code> Pronoun t\u00f4i, b\u1ea1n, n\u00f3 <code>R</code> Adverb r\u1ea5t, \u0111ang, s\u1ebd <code>E</code> Preposition \u1edf, trong, tr\u00ean <code>C</code> Conjunction v\u00e0, ho\u1eb7c, nh\u01b0ng <code>M</code> Number m\u1ed9t, hai, ba <code>L</code> Determiner c\u00e1c, nh\u1eefng, m\u1ecdi <code>X</code> Unknown - <code>CH</code> Punctuation . , ? !"},{"location":"api/pos_tag/#examples","title":"Examples","text":""},{"location":"api/pos_tag/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nfor word, tag in tagged:\n    print(f\"{word}: {tag}\")\n# Ch\u1ee3: N\n# th\u1ecbt: N\n# ch\u00f3: N\n# n\u1ed5i ti\u1ebfng: A\n# \u1edf: E\n# S\u00e0i G\u00f2n: Np\n# b\u1ecb: V\n# truy qu\u00e9t: V\n</code></pre>"},{"location":"api/pos_tag/#filtering-by-pos-tag","title":"Filtering by POS Tag","text":"<pre><code>text = \"T\u00f4i y\u00eau Vi\u1ec7t Nam v\u00ec Vi\u1ec7t Nam r\u1ea5t \u0111\u1eb9p\"\ntagged = pos_tag(text)\n\n# Get all nouns\nnouns = [word for word, tag in tagged if tag in ('N', 'Np')]\nprint(nouns)\n# ['Vi\u1ec7t Nam', 'Vi\u1ec7t Nam']\n\n# Get all verbs\nverbs = [word for word, tag in tagged if tag == 'V']\nprint(verbs)\n# ['y\u00eau']\n</code></pre>"},{"location":"api/pos_tag/#processing-multiple-sentences","title":"Processing Multiple Sentences","text":"<pre><code>sentences = [\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\",\n    \"Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t\"\n]\n\nfor sentence in sentences:\n    tagged = pos_tag(sentence)\n    print(tagged)\n</code></pre>"},{"location":"api/pos_tag/#notes","title":"Notes","text":"<ul> <li>Word segmentation is performed automatically before POS tagging</li> <li>The model is trained on Vietnamese treebank data</li> <li>Proper nouns (Np) include names, locations, organizations</li> </ul>"},{"location":"api/sent_tokenize/","title":"sent_tokenize","text":"<p>Segment text into sentences.</p>"},{"location":"api/sent_tokenize/#usage","title":"Usage","text":"<pre><code>from underthesea import sent_tokenize\n\ntext = 'Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng. Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.'\n\nsentences = sent_tokenize(text)\nprint(sentences)\n# [\n#   \"Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng.\",\n#   \"Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.\"\n# ]\n</code></pre>"},{"location":"api/sent_tokenize/#function-signature","title":"Function Signature","text":"<pre><code>def sent_tokenize(text: str) -&gt; list[str]\n</code></pre>"},{"location":"api/sent_tokenize/#parameters","title":"Parameters","text":"Parameter Type Description <code>text</code> <code>str</code> The input text to segment into sentences"},{"location":"api/sent_tokenize/#returns","title":"Returns","text":"Type Description <code>list[str]</code> A list of sentences"},{"location":"api/sent_tokenize/#examples","title":"Examples","text":""},{"location":"api/sent_tokenize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sent_tokenize\n\ntext = \"Xin ch\u00e0o. T\u00f4i l\u00e0 sinh vi\u00ean. T\u00f4i h\u1ecdc \u1edf H\u00e0 N\u1ed9i.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['Xin ch\u00e0o.', 'T\u00f4i l\u00e0 sinh vi\u00ean.', 'T\u00f4i h\u1ecdc \u1edf H\u00e0 N\u1ed9i.']\n</code></pre>"},{"location":"api/sent_tokenize/#multiple-sentences","title":"Multiple Sentences","text":"<pre><code>text = \"\"\"Vi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia. Th\u1ee7 \u0111\u00f4 l\u00e0 H\u00e0 N\u1ed9i. Th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 TP. H\u1ed3 Ch\u00ed Minh.\"\"\"\nsentences = sent_tokenize(text)\nprint(len(sentences))  # 3\n</code></pre>"},{"location":"api/sent_tokenize/#handling-abbreviations","title":"Handling Abbreviations","text":"<p>The function handles common Vietnamese abbreviations:</p> <pre><code>text = \"TP. H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam. D\u00e2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['TP. H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam.', 'D\u00e2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.']\n</code></pre>"},{"location":"api/sent_tokenize/#notes","title":"Notes","text":"<ul> <li>The function uses rule-based sentence boundary detection</li> <li>It handles common Vietnamese punctuation patterns</li> <li>Abbreviations like \"TP.\" (th\u00e0nh ph\u1ed1) are handled correctly</li> </ul>"},{"location":"api/sentiment/","title":"sentiment","text":"<p>Analyze the sentiment of text.</p>"},{"location":"api/sentiment/#usage","title":"Usage","text":"<pre><code>from underthesea import sentiment\n\ntext = \"S\u1ea3n ph\u1ea9m h\u01a1i nh\u1ecf so v\u1edbi t\u01b0\u1edfng t\u01b0\u1ee3ng nh\u01b0ng ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t\"\nresult = sentiment(text)\nprint(result)\n# 'positive'\n</code></pre>"},{"location":"api/sentiment/#function-signature","title":"Function Signature","text":"<pre><code>def sentiment(\n    X: str,\n    domain: str = 'general'\n) -&gt; str | list[str]\n</code></pre>"},{"location":"api/sentiment/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> <code>str</code> The input text to analyze <code>domain</code> <code>str</code> <code>'general'</code> Domain for analysis (<code>'general'</code> or <code>'bank'</code>)"},{"location":"api/sentiment/#returns","title":"Returns","text":""},{"location":"api/sentiment/#general-domain","title":"General Domain","text":"Type Description <code>str</code> Sentiment label: <code>'positive'</code>, <code>'negative'</code>, or <code>'neutral'</code>"},{"location":"api/sentiment/#bank-domain","title":"Bank Domain","text":"Type Description <code>list[str]</code> List of aspect-sentiment pairs (e.g., <code>['ASPECT#sentiment']</code>)"},{"location":"api/sentiment/#examples","title":"Examples","text":""},{"location":"api/sentiment/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sentiment\n\n# Positive sentiment\nsentiment(\"S\u1ea3n ph\u1ea9m ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t, \u0111\u00f3ng g\u00f3i c\u1ea9n th\u1eadn\")\n# 'positive'\n\n# Negative sentiment\nsentiment(\"h\u00e0ng k\u00e9m ch\u1ea5t lg, ch\u0103n \u0111\u1eafp l\u00ean d\u00ednh l\u00f4ng l\u00e1 kh\u1eafp ng\u01b0\u1eddi. th\u1ea5t v\u1ecdng\")\n# 'negative'\n</code></pre>"},{"location":"api/sentiment/#bank-domain_1","title":"Bank Domain","text":"<p>The bank domain provides aspect-based sentiment analysis:</p> <pre><code>from underthesea import sentiment\n\n# Customer support aspect\nsentiment(\"\u0110ky qua \u0111\u01b0\u1eddng link \u1edf b\u00e0i vi\u1ebft n\u00e0y t\u1eeb th\u1ee9 6 m\u00e0 gi\u1edd ch\u01b0a th\u1ea5y ai lhe h\u1ebft\", domain='bank')\n# ['CUSTOMER_SUPPORT#negative']\n\n# Trademark aspect\nsentiment(\"Xem l\u1ea1i v\u1eabn th\u1ea5y x\u00fac \u0111\u1ed9ng v\u00e0 t\u1ef1 h\u00e0o v\u1ec1 BIDV c\u1ee7a m\u00ecnh\", domain='bank')\n# ['TRADEMARK#positive']\n</code></pre>"},{"location":"api/sentiment/#bank-domain-aspects","title":"Bank Domain Aspects","text":"Aspect Description <code>INTEREST_RATE</code> Interest rate related <code>CUSTOMER_SUPPORT</code> Customer service quality <code>PRODUCT</code> Product/service quality <code>TRADEMARK</code> Brand perception"},{"location":"api/sentiment/#processing-reviews","title":"Processing Reviews","text":"<pre><code>from underthesea import sentiment\n\nreviews = [\n    \"D\u1ecbch v\u1ee5 tuy\u1ec7t v\u1eddi, nh\u00e2n vi\u00ean nhi\u1ec7t t\u00ecnh\",\n    \"Giao h\u00e0ng ch\u1eadm, \u0111\u00f3ng g\u00f3i kh\u00f4ng c\u1ea9n th\u1eadn\",\n    \"S\u1ea3n ph\u1ea9m b\u00ecnh th\u01b0\u1eddng, kh\u00f4ng c\u00f3 g\u00ec \u0111\u1eb7c bi\u1ec7t\"\n]\n\nfor review in reviews:\n    result = sentiment(review)\n    print(f\"{review[:30]}... -&gt; {result}\")\n# D\u1ecbch v\u1ee5 tuy\u1ec7t v\u1eddi, nh\u00e2n vi\u00ean n... -&gt; positive\n# Giao h\u00e0ng ch\u1eadm, \u0111\u00f3ng g\u00f3i kh\u00f4ng... -&gt; negative\n# S\u1ea3n ph\u1ea9m b\u00ecnh th\u01b0\u1eddng, kh\u00f4ng c\u00f3... -&gt; neutral\n</code></pre>"},{"location":"api/sentiment/#counting-sentiment-distribution","title":"Counting Sentiment Distribution","text":"<pre><code>from collections import Counter\nfrom underthesea import sentiment\n\nreviews = [\n    \"S\u1ea3n ph\u1ea9m t\u1ed1t\",\n    \"Kh\u00f4ng h\u00e0i l\u00f2ng\",\n    \"R\u1ea5t th\u00edch\",\n    \"T\u1ec7 qu\u00e1\",\n    \"B\u00ecnh th\u01b0\u1eddng\"\n]\n\nsentiments = [sentiment(r) for r in reviews]\ndistribution = Counter(sentiments)\nprint(distribution)\n# Counter({'positive': 2, 'negative': 2, 'neutral': 1})\n</code></pre>"},{"location":"api/sentiment/#accessing-available-labels","title":"Accessing Available Labels","text":"<p>You can access all available sentiment labels using the <code>labels</code> property:</p> <pre><code>from underthesea import sentiment\n\n# Get labels for general domain\nsentiment.labels\n# ['positive', 'negative']\n\n# Get labels for bank domain\nsentiment.bank.labels\n# ['ACCOUNT#negative', 'CARD#negative', 'CARD#neutral', 'CARD#positive',\n#  'CUSTOMER_SUPPORT#negative', 'CUSTOMER_SUPPORT#neutral', 'CUSTOMER_SUPPORT#positive',\n#  'DISCOUNT#negative', 'DISCOUNT#neutral', 'DISCOUNT#positive', ...]\n</code></pre>"},{"location":"api/sentiment/#notes","title":"Notes","text":"<ul> <li>The general domain model classifies into positive/negative</li> <li>The bank domain model provides aspect-based sentiment</li> <li>First call may take longer due to model loading</li> <li>Use <code>sentiment.labels</code> to get all available labels for the general domain</li> <li>Use <code>sentiment.bank.labels</code> to get all available labels for the bank domain</li> </ul>"},{"location":"api/text_normalize/","title":"text_normalize","text":"<p>Normalize Vietnamese text by fixing common encoding and diacritic issues.</p>"},{"location":"api/text_normalize/#usage","title":"Usage","text":"<pre><code>from underthesea import text_normalize\n\ntext = \"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng ph\u00f2ng th\u00ed ngh\u1ecb\u00eam ho\u00e1 h\u1ecdc\"\nnormalized = text_normalize(text)\nprint(normalized)\n# \"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng ph\u00f2ng th\u00ed nghi\u1ec7m h\u00f3a h\u1ecdc\"\n</code></pre>"},{"location":"api/text_normalize/#function-signature","title":"Function Signature","text":"<pre><code>def text_normalize(text: str, tokenizer: str = 'underthesea') -&gt; str\n</code></pre>"},{"location":"api/text_normalize/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The input text to normalize <code>tokenizer</code> <code>str</code> <code>'underthesea'</code> The tokenizer to use"},{"location":"api/text_normalize/#returns","title":"Returns","text":"Type Description <code>str</code> The normalized text"},{"location":"api/text_normalize/#examples","title":"Examples","text":""},{"location":"api/text_normalize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import text_normalize\n\n# Fix diacritic issues\ntext_normalize(\"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng\")\n# \"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng\"\n</code></pre>"},{"location":"api/text_normalize/#common-fixes","title":"Common Fixes","text":"<pre><code># Fix \u0110/\u00d0 confusion\ntext_normalize(\"\u00d0\u1ea1i h\u1ecdc\")\n# \"\u0110\u1ea1i h\u1ecdc\"\n\n# Fix vowel diacritics\ntext_normalize(\"ho\u00e1 h\u1ecdc\")\n# \"h\u00f3a h\u1ecdc\"\n\n# Fix tone marks\ntext_normalize(\"ngh\u1ecb\u00eam\")\n# \"nghi\u1ec7m\"\n</code></pre>"},{"location":"api/text_normalize/#full-text-normalization","title":"Full Text Normalization","text":"<pre><code>text = \"\u00d0\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5 v\u1ec1 vi\u1ec7c chu\u1ea9n ho\u00e1 v\u0103n b\u1ea3n ti\u1ebfng Vi\u1ec7t\"\nnormalized = text_normalize(text)\nprint(normalized)\n# \"\u0110\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5 v\u1ec1 vi\u1ec7c chu\u1ea9n h\u00f3a v\u0103n b\u1ea3n ti\u1ebfng Vi\u1ec7t\"\n</code></pre>"},{"location":"api/text_normalize/#common-issues-fixed","title":"Common Issues Fixed","text":"Issue Example Fixed \u0110/\u00d0 confusion <code>\u00d0\u1ea1i</code> <code>\u0110\u1ea1i</code> Old-style diacritics <code>ho\u00e1</code> <code>h\u00f3a</code> Incorrect vowel composition <code>l\u1ef1\u01a1ng</code> <code>l\u01b0\u1ee3ng</code> Unicode normalization Various NFC form"},{"location":"api/text_normalize/#notes","title":"Notes","text":"<ul> <li>This function is useful for preprocessing Vietnamese text</li> <li>It handles common encoding issues from legacy systems</li> <li>The output is in Unicode NFC normalized form</li> </ul>"},{"location":"api/translate/","title":"translate","text":"<p>Translate text between Vietnamese and English.</p> <p>Requires Deep Learning</p> <p>This function requires the deep learning dependencies: <pre><code>pip install \"underthesea[deep]\"\n</code></pre></p>"},{"location":"api/translate/#usage","title":"Usage","text":"<pre><code>from underthesea import translate\n\n# Vietnamese to English (default)\ntext = \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\nenglish = translate(text)\nprint(english)\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"api/translate/#function-signature","title":"Function Signature","text":"<pre><code>def translate(\n    text: str,\n    source_lang: str = 'vi',\n    target_lang: str = 'en'\n) -&gt; str\n</code></pre>"},{"location":"api/translate/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The input text to translate <code>source_lang</code> <code>str</code> <code>'vi'</code> Source language code <code>target_lang</code> <code>str</code> <code>'en'</code> Target language code"},{"location":"api/translate/#returns","title":"Returns","text":"Type Description <code>str</code> The translated text"},{"location":"api/translate/#supported-languages","title":"Supported Languages","text":"Code Language <code>vi</code> Vietnamese <code>en</code> English"},{"location":"api/translate/#examples","title":"Examples","text":""},{"location":"api/translate/#vietnamese-to-english","title":"Vietnamese to English","text":"<pre><code>from underthesea import translate\n\n# Basic translation\ntranslate(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\")\n# 'Hanoi is the capital of Vietnam'\n\ntranslate(\"\u1ea8m th\u1ef1c Vi\u1ec7t Nam n\u1ed5i ti\u1ebfng tr\u00ean th\u1ebf gi\u1edbi\")\n# 'Vietnamese cuisine is famous around the world'\n\ntranslate(\"Xin ch\u00e0o, t\u00f4i l\u00e0 sinh vi\u00ean\")\n# 'Hello, I am a student'\n</code></pre>"},{"location":"api/translate/#english-to-vietnamese","title":"English to Vietnamese","text":"<pre><code>from underthesea import translate\n\ntranslate(\"I love Vietnamese food\", source_lang='en', target_lang='vi')\n# 'T\u00f4i y\u00eau \u1ea9m th\u1ef1c Vi\u1ec7t Nam'\n\ntranslate(\"Vietnam is a beautiful country\", source_lang='en', target_lang='vi')\n# 'Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p'\n</code></pre>"},{"location":"api/translate/#translating-multiple-sentences","title":"Translating Multiple Sentences","text":"<pre><code>from underthesea import translate\n\nsentences = [\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\",\n    \"Vi\u1ec7t Nam c\u00f3 nhi\u1ec1u \u0111\u1ecba \u0111i\u1ec3m du l\u1ecbch \u0111\u1eb9p\",\n    \"\u1ea8m th\u1ef1c Vi\u1ec7t Nam r\u1ea5t phong ph\u00fa\"\n]\n\nfor sentence in sentences:\n    english = translate(sentence)\n    print(f\"{sentence}\")\n    print(f\"-&gt; {english}\\n\")\n</code></pre>"},{"location":"api/translate/#handling-long-text","title":"Handling Long Text","text":"<p>For long documents, consider splitting into sentences first:</p> <pre><code>from underthesea import sent_tokenize, translate\n\ntext = \"\"\"Vi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia. Th\u1ee7 \u0111\u00f4 l\u00e0 H\u00e0 N\u1ed9i.\nTh\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 TP. H\u1ed3 Ch\u00ed Minh.\"\"\"\n\nsentences = sent_tokenize(text)\ntranslations = [translate(s) for s in sentences]\n\nfor vi, en in zip(sentences, translations):\n    print(f\"VI: {vi}\")\n    print(f\"EN: {en}\\n\")\n</code></pre>"},{"location":"api/translate/#notes","title":"Notes","text":"<ul> <li>Uses a transformer-based neural machine translation model</li> <li>First call may take longer due to model loading</li> <li>Works best with well-formed sentences</li> <li>Long texts should be split into sentences for better results</li> </ul>"},{"location":"api/tts/","title":"tts","text":"<p>Convert Vietnamese text to speech.</p> <p>Requires Additional Setup</p> <p>This function requires extra dependencies and model download: <pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre></p>"},{"location":"api/tts/#usage","title":"Usage","text":"<pre><code>from underthesea.pipeline.tts import tts\n\ntext = \"Xin ch\u00e0o Vi\u1ec7t Nam\"\ntts(text)\n# Generates sound.wav in current directory\n</code></pre>"},{"location":"api/tts/#function-signature","title":"Function Signature","text":"<pre><code>def tts(text: str, outfile: str = \"sound.wav\", play: bool = False) -&gt; tuple[int, np.ndarray]\n</code></pre>"},{"location":"api/tts/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The Vietnamese text to convert to speech <code>outfile</code> <code>str</code> <code>\"sound.wav\"</code> Output audio file path <code>play</code> <code>bool</code> <code>False</code> Whether to play audio after generation"},{"location":"api/tts/#returns","title":"Returns","text":"Type Description <code>tuple[int, np.ndarray]</code> Sample rate (16000) and audio waveform array"},{"location":"api/tts/#examples","title":"Examples","text":""},{"location":"api/tts/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea.pipeline.tts import tts\n\n# Generate speech\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n# Creates sound.wav\n\n# Custom output file\ntts(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\", outfile=\"hanoi.wav\")\n\n# Generate and play immediately\ntts(\"Xin ch\u00e0o\", play=True)\n</code></pre>"},{"location":"api/tts/#command-line-usage","title":"Command Line Usage","text":"<pre><code>underthesea tts \"Xin ch\u00e0o Vi\u1ec7t Nam\"\n# Creates sound.wav and plays it\n</code></pre>"},{"location":"api/tts/#generating-multiple-audio-files","title":"Generating Multiple Audio Files","text":"<pre><code>from underthesea.pipeline.tts import tts\n\nsentences = [\n    (\"Xin ch\u00e0o\", \"hello.wav\"),\n    (\"T\u1ea1m bi\u1ec7t\", \"goodbye.wav\"),\n    (\"C\u1ea3m \u01a1n\", \"thanks.wav\")\n]\n\nfor text, filename in sentences:\n    tts(text, outfile=filename)\n    print(f\"Generated: {filename}\")\n</code></pre>"},{"location":"api/tts/#playing-audio-with-external-library","title":"Playing Audio (with external library)","text":"<pre><code>from underthesea.pipeline.tts import tts\nimport subprocess\n\n# Generate audio\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n\n# Play audio (macOS)\nsubprocess.run([\"afplay\", \"sound.wav\"])\n\n# Play audio (Linux with aplay)\n# subprocess.run([\"aplay\", \"sound.wav\"])\n</code></pre>"},{"location":"api/tts/#notes","title":"Notes","text":"<ul> <li>Uses the VietTTS model for high-quality Vietnamese speech synthesis</li> <li>Output format is WAV audio at 16kHz sample rate</li> <li>First call may take longer due to model loading</li> <li>Requires downloading the TTS model before first use</li> <li>Credits: Based on NTT123/vietTTS</li> </ul>"},{"location":"api/tts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/tts/#model-not-found","title":"Model Not Found","text":"<p>If you get a model not found error:</p> <pre><code>underthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"api/tts/#audio-quality-issues","title":"Audio Quality Issues","text":"<ul> <li>Ensure input text is in Vietnamese</li> <li>Longer sentences produce smoother audio</li> <li>Punctuation affects prosody</li> </ul>"},{"location":"api/word_tokenize/","title":"word_tokenize","text":"<p>Segment Vietnamese text into words.</p>"},{"location":"api/word_tokenize/#usage","title":"Usage","text":"<pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# [\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n</code></pre>"},{"location":"api/word_tokenize/#function-signature","title":"Function Signature","text":"<pre><code>def word_tokenize(\n    sentence: str,\n    format: str = None,\n    use_token_normalize: bool = True,\n    fixed_words: list = None\n) -&gt; list[str] | str\n</code></pre>"},{"location":"api/word_tokenize/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to tokenize <code>format</code> <code>str</code> <code>None</code> Output format: <code>None</code> for list, <code>\"text\"</code> for string <code>use_token_normalize</code> <code>bool</code> <code>True</code> Whether to normalize tokens <code>fixed_words</code> <code>list</code> <code>None</code> List of words that should not be split"},{"location":"api/word_tokenize/#returns","title":"Returns","text":"Type Description <code>list[str]</code> List of words (when <code>format=None</code>) <code>str</code> Space-separated string with underscores (when <code>format=\"text\"</code>)"},{"location":"api/word_tokenize/#examples","title":"Examples","text":""},{"location":"api/word_tokenize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# [\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n</code></pre>"},{"location":"api/word_tokenize/#text-format","title":"Text Format","text":"<pre><code>text = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nresult = word_tokenize(text, format=\"text\")\nprint(result)\n# \"Ch\u00e0ng_trai 9X Qu\u1ea3ng_Tr\u1ecb kh\u1edfi_nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\n</code></pre>"},{"location":"api/word_tokenize/#fixed-words","title":"Fixed Words","text":"<p>Use <code>fixed_words</code> to ensure certain words are kept together:</p> <pre><code>text = \"Vi\u1ec7n Nghi\u00ean C\u1ee9u chi\u1ebfn l\u01b0\u1ee3c qu\u1ed1c gia v\u1ec1 h\u1ecdc m\u00e1y\"\nfixed_words = [\"Vi\u1ec7n Nghi\u00ean C\u1ee9u\", \"h\u1ecdc m\u00e1y\"]\nresult = word_tokenize(text, fixed_words=fixed_words, format=\"text\")\nprint(result)\n# \"Vi\u1ec7n_Nghi\u00ean_C\u1ee9u chi\u1ebfn_l\u01b0\u1ee3c qu\u1ed1c_gia v\u1ec1 h\u1ecdc_m\u00e1y\"\n</code></pre>"},{"location":"api/word_tokenize/#processing-multiple-sentences","title":"Processing Multiple Sentences","text":"<pre><code>sentences = [\n    \"T\u00f4i y\u00eau Vi\u1ec7t Nam\",\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\n]\n\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    print(words)\n# ['T\u00f4i', 'y\u00eau', 'Vi\u1ec7t Nam']\n# ['H\u00e0 N\u1ed9i', 'l\u00e0', 'th\u1ee7 \u0111\u00f4', 'c\u1ee7a', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"api/word_tokenize/#notes","title":"Notes","text":"<ul> <li>Vietnamese word segmentation is challenging because spaces don't always indicate word boundaries</li> <li>The function uses a CRF model trained on Vietnamese text</li> <li>Multi-syllable words are joined (e.g., \"Vi\u1ec7t Nam\" is one word, not two)</li> <li>Use <code>fixed_words</code> parameter for domain-specific terminology</li> </ul>"},{"location":"contribute/SPONSORS/","title":"Sponsors","text":"<p>\ud83d\udc4b If you are a fan of the project or a company that relies on Underthesea, you might want to consider sponsoring \ud83d\udcb0. This will help us devote more time to answering questions \ud83e\udd14 and doing feature development \ud83d\ude80. </p> <p>We thank those who support \ud83d\udc9d Underthesea! </p>"},{"location":"contribute/SPONSORS/#corporate","title":"Corporate \ud83c\udfe2","text":"<p>\ud83c\udf0a Underthesea has been dedicatedly working on Open Source Vietnamese Natural Lanugage Processing for the past six years. Our team of developers and researchers have been exploring various AI techniques to create tools that can simplify and automate complex tasks, thereby empowering businesses to thrive in the AI age.</p> <p>\ud83d\udcaa As we continue to build and refine our projects, we need support from corporate sponsors to help us achieve our goals. Sponsorship is an essential part of our growth strategy, and it will allow us to accelerate our research and development efforts, expand our team, and bring our technology to market.</p> <p>Join us in Advancing Vietnamese AI Technology:   \ud83e\udd1d Become a Corporate Sponsor Today </p>"},{"location":"contribute/SPONSORS/#individuals","title":"Individuals \ud83e\uddb8\u200d\u2642\ufe0f\ud83e\uddb8\u200d\u2640\ufe0f","text":"<ul> <li>Nguyen Xuan Duc </li> <li>Nguyen Huu Thanh</li> <li>Nguyen Thanh Duc</li> <li>Pham Hong Quang</li> <li>Thang Pham Ngoc</li> <li>Hoai-Thu Vuong</li> </ul>"},{"location":"contribute/SUPPORT_US/","title":"\ud83d\udc9d Support Us","text":"<p>\ud83c\udf0a Underthesea has been dedicatedly working on Open Source Vietnamese Natural Lanugage Processing for the past six years. Our team of developers and researchers have been exploring various AI techniques to create tools that can simplify and automate complex tasks, thereby empowering businesses to thrive in the AI age.</p> <p>If you found this project helpful and would like to support our work, you can just buy us a coffee \u2615.</p> <p>Your support is our biggest encouragement \ud83c\udf81!</p> <p></p>"},{"location":"developer/architecture/","title":"Architecture","text":"<p>This document describes the internal architecture of Underthesea.</p>"},{"location":"developer/architecture/#overview","title":"Overview","text":"<p>Underthesea is organized as a collection of NLP pipelines, each handling a specific task.</p> <pre><code>underthesea/\n\u251c\u2500\u2500 pipeline/              # Main NLP modules\n\u2502   \u251c\u2500\u2500 sent_tokenize/     # Sentence segmentation\n\u2502   \u251c\u2500\u2500 text_normalize/    # Text normalization\n\u2502   \u251c\u2500\u2500 word_tokenize/     # Word segmentation\n\u2502   \u251c\u2500\u2500 pos_tag/           # POS tagging\n\u2502   \u251c\u2500\u2500 chunking/          # Phrase chunking\n\u2502   \u251c\u2500\u2500 dependency_parse/  # Dependency parsing\n\u2502   \u251c\u2500\u2500 ner/               # Named entity recognition\n\u2502   \u251c\u2500\u2500 classification/    # Text classification\n\u2502   \u251c\u2500\u2500 sentiment/         # Sentiment analysis\n\u2502   \u251c\u2500\u2500 translate/         # Translation\n\u2502   \u251c\u2500\u2500 lang_detect/       # Language detection\n\u2502   \u2514\u2500\u2500 tts/               # Text-to-speech\n\u251c\u2500\u2500 models/                # Model implementations\n\u251c\u2500\u2500 datasets/              # Built-in datasets\n\u251c\u2500\u2500 corpus/                # Corpus handling\n\u251c\u2500\u2500 resources/             # Static resources\n\u2514\u2500\u2500 cli.py                 # CLI interface\n</code></pre>"},{"location":"developer/architecture/#pipeline-module-structure","title":"Pipeline Module Structure","text":"<p>Each pipeline module follows a consistent pattern:</p> <pre><code>pipeline/word_tokenize/\n\u251c\u2500\u2500 __init__.py            # Main API function\n\u251c\u2500\u2500 model.py               # Model implementation\n\u251c\u2500\u2500 feature.py             # Feature extraction\n\u2514\u2500\u2500 default_model/         # Default model files\n</code></pre>"},{"location":"developer/architecture/#main-api-__init__py","title":"Main API (<code>__init__.py</code>)","text":"<pre><code># Lazy loading pattern\n_model = None\n\ndef word_tokenize(sentence, format=None):\n    global _model\n    if _model is None:\n        _model = load_model()\n    return _model.predict(sentence, format)\n</code></pre>"},{"location":"developer/architecture/#model-implementation","title":"Model Implementation","text":"<pre><code>class CRFModel:\n    def __init__(self, model_path):\n        self.model = load_crf(model_path)\n\n    def predict(self, text):\n        features = extract_features(text)\n        return self.model.tag(features)\n</code></pre>"},{"location":"developer/architecture/#lazy-loading","title":"Lazy Loading","text":"<p>Models are loaded on first use to minimize startup time:</p> <pre><code># At import time - no model loaded\nfrom underthesea import word_tokenize\n\n# First call - model loaded and cached\nresult = word_tokenize(\"text\")\n\n# Subsequent calls - uses cached model\nresult = word_tokenize(\"more text\")\n</code></pre> <p>Benefits: - Fast import time - Memory efficiency (only used models loaded) - Simple API</p>"},{"location":"developer/architecture/#model-types","title":"Model Types","text":""},{"location":"developer/architecture/#crf-models","title":"CRF Models","text":"<p>Used for: word segmentation, POS tagging, chunking, NER, classification, sentiment</p> <pre><code># Uses python-crfsuite\nimport pycrfsuite\n\nclass CRFTagger:\n    def __init__(self, model_path):\n        self.tagger = pycrfsuite.Tagger()\n        self.tagger.open(model_path)\n\n    def tag(self, features):\n        return self.tagger.tag(features)\n</code></pre>"},{"location":"developer/architecture/#deep-learning-models","title":"Deep Learning Models","text":"<p>Used for: dependency parsing, deep NER, translation</p> <pre><code># Uses transformers\nfrom transformers import AutoModel, AutoTokenizer\n\nclass TransformerModel:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n</code></pre>"},{"location":"developer/architecture/#fasttext-models","title":"FastText Models","text":"<p>Used for: language detection</p> <pre><code>import fasttext\n\nclass LangDetector:\n    def __init__(self, model_path):\n        self.model = fasttext.load_model(model_path)\n\n    def detect(self, text):\n        prediction = self.model.predict(text)\n        return prediction[0][0].replace('__label__', '')\n</code></pre>"},{"location":"developer/architecture/#feature-extraction","title":"Feature Extraction","text":"<p>Features are extracted for CRF models:</p> <pre><code>def extract_features(sentence):\n    features = []\n    for i, word in enumerate(sentence):\n        word_features = {\n            'word': word,\n            'is_upper': word.isupper(),\n            'is_title': word.istitle(),\n            'prev_word': sentence[i-1] if i &gt; 0 else 'BOS',\n            'next_word': sentence[i+1] if i &lt; len(sentence)-1 else 'EOS',\n        }\n        features.append(word_features)\n    return features\n</code></pre>"},{"location":"developer/architecture/#resource-management","title":"Resource Management","text":""},{"location":"developer/architecture/#model-storage","title":"Model Storage","text":"<p>Models are stored in <code>~/.underthesea/models/</code>:</p> <pre><code>~/.underthesea/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 WS_VLSP2013_CRF/\n\u2502   \u251c\u2500\u2500 POS_VLSP2013_CRF/\n\u2502   \u2514\u2500\u2500 NER_VLSP2016_BERT/\n\u2514\u2500\u2500 datasets/\n    \u251c\u2500\u2500 VNTC/\n    \u2514\u2500\u2500 UTS2017-BANK/\n</code></pre>"},{"location":"developer/architecture/#model-download","title":"Model Download","text":"<pre><code>def download_model(model_name):\n    url = get_model_url(model_name)\n    local_path = get_local_path(model_name)\n\n    if not os.path.exists(local_path):\n        download_file(url, local_path)\n        extract_archive(local_path)\n\n    return local_path\n</code></pre>"},{"location":"developer/architecture/#rust-extension","title":"Rust Extension","text":"<p>Performance-critical code uses the Rust extension:</p> <pre><code>extensions/underthesea_core/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 lib.rs             # Rust implementation\n\u251c\u2500\u2500 Cargo.toml             # Rust dependencies\n\u2514\u2500\u2500 pyproject.toml         # Python binding config\n</code></pre> <p>Built with maturin:</p> <pre><code>cd extensions/underthesea_core\nmaturin develop\n</code></pre>"},{"location":"developer/architecture/#cli-architecture","title":"CLI Architecture","text":"<p>The CLI uses Click:</p> <pre><code># cli.py\nimport click\n\n@click.group()\ndef cli():\n    pass\n\n@cli.command()\ndef list_data():\n    \"\"\"List available datasets.\"\"\"\n    for dataset in get_datasets():\n        print(dataset)\n\n@cli.command()\n@click.argument('text')\ndef tts(text):\n    \"\"\"Convert text to speech.\"\"\"\n    from underthesea.pipeline.tts import tts\n    tts(text)\n</code></pre>"},{"location":"developer/architecture/#optional-dependencies","title":"Optional Dependencies","text":"<p>Optional features are guarded:</p> <pre><code>def translate(text):\n    try:\n        from transformers import AutoModel\n    except ImportError:\n        raise ImportError(\n            \"Translation requires deep learning dependencies. \"\n            \"Install with: pip install 'underthesea[deep]'\"\n        )\n    # ... translation logic\n</code></pre>"},{"location":"developer/architecture/#testing-architecture","title":"Testing Architecture","text":"<pre><code>tests/\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 word_tokenize/\n\u2502   \u2502   \u2514\u2500\u2500 test_word_tokenize.py\n\u2502   \u251c\u2500\u2500 pos_tag/\n\u2502   \u2502   \u2514\u2500\u2500 test_pos_tag.py\n\u2502   \u2514\u2500\u2500 ner/\n\u2502       \u2514\u2500\u2500 test_ner.py\n\u2514\u2500\u2500 conftest.py            # Pytest fixtures\n</code></pre>"},{"location":"developer/architecture/#extending-underthesea","title":"Extending Underthesea","text":""},{"location":"developer/architecture/#adding-a-new-pipeline","title":"Adding a New Pipeline","text":"<ol> <li>Create directory: <code>underthesea/pipeline/new_task/</code></li> <li>Implement <code>__init__.py</code> with main API</li> <li>Add model implementation</li> <li>Export from <code>underthesea/__init__.py</code></li> <li>Add tests in <code>tests/pipeline/new_task/</code></li> <li>Add documentation</li> </ol>"},{"location":"developer/architecture/#adding-a-new-model","title":"Adding a New Model","text":"<ol> <li>Train the model using appropriate toolkit</li> <li>Save model files</li> <li>Update model registry</li> <li>Add download logic</li> <li>Test with existing pipeline</li> </ol>"},{"location":"developer/contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Underthesea! This guide will help you get started.</p>"},{"location":"developer/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"developer/contributing/#bug-reports","title":"Bug Reports","text":"<ul> <li>Check existing GitHub Issues first</li> <li>Include Python version, OS, and Underthesea version</li> <li>Provide minimal code to reproduce the issue</li> <li>Include full error traceback</li> </ul>"},{"location":"developer/contributing/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Reference the issue number in your PR</li> <li>Include tests that demonstrate the fix</li> <li>Update documentation if needed</li> </ul>"},{"location":"developer/contributing/#new-features","title":"New Features","text":"<ul> <li>Open an issue to discuss the feature first</li> <li>Follow the existing code style</li> <li>Add tests and documentation</li> </ul>"},{"location":"developer/contributing/#documentation","title":"Documentation","text":"<ul> <li>Fix typos and improve clarity</li> <li>Add examples and tutorials</li> <li>Translate documentation</li> </ul>"},{"location":"developer/contributing/#development-setup","title":"Development Setup","text":""},{"location":"developer/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"developer/contributing/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/undertheseanlp/underthesea.git\ncd underthesea\n\n# Create virtual environment with uv\nuv venv\nsource .venv/bin/activate\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"developer/contributing/#macos-arm64-apple-silicon","title":"macOS ARM64 (Apple Silicon)","text":"<p>Build the Rust extension:</p> <pre><code>cd extensions/underthesea_core\nuv pip install maturin\nmaturin develop\ncd ../..\n</code></pre>"},{"location":"developer/contributing/#code-style","title":"Code Style","text":""},{"location":"developer/contributing/#linting","title":"Linting","text":"<p>We use Ruff for linting:</p> <pre><code># Check for issues\nruff check underthesea/\n\n# Auto-fix issues\nruff check underthesea/ --fix\n</code></pre>"},{"location":"developer/contributing/#configuration","title":"Configuration","text":"<p>Ruff configuration is in <code>pyproject.toml</code>.</p>"},{"location":"developer/contributing/#testing","title":"Testing","text":""},{"location":"developer/contributing/#test-categories","title":"Test Categories","text":"Command Description <code>tox -e lint</code> Linting with Ruff <code>tox -e core</code> Core module tests <code>tox -e deep</code> Deep learning tests <code>tox -e prompt</code> Prompt model tests <code>tox -e langdetect</code> Language detection tests"},{"location":"developer/contributing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Word tokenization tests\nuv run python -m unittest discover tests.pipeline.word_tokenize\n\n# POS tagging tests\nuv run python -m unittest discover tests.pipeline.pos_tag\n\n# NER tests\nuv run python -m unittest tests.pipeline.ner.test_ner\n\n# Classification tests\nuv run python -m unittest tests.pipeline.classification.test_bank\n\n# Translation tests\nuv run python -m unittest discover tests.pipeline.translate\n</code></pre>"},{"location":"developer/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Mirror the source structure</li> <li>Use Python's <code>unittest</code> framework</li> <li>Include both positive and edge case tests</li> </ul> <pre><code>import unittest\nfrom underthesea import word_tokenize\n\nclass TestWordTokenize(unittest.TestCase):\n    def test_basic(self):\n        result = word_tokenize(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n        self.assertEqual(result, ['Xin', 'ch\u00e0o', 'Vi\u1ec7t Nam'])\n\n    def test_empty_string(self):\n        result = word_tokenize(\"\")\n        self.assertEqual(result, [])\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"developer/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"developer/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Update your branch: Rebase on latest <code>main</code></li> <li>Run linting: <code>ruff check underthesea/</code></li> <li>Run tests: <code>tox -e core</code></li> <li>Update docs: If adding features</li> </ol>"},{"location":"developer/contributing/#pr-guidelines","title":"PR Guidelines","text":"<ul> <li>Use clear, descriptive titles</li> <li>Reference related issues</li> <li>Describe changes and motivation</li> <li>Include test results</li> <li>Add screenshots for UI changes</li> </ul>"},{"location":"developer/contributing/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Related Issues\nFixes #123\n\n## Changes\n- Added X feature\n- Fixed Y bug\n- Updated Z documentation\n\n## Testing\n- [ ] Linting passes\n- [ ] Unit tests pass\n- [ ] Manual testing done\n\n## Documentation\n- [ ] Updated relevant docs\n- [ ] Added docstrings\n</code></pre>"},{"location":"developer/contributing/#project-structure","title":"Project Structure","text":"<pre><code>underthesea/\n\u251c\u2500\u2500 underthesea/           # Main package\n\u2502   \u251c\u2500\u2500 pipeline/          # NLP modules\n\u2502   \u251c\u2500\u2500 models/            # Model implementations\n\u2502   \u251c\u2500\u2500 datasets/          # Built-in datasets\n\u2502   \u251c\u2500\u2500 corpus/            # Corpus handling\n\u2502   \u2514\u2500\u2500 cli.py             # CLI commands\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 extensions/            # Rust extension, apps\n\u2514\u2500\u2500 pyproject.toml         # Project configuration\n</code></pre>"},{"location":"developer/contributing/#cli-commands","title":"CLI Commands","text":"<pre><code># List available data\nunderthesea list-data\n\n# List available models\nunderthesea list-model\n\n# Download data\nunderthesea download-data VNTC\n</code></pre>"},{"location":"developer/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> <li>Facebook Community</li> </ul>"},{"location":"developer/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers feel welcome</li> </ul>"},{"location":"developer/releasing/","title":"Releasing","text":"<p>This document describes the release process for Underthesea.</p>"},{"location":"developer/releasing/#version-scheme","title":"Version Scheme","text":"<p>Underthesea follows Semantic Versioning:</p> <ul> <li>MAJOR.MINOR.PATCH (e.g., 9.0.0)</li> <li>MAJOR: Breaking changes</li> <li>MINOR: New features, backward compatible</li> <li>PATCH: Bug fixes, backward compatible</li> </ul>"},{"location":"developer/releasing/#release-checklist","title":"Release Checklist","text":""},{"location":"developer/releasing/#pre-release","title":"Pre-release","text":"<ul> <li>[ ] All tests passing</li> <li>[ ] Documentation updated</li> <li>[ ] CHANGELOG updated</li> <li>[ ] Version bumped in <code>pyproject.toml</code></li> <li>[ ] Version bumped in <code>__init__.py</code></li> </ul>"},{"location":"developer/releasing/#release-steps","title":"Release Steps","text":"<ol> <li>Update version</li> </ol> <pre><code># underthesea/__init__.py\n__version__ = \"9.1.0\"\n</code></pre> <pre><code># pyproject.toml\n[project]\nversion = \"9.1.0\"\n</code></pre> <ol> <li>Update CHANGELOG</li> </ol> <p>Add entry to <code>docs/history.md</code>:</p> <pre><code>## 9.1.0 (2024-XX-XX)\n\n### New Features\n- Added X feature\n\n### Bug Fixes\n- Fixed Y issue\n\n### Documentation\n- Updated Z docs\n</code></pre> <ol> <li>Run tests</li> </ol> <pre><code>tox -e lint\ntox -e core\n</code></pre> <ol> <li>Create release commit</li> </ol> <pre><code>git add -A\ngit commit -m \"Release version 9.1.0\"\ngit tag v9.1.0\n</code></pre> <ol> <li>Push to GitHub</li> </ol> <pre><code>git push origin main\ngit push origin v9.1.0\n</code></pre> <ol> <li>Publish to PyPI</li> </ol> <p>GitHub Actions will automatically publish when a tag is pushed.</p> <p>Or manually:</p> <pre><code>uv pip install build twine\npython -m build\ntwine upload dist/*\n</code></pre> <ol> <li> <p>Create GitHub Release</p> </li> <li> <p>Go to Releases</p> </li> <li>Click \"Draft a new release\"</li> <li>Select the tag</li> <li>Add release notes</li> <li>Publish</li> </ol>"},{"location":"developer/releasing/#versioning-guidelines","title":"Versioning Guidelines","text":""},{"location":"developer/releasing/#when-to-bump-major","title":"When to bump MAJOR","text":"<ul> <li>Removing a public function</li> <li>Changing function signatures</li> <li>Dropping Python version support</li> <li>Breaking changes to output format</li> </ul>"},{"location":"developer/releasing/#when-to-bump-minor","title":"When to bump MINOR","text":"<ul> <li>Adding new functions</li> <li>Adding new parameters (with defaults)</li> <li>New model support</li> <li>New optional features</li> </ul>"},{"location":"developer/releasing/#when-to-bump-patch","title":"When to bump PATCH","text":"<ul> <li>Bug fixes</li> <li>Performance improvements</li> <li>Documentation updates</li> <li>Dependency updates</li> </ul>"},{"location":"developer/releasing/#hotfix-process","title":"Hotfix Process","text":"<p>For urgent bug fixes:</p> <ol> <li> <p>Create branch from latest release tag:    <pre><code>git checkout -b hotfix/9.0.1 v9.0.0\n</code></pre></p> </li> <li> <p>Fix the bug and add tests</p> </li> <li> <p>Bump PATCH version</p> </li> <li> <p>Merge to main and tag:    <pre><code>git checkout main\ngit merge hotfix/9.0.1\ngit tag v9.0.1\ngit push origin main v9.0.1\n</code></pre></p> </li> </ol>"},{"location":"developer/releasing/#release-automation","title":"Release Automation","text":"<p>GitHub Actions workflow (<code>.github/workflows/publish.yml</code>):</p> <pre><code>name: Publish to PyPI\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n      - run: pip install build twine\n      - run: python -m build\n      - run: twine upload dist/*\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}\n</code></pre>"},{"location":"developer/releasing/#post-release","title":"Post-release","text":"<ul> <li>[ ] Verify package on PyPI</li> <li>[ ] Test installation: <code>pip install underthesea==9.1.0</code></li> <li>[ ] Update ReadTheDocs if needed</li> <li>[ ] Announce on social media</li> <li>[ ] Close related GitHub issues</li> </ul>"},{"location":"technical_reports/voice/","title":"Voice Module: Technical Report","text":"<p>This document provides a technical overview of the AI models used in the underthesea voice (text-to-speech) module.</p>"},{"location":"technical_reports/voice/#overview","title":"Overview","text":"<p>The voice module implements a neural text-to-speech (TTS) system for Vietnamese. It is based on VietTTS by NTT123 and uses a two-stage architecture:</p> <ol> <li>Text-to-Mel: Converts text/phonemes to mel-spectrogram</li> <li>Mel-to-Wave (Vocoder): Converts mel-spectrogram to audio waveform</li> </ol> <pre><code>Text \u2192 [Text Normalization] \u2192 [Duration Model] \u2192 [Acoustic Model] \u2192 Mel \u2192 [HiFi-GAN] \u2192 Audio\n</code></pre>"},{"location":"technical_reports/voice/#installation","title":"Installation","text":"<pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"technical_reports/voice/#model-architecture","title":"Model Architecture","text":""},{"location":"technical_reports/voice/#1-duration-model","title":"1. Duration Model","text":"<p>The Duration Model predicts the duration (in seconds) for each phoneme in the input sequence.</p> <p>Architecture:</p> Component Description Token Encoder Embedding + 3\u00d7 Conv1D + Bidirectional LSTM Projection Linear \u2192 GELU \u2192 Linear \u2192 Softplus <p>Parameters:</p> Parameter Value Vocabulary Size 256 LSTM Dimension 256 Dropout Rate 0.5 <p>Input: Phoneme sequence with lengths Output: Duration for each phoneme (in seconds)</p>"},{"location":"technical_reports/voice/#2-acoustic-model","title":"2. Acoustic Model","text":"<p>The Acoustic Model generates mel-spectrograms from phonemes and their predicted durations.</p> <p>Architecture:</p> Component Description Token Encoder Embedding + 3\u00d7 Conv1D + Bidirectional LSTM Upsampling Gaussian attention-based upsampling PreNet 2\u00d7 Linear (256 dim) with dropout Decoder 2\u00d7 LSTM with skip connections PostNet 5\u00d7 Conv1D with batch normalization Projection Linear to mel dimension <p>Parameters:</p> Parameter Value Encoder Dimension 256 Decoder Dimension 512 PostNet Dimension 512 Mel Dimension 80 <p>Key Features:</p> <ul> <li>Gaussian Upsampling: Uses soft attention to upsample encoder outputs to match target frame length</li> <li>Autoregressive Decoder: Generates mel frames sequentially with teacher forcing during training</li> <li>Zoneout Regularization: Applies zoneout to LSTM states during training for better generalization</li> <li>PostNet Refinement: Residual convolutional network refines the predicted mel-spectrogram</li> </ul>"},{"location":"technical_reports/voice/#3-hifi-gan-vocoder","title":"3. HiFi-GAN Vocoder","text":"<p>The vocoder converts mel-spectrograms to raw audio waveforms using the HiFi-GAN architecture.</p> <p>Architecture:</p> Component Description Conv Pre Conv1D (7 kernel) Upsampling Multiple Conv1DTranspose layers Multi-Receptive Field Fusion (MRF) ResBlocks with varying kernel sizes and dilations Conv Post Conv1D (7 kernel) + Tanh <p>Key Features:</p> <ul> <li>Multi-Scale Upsampling: Progressive upsampling from mel frame rate to audio sample rate</li> <li>Multi-Receptive Field Fusion: Combines outputs from residual blocks with different receptive fields</li> <li>Leaky ReLU Activation: Uses leaky ReLU with slope 0.1 throughout</li> </ul> <p>ResBlock Types:</p> <ul> <li>ResBlock1: 3 dilated convolutions (dilation: 1, 3, 5) with residual connections</li> <li>ResBlock2: 2 dilated convolutions (dilation: 1, 3) with residual connections</li> </ul>"},{"location":"technical_reports/voice/#audio-configuration","title":"Audio Configuration","text":"Parameter Value Sample Rate 16,000 Hz FFT Size 1,024 Mel Channels 80 Frequency Range 0 - 8,000 Hz"},{"location":"technical_reports/voice/#text-processing-pipeline","title":"Text Processing Pipeline","text":""},{"location":"technical_reports/voice/#1-text-normalization","title":"1. Text Normalization","text":"<p>The input text is normalized before synthesis:</p> <pre><code># Normalization steps:\n1. Unicode NFKC normalization\n2. Lowercase conversion\n3. Punctuation \u2192 silence markers\n4. Multiple spaces \u2192 single space\n</code></pre>"},{"location":"technical_reports/voice/#2-phoneme-conversion","title":"2. Phoneme Conversion","text":"<p>Text is converted to phonemes using a lexicon lookup:</p> <ul> <li>Vietnamese characters are mapped to phoneme sequences</li> <li>Special tokens: <code>sil</code> (silence), <code>sp</code> (short pause), <code></code> (word boundary)</li> <li>Unknown words are processed character-by-character</li> </ul>"},{"location":"technical_reports/voice/#model-files","title":"Model Files","text":"<p>The <code>VIET_TTS_V0_4_1</code> model package includes:</p> File Description <code>lexicon.txt</code> Word-to-phoneme mapping <code>duration_latest_ckpt.pickle</code> Duration model weights <code>acoustic_latest_ckpt.pickle</code> Acoustic model weights <code>hk_hifi.pickle</code> HiFi-GAN vocoder weights <code>config.json</code> HiFi-GAN configuration"},{"location":"technical_reports/voice/#framework-dependencies","title":"Framework Dependencies","text":"<p>The voice module uses JAX ecosystem:</p> Library Purpose JAX Numerical computation and automatic differentiation JAXlib JAX backend (CPU/GPU/TPU support) dm-haiku Neural network library for JAX Optax Gradient processing and optimization"},{"location":"technical_reports/voice/#usage-example","title":"Usage Example","text":"<pre><code>from underthesea.pipeline.tts import tts\n\n# Basic usage\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")  # Creates sound.wav\n\n# Custom output file\ntts(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4\", outfile=\"output.wav\")\n\n# With playback\ntts(\"\u0110\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5\", play=True)\n</code></pre>"},{"location":"technical_reports/voice/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>First Call Latency: Model loading on first call may take several seconds</li> <li>JAX Compilation: JIT compilation occurs on first inference, subsequent calls are faster</li> <li>Text Length: Maximum recommended text length is 500 characters</li> <li>Memory Usage: GPU memory usage depends on input text length</li> </ul>"},{"location":"technical_reports/voice/#limitations","title":"Limitations","text":"<ul> <li>Single Speaker: Current model supports only one voice</li> <li>Vietnamese Only: Designed specifically for Vietnamese language</li> <li>Prosody: Limited control over prosody and emotion</li> <li>Real-time: Not optimized for real-time streaming</li> </ul>"},{"location":"technical_reports/voice/#references","title":"References","text":"<ul> <li>VietTTS - Original implementation by NTT123</li> <li>HiFi-GAN - Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</li> <li>Non-Attentive Tacotron - Robust and Controllable Neural TTS Synthesis</li> <li>dm-haiku - JAX neural network library by DeepMind</li> </ul>"},{"location":"tutorials/ner/","title":"Named Entity Recognition Tutorial","text":"<p>Learn how to identify and extract named entities from Vietnamese text.</p>"},{"location":"tutorials/ner/#introduction","title":"Introduction","text":"<p>Named Entity Recognition (NER) identifies and classifies named entities in text into predefined categories such as:</p> <ul> <li>PER: Person names</li> <li>LOC: Locations (countries, cities, addresses)</li> <li>ORG: Organizations (companies, institutions)</li> </ul>"},{"location":"tutorials/ner/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\nprint(entities)\n</code></pre> <p>Output: <pre><code>[('Ch\u01b0a', 'R', 'O', 'O'),\n ('ti\u1ebft l\u1ed9', 'V', 'B-VP', 'O'),\n ('l\u1ecbch tr\u00ecnh', 'V', 'B-VP', 'O'),\n ('t\u1edbi', 'E', 'B-PP', 'O'),\n ('Vi\u1ec7t Nam', 'Np', 'B-NP', 'B-LOC'),\n ('c\u1ee7a', 'E', 'B-PP', 'O'),\n ('T\u1ed5ng th\u1ed1ng', 'N', 'B-NP', 'O'),\n ('M\u1ef9', 'Np', 'B-NP', 'B-LOC'),\n ('Donald', 'Np', 'B-NP', 'B-PER'),\n ('Trump', 'Np', 'B-NP', 'I-PER')]\n</code></pre></p>"},{"location":"tutorials/ner/#understanding-the-output","title":"Understanding the Output","text":"<p>Each tuple contains: 1. Word: The token 2. POS: Part-of-speech tag 3. Chunk: Phrase chunk tag 4. Entity: Named entity tag</p>"},{"location":"tutorials/ner/#entity-tags-bio-format","title":"Entity Tags (BIO Format)","text":"Tag Meaning <code>B-PER</code> Beginning of Person <code>I-PER</code> Inside Person <code>B-LOC</code> Beginning of Location <code>I-LOC</code> Inside Location <code>B-ORG</code> Beginning of Organization <code>I-ORG</code> Inside Organization <code>O</code> Outside (not an entity)"},{"location":"tutorials/ner/#extracting-entities","title":"Extracting Entities","text":""},{"location":"tutorials/ner/#simple-extraction","title":"Simple Extraction","text":"<pre><code>from underthesea import ner\n\ntext = \"T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump th\u0103m Vi\u1ec7t Nam\"\nentities = ner(text)\n\n# Extract only named entities\nfor word, pos, chunk, entity in entities:\n    if entity != 'O':\n        print(f\"{word}: {entity}\")\n# M\u1ef9: B-LOC\n# Donald: B-PER\n# Trump: I-PER\n# Vi\u1ec7t Nam: B-LOC\n</code></pre>"},{"location":"tutorials/ner/#grouping-multi-word-entities","title":"Grouping Multi-word Entities","text":"<pre><code>def extract_entities(text):\n    \"\"\"Extract and group multi-word entities.\"\"\"\n    results = ner(text)\n\n    entities = []\n    current_entity = []\n    current_type = None\n\n    for word, pos, chunk, entity in results:\n        if entity.startswith('B-'):\n            # Save previous entity if exists\n            if current_entity:\n                entities.append((' '.join(current_entity), current_type))\n            # Start new entity\n            current_entity = [word]\n            current_type = entity[2:]  # Remove 'B-' prefix\n        elif entity.startswith('I-'):\n            # Continue current entity\n            current_entity.append(word)\n        else:\n            # End current entity\n            if current_entity:\n                entities.append((' '.join(current_entity), current_type))\n                current_entity = []\n                current_type = None\n\n    # Don't forget the last entity\n    if current_entity:\n        entities.append((' '.join(current_entity), current_type))\n\n    return entities\n\ntext = \"T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump th\u0103m Vi\u1ec7t Nam v\u00e0o th\u00e1ng 11\"\nprint(extract_entities(text))\n# [('M\u1ef9', 'LOC'), ('Donald Trump', 'PER'), ('Vi\u1ec7t Nam', 'LOC')]\n</code></pre>"},{"location":"tutorials/ner/#filtering-by-entity-type","title":"Filtering by Entity Type","text":"<pre><code>from underthesea import ner\n\ntext = \"\u00d4ng Nguy\u1ec5n V\u0103n A t\u1eeb C\u00f4ng ty ABC \u1edf H\u00e0 N\u1ed9i \u0111\u1ebfn th\u0103m \u0110\u00e0 N\u1eb5ng\"\nentities = ner(text)\n\n# Extract by type\npersons = [w for w, p, c, e in entities if e.endswith('PER')]\nlocations = [w for w, p, c, e in entities if e.endswith('LOC')]\norganizations = [w for w, p, c, e in entities if e.endswith('ORG')]\n\nprint(f\"Persons: {persons}\")\nprint(f\"Locations: {locations}\")\nprint(f\"Organizations: {organizations}\")\n</code></pre>"},{"location":"tutorials/ner/#deep-learning-model","title":"Deep Learning Model","text":"<p>For better accuracy, use the deep learning model:</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>from underthesea import ner\n\ntext = \"B\u1ed9 C\u00f4ng Th\u01b0\u01a1ng x\u00f3a m\u1ed9t t\u1ed5ng c\u1ee5c, gi\u1ea3m nhi\u1ec1u \u0111\u1ea7u m\u1ed1i\"\nentities = ner(text, deep=True)\nprint(entities)\n# [\n#   {'entity': 'B-ORG', 'word': 'B\u1ed9'},\n#   {'entity': 'I-ORG', 'word': 'C\u00f4ng'},\n#   {'entity': 'I-ORG', 'word': 'Th\u01b0\u01a1ng'}\n# ]\n</code></pre>"},{"location":"tutorials/ner/#practical-applications","title":"Practical Applications","text":""},{"location":"tutorials/ner/#news-article-analysis","title":"News Article Analysis","text":"<pre><code>from underthesea import ner\nfrom collections import defaultdict\n\narticle = \"\"\"\nTh\u1ee7 t\u01b0\u1edbng Ph\u1ea1m Minh Ch\u00ednh \u0111\u00e3 g\u1eb7p T\u1ed5ng th\u1ed1ng M\u1ef9 Joe Biden t\u1ea1i Washington.\nCu\u1ed9c g\u1eb7p di\u1ec5n ra t\u1ea1i Nh\u00e0 Tr\u1eafng v\u00e0o ng\u00e0y 25 th\u00e1ng 9.\n\u0110\u1ea1i s\u1ee9 qu\u00e1n Vi\u1ec7t Nam t\u1ea1i M\u1ef9 \u0111\u00e3 t\u1ed5 ch\u1ee9c bu\u1ed5i ti\u1ebfp \u0111\u00f3n.\n\"\"\"\n\n# Analyze entities\nentities_by_type = defaultdict(set)\n\nfor sentence in article.strip().split('\\n'):\n    if sentence:\n        for word, pos, chunk, entity in ner(sentence):\n            if entity != 'O':\n                entity_type = entity.split('-')[1]\n                entities_by_type[entity_type].add(word)\n\nprint(\"Entities found:\")\nfor entity_type, words in entities_by_type.items():\n    print(f\"  {entity_type}: {', '.join(words)}\")\n</code></pre>"},{"location":"tutorials/ner/#building-an-entity-database","title":"Building an Entity Database","text":"<pre><code>from underthesea import ner\n\ndef build_entity_database(documents):\n    \"\"\"Build a database of entities from documents.\"\"\"\n    database = {\n        'PER': set(),\n        'LOC': set(),\n        'ORG': set()\n    }\n\n    for doc in documents:\n        entities = ner(doc)\n        for word, pos, chunk, entity in entities:\n            if entity.endswith('PER'):\n                database['PER'].add(word)\n            elif entity.endswith('LOC'):\n                database['LOC'].add(word)\n            elif entity.endswith('ORG'):\n                database['ORG'].add(word)\n\n    return database\n\ndocs = [\n    \"Vi\u1ec7t Nam v\u00e0 M\u1ef9 t\u0103ng c\u01b0\u1eddng h\u1ee3p t\u00e1c\",\n    \"C\u00f4ng ty Samsung \u0111\u1ea7u t\u01b0 t\u1ea1i B\u1eafc Ninh\",\n    \"\u00d4ng Nguy\u1ec5n V\u0103n A \u0111\u01b0\u1ee3c b\u1ed5 nhi\u1ec7m l\u00e0m gi\u00e1m \u0111\u1ed1c\"\n]\n\ndb = build_entity_database(docs)\nprint(db)\n</code></pre>"},{"location":"tutorials/ner/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Text Classification</li> <li>Explore Sentiment Analysis</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/pos_tagging/","title":"POS Tagging Tutorial","text":"<p>Learn how to perform Part-of-Speech tagging on Vietnamese text.</p>"},{"location":"tutorials/pos_tagging/#introduction","title":"Introduction","text":"<p>Part-of-Speech (POS) tagging assigns grammatical labels (noun, verb, adjective, etc.) to each word in a sentence. This is essential for understanding the structure of text.</p>"},{"location":"tutorials/pos_tagging/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nprint(tagged)\n# [('Ch\u1ee3', 'N'), ('th\u1ecbt', 'N'), ('ch\u00f3', 'N'), ('n\u1ed5i ti\u1ebfng', 'A'),\n#  ('\u1edf', 'E'), ('S\u00e0i G\u00f2n', 'Np'), ('b\u1ecb', 'V'), ('truy qu\u00e9t', 'V')]\n</code></pre>"},{"location":"tutorials/pos_tagging/#understanding-pos-tags","title":"Understanding POS Tags","text":"Tag Meaning Examples <code>N</code> Common noun ch\u1ee3, th\u1ecbt, ch\u00f3, s\u00e1ch <code>Np</code> Proper noun Vi\u1ec7t Nam, H\u00e0 N\u1ed9i, Trump <code>V</code> Verb ch\u1ea1y, \u0103n, n\u00f3i, b\u1ecb <code>A</code> Adjective \u0111\u1eb9p, n\u1ed5i ti\u1ebfng, l\u1edbn <code>P</code> Pronoun t\u00f4i, b\u1ea1n, n\u00f3, h\u1ecd <code>R</code> Adverb r\u1ea5t, \u0111ang, s\u1ebd, \u0111\u00e3 <code>E</code> Preposition \u1edf, trong, tr\u00ean, d\u01b0\u1edbi <code>C</code> Conjunction v\u00e0, ho\u1eb7c, nh\u01b0ng <code>M</code> Number m\u1ed9t, hai, ba, 100 <code>L</code> Determiner c\u00e1c, nh\u1eefng, m\u1ecdi <code>CH</code> Punctuation . , ? !"},{"location":"tutorials/pos_tagging/#practical-examples","title":"Practical Examples","text":""},{"location":"tutorials/pos_tagging/#extracting-nouns","title":"Extracting Nouns","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Sinh vi\u00ean \u0110\u1ea1i h\u1ecdc B\u00e1ch Khoa H\u00e0 N\u1ed9i \u0111\u1ea1t gi\u1ea3i nh\u1ea5t cu\u1ed9c thi l\u1eadp tr\u00ecnh\"\ntagged = pos_tag(text)\n\n# Extract all nouns\nnouns = [word for word, tag in tagged if tag in ('N', 'Np')]\nprint(\"Nouns:\", nouns)\n# ['Sinh vi\u00ean', '\u0110\u1ea1i h\u1ecdc', 'B\u00e1ch Khoa', 'H\u00e0 N\u1ed9i', 'gi\u1ea3i', 'cu\u1ed9c thi', 'l\u1eadp tr\u00ecnh']\n</code></pre>"},{"location":"tutorials/pos_tagging/#extracting-verbs","title":"Extracting Verbs","text":"<pre><code>text = \"Anh \u1ea5y ch\u1ea1y nhanh v\u00e0 nh\u1ea3y cao\"\ntagged = pos_tag(text)\n\nverbs = [word for word, tag in tagged if tag == 'V']\nprint(\"Verbs:\", verbs)\n# ['ch\u1ea1y', 'nh\u1ea3y']\n</code></pre>"},{"location":"tutorials/pos_tagging/#finding-proper-nouns-names-locations","title":"Finding Proper Nouns (Names, Locations)","text":"<pre><code>text = \"T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump th\u0103m Vi\u1ec7t Nam v\u00e0o th\u00e1ng 11\"\ntagged = pos_tag(text)\n\nproper_nouns = [word for word, tag in tagged if tag == 'Np']\nprint(\"Proper nouns:\", proper_nouns)\n# ['M\u1ef9', 'Donald', 'Trump', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"tutorials/pos_tagging/#analyzing-sentence-structure","title":"Analyzing Sentence Structure","text":"<pre><code>from collections import Counter\n\ntext = \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 xinh \u0111\u1eb9p c\u1ee7a Vi\u1ec7t Nam v\u1edbi nhi\u1ec1u \u0111\u1ecba \u0111i\u1ec3m du l\u1ecbch n\u1ed5i ti\u1ebfng\"\ntagged = pos_tag(text)\n\n# Count POS distribution\npos_counts = Counter(tag for word, tag in tagged)\nprint(\"POS Distribution:\")\nfor tag, count in pos_counts.most_common():\n    print(f\"  {tag}: {count}\")\n</code></pre>"},{"location":"tutorials/pos_tagging/#building-a-simple-grammar-checker","title":"Building a Simple Grammar Checker","text":"<pre><code>from underthesea import pos_tag\n\ndef check_sentence_structure(text):\n    \"\"\"Check if sentence has basic subject-verb structure.\"\"\"\n    tagged = pos_tag(text)\n\n    has_noun = any(tag in ('N', 'Np', 'P') for word, tag in tagged)\n    has_verb = any(tag == 'V' for word, tag in tagged)\n\n    if has_noun and has_verb:\n        return \"Valid sentence structure\"\n    elif not has_verb:\n        return \"Warning: No verb found\"\n    else:\n        return \"Warning: No subject found\"\n\nsentences = [\n    \"T\u00f4i y\u00eau Vi\u1ec7t Nam\",\n    \"R\u1ea5t \u0111\u1eb9p\",\n    \"Ch\u1ea1y nhanh\"\n]\n\nfor sent in sentences:\n    result = check_sentence_structure(sent)\n    print(f\"{sent}: {result}\")\n</code></pre>"},{"location":"tutorials/pos_tagging/#filtering-content-words","title":"Filtering Content Words","text":"<pre><code>def get_content_words(text):\n    \"\"\"Extract content words (nouns, verbs, adjectives).\"\"\"\n    tagged = pos_tag(text)\n    content_tags = {'N', 'Np', 'V', 'A'}\n    return [word for word, tag in tagged if tag in content_tags]\n\ntext = \"Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p v\u1edbi nhi\u1ec1u c\u1ea3nh quan tuy\u1ec7t v\u1eddi\"\ncontent = get_content_words(text)\nprint(\"Content words:\", content)\n# ['Vi\u1ec7t Nam', '\u0111\u1ea5t n\u01b0\u1edbc', 'xinh \u0111\u1eb9p', 'c\u1ea3nh quan', 'tuy\u1ec7t v\u1eddi']\n</code></pre>"},{"location":"tutorials/pos_tagging/#combining-with-word-segmentation","title":"Combining with Word Segmentation","text":"<p>POS tagging automatically performs word segmentation first:</p> <pre><code>from underthesea import word_tokenize, pos_tag\n\ntext = \"T\u00f4i y\u00eau Vi\u1ec7t Nam\"\n\n# These produce consistent results\nwords = word_tokenize(text)\ntagged = pos_tag(text)\n\nprint(f\"Words: {words}\")\nprint(f\"Tagged: {tagged}\")\n# Words: ['T\u00f4i', 'y\u00eau', 'Vi\u1ec7t Nam']\n# Tagged: [('T\u00f4i', 'P'), ('y\u00eau', 'V'), ('Vi\u1ec7t Nam', 'Np')]\n</code></pre>"},{"location":"tutorials/pos_tagging/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Named Entity Recognition</li> <li>Explore Chunking</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/sentiment_analysis/","title":"Sentiment Analysis Tutorial","text":"<p>Learn how to analyze the sentiment of Vietnamese text.</p>"},{"location":"tutorials/sentiment_analysis/#introduction","title":"Introduction","text":"<p>Sentiment analysis determines the emotional tone of text: - Positive: Happy, satisfied, praising - Negative: Unhappy, disappointed, complaining - Neutral: Factual, no strong emotion</p>"},{"location":"tutorials/sentiment_analysis/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sentiment\n\ntext = \"S\u1ea3n ph\u1ea9m h\u01a1i nh\u1ecf so v\u1edbi t\u01b0\u1edfng t\u01b0\u1ee3ng nh\u01b0ng ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t\"\nresult = sentiment(text)\nprint(result)\n# 'positive'\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#general-sentiment-analysis","title":"General Sentiment Analysis","text":"<pre><code>from underthesea import sentiment\n\nexamples = [\n    \"S\u1ea3n ph\u1ea9m ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t, \u0111\u00f3ng g\u00f3i c\u1ea9n th\u1eadn\",  # positive\n    \"h\u00e0ng k\u00e9m ch\u1ea5t lg, th\u1ea5t v\u1ecdng\",                   # negative\n    \"S\u1ea3n ph\u1ea9m b\u00ecnh th\u01b0\u1eddng, kh\u00f4ng c\u00f3 g\u00ec \u0111\u1eb7c bi\u1ec7t\"    # neutral\n]\n\nfor text in examples:\n    result = sentiment(text)\n    print(f\"{result:10} | {text[:40]}...\")\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#bank-domain-sentiment","title":"Bank Domain Sentiment","text":"<p>For bank-related feedback, use <code>domain='bank'</code>:</p> <pre><code>from underthesea import sentiment\n\nexamples = [\n    \"L\u00e3i su\u1ea5t r\u1ea5t h\u1ea5p d\u1eabn, t\u00f4i r\u1ea5t h\u00e0i l\u00f2ng\",\n    \"Nh\u00e2n vi\u00ean h\u1ed7 tr\u1ee3 ch\u1eadm, ph\u1ea3i ch\u1edd l\u00e2u\",\n    \"App ng\u00e2n h\u00e0ng r\u1ea5t d\u1ec5 s\u1eed d\u1ee5ng\"\n]\n\nfor text in examples:\n    result = sentiment(text, domain='bank')\n    print(f\"{text}\")\n    print(f\"  -&gt; {result}\\n\")\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#bank-domain-output","title":"Bank Domain Output","text":"<p>The bank domain returns aspect-based sentiment:</p> Aspect Example <code>INTEREST_RATE#positive</code> \"L\u00e3i su\u1ea5t r\u1ea5t t\u1ed1t\" <code>CUSTOMER_SUPPORT#negative</code> \"Nh\u00e2n vi\u00ean kh\u00f4ng nhi\u1ec7t t\u00ecnh\" <code>PRODUCT#positive</code> \"Th\u1ebb t\u00edn d\u1ee5ng c\u00f3 nhi\u1ec1u \u01b0u \u0111\u00e3i\" <code>TRADEMARK#positive</code> \"T\u1ef1 h\u00e0o l\u00e0 kh\u00e1ch h\u00e0ng c\u1ee7a BIDV\""},{"location":"tutorials/sentiment_analysis/#practical-applications","title":"Practical Applications","text":""},{"location":"tutorials/sentiment_analysis/#product-review-analysis","title":"Product Review Analysis","text":"<pre><code>from underthesea import sentiment\nfrom collections import Counter\n\ndef analyze_reviews(reviews):\n    \"\"\"Analyze sentiment of product reviews.\"\"\"\n    sentiments = [sentiment(review) for review in reviews]\n\n    # Count distribution\n    distribution = Counter(sentiments)\n    total = len(sentiments)\n\n    print(\"Sentiment Distribution:\")\n    for sent, count in distribution.most_common():\n        percentage = count / total * 100\n        print(f\"  {sent}: {count} ({percentage:.1f}%)\")\n\n    return distribution\n\nreviews = [\n    \"S\u1ea3n ph\u1ea9m r\u1ea5t t\u1ed1t, \u0111\u00e1ng mua\",\n    \"Giao h\u00e0ng nhanh, \u0111\u00f3ng g\u00f3i c\u1ea9n th\u1eadn\",\n    \"H\u00e0ng kh\u00f4ng gi\u1ed1ng h\u00ecnh, th\u1ea5t v\u1ecdng\",\n    \"Ch\u1ea5t l\u01b0\u1ee3ng t\u1ea1m \u0111\u01b0\u1ee3c\",\n    \"Tuy\u1ec7t v\u1eddi, s\u1ebd mua l\u1ea1i\",\n    \"Qu\u00e1 t\u1ec7, kh\u00f4ng khuy\u00ean mua\"\n]\n\nanalyze_reviews(reviews)\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#customer-feedback-dashboard","title":"Customer Feedback Dashboard","text":"<pre><code>from underthesea import sentiment\n\nclass FeedbackAnalyzer:\n    def __init__(self):\n        self.feedback = []\n\n    def add_feedback(self, text, source=\"unknown\"):\n        result = sentiment(text)\n        self.feedback.append({\n            'text': text,\n            'sentiment': result,\n            'source': source\n        })\n\n    def get_summary(self):\n        if not self.feedback:\n            return \"No feedback yet\"\n\n        positive = sum(1 for f in self.feedback if f['sentiment'] == 'positive')\n        negative = sum(1 for f in self.feedback if f['sentiment'] == 'negative')\n        neutral = sum(1 for f in self.feedback if f['sentiment'] == 'neutral')\n\n        total = len(self.feedback)\n        return {\n            'total': total,\n            'positive': positive,\n            'negative': negative,\n            'neutral': neutral,\n            'satisfaction_rate': positive / total * 100\n        }\n\n    def get_negative_feedback(self):\n        return [f for f in self.feedback if f['sentiment'] == 'negative']\n\n# Usage\nanalyzer = FeedbackAnalyzer()\nanalyzer.add_feedback(\"D\u1ecbch v\u1ee5 tuy\u1ec7t v\u1eddi\", source=\"email\")\nanalyzer.add_feedback(\"Ch\u1edd l\u00e2u qu\u00e1\", source=\"phone\")\nanalyzer.add_feedback(\"Nh\u00e2n vi\u00ean nhi\u1ec7t t\u00ecnh\", source=\"survey\")\n\nprint(\"Summary:\", analyzer.get_summary())\nprint(\"Negative:\", analyzer.get_negative_feedback())\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#social-media-monitoring","title":"Social Media Monitoring","text":"<pre><code>from underthesea import sentiment\nfrom datetime import datetime\n\ndef monitor_brand_sentiment(mentions):\n    \"\"\"Monitor brand sentiment from social media mentions.\"\"\"\n    results = []\n\n    for mention in mentions:\n        result = sentiment(mention['text'])\n        results.append({\n            **mention,\n            'sentiment': result\n        })\n\n    # Aggregate by sentiment\n    positive_mentions = [r for r in results if r['sentiment'] == 'positive']\n    negative_mentions = [r for r in results if r['sentiment'] == 'negative']\n\n    print(f\"Total mentions: {len(results)}\")\n    print(f\"Positive: {len(positive_mentions)}\")\n    print(f\"Negative: {len(negative_mentions)}\")\n\n    if negative_mentions:\n        print(\"\\nNegative mentions (need attention):\")\n        for m in negative_mentions:\n            print(f\"  - {m['text'][:50]}...\")\n\n    return results\n\nmentions = [\n    {'text': 'S\u1ea3n ph\u1ea9m ABC r\u1ea5t t\u1ed1t, \u0111\u00e1ng mua', 'platform': 'facebook'},\n    {'text': 'D\u1ecbch v\u1ee5 c\u1ee7a c\u00f4ng ty XYZ qu\u00e1 t\u1ec7', 'platform': 'twitter'},\n    {'text': 'R\u1ea5t h\u00e0i l\u00f2ng v\u1edbi \u0111\u01a1n h\u00e0ng', 'platform': 'instagram'}\n]\n\nmonitor_brand_sentiment(mentions)\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#sentiment-trend-analysis","title":"Sentiment Trend Analysis","text":"<pre><code>from underthesea import sentiment\nfrom collections import defaultdict\n\ndef analyze_sentiment_trend(data_by_date):\n    \"\"\"Analyze sentiment trend over time.\"\"\"\n    trend = {}\n\n    for date, texts in data_by_date.items():\n        sentiments = [sentiment(t) for t in texts]\n        positive_rate = sum(1 for s in sentiments if s == 'positive') / len(sentiments)\n        trend[date] = positive_rate * 100\n\n    print(\"Sentiment Trend (Positive %):\")\n    for date, rate in sorted(trend.items()):\n        bar = '\u2588' * int(rate / 5)\n        print(f\"  {date}: {bar} {rate:.1f}%\")\n\n    return trend\n\ndata = {\n    '2024-01-01': [\"S\u1ea3n ph\u1ea9m t\u1ed1t\", \"H\u00e0i l\u00f2ng\", \"T\u1ea1m \u0111\u01b0\u1ee3c\"],\n    '2024-01-02': [\"Th\u1ea5t v\u1ecdng\", \"Kh\u00f4ng t\u1ed1t\", \"B\u00ecnh th\u01b0\u1eddng\"],\n    '2024-01-03': [\"Tuy\u1ec7t v\u1eddi\", \"R\u1ea5t th\u00edch\", \"S\u1ebd mua l\u1ea1i\", \"T\u1ed1t\"]\n}\n\nanalyze_sentiment_trend(data)\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#tips-for-better-results","title":"Tips for Better Results","text":"<ol> <li>Preprocessing: Clean text before analysis (remove noise, fix typos)</li> <li>Context: Consider using domain-specific model when available</li> <li>Batch Processing: Process multiple texts for efficiency</li> <li>Error Handling: Handle edge cases (empty text, very short text)</li> </ol> <pre><code>def safe_sentiment(text):\n    \"\"\"Safely analyze sentiment with error handling.\"\"\"\n    if not text or len(text.strip()) &lt; 5:\n        return 'neutral'\n    try:\n        return sentiment(text)\n    except Exception as e:\n        print(f\"Error analyzing: {text[:20]}... - {e}\")\n        return 'unknown'\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Translation</li> <li>Explore Text-to-Speech</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/text_classification/","title":"Text Classification Tutorial","text":"<p>Learn how to classify Vietnamese text into categories.</p>"},{"location":"tutorials/text_classification/#introduction","title":"Introduction","text":"<p>Text classification assigns predefined categories to text. Underthesea provides models for:</p> <ul> <li>General news classification: Sports, Business, Politics, etc.</li> <li>Bank domain classification: Interest rates, Customer support, etc.</li> <li>Prompt-based classification: Using OpenAI for custom categories</li> </ul>"},{"location":"tutorials/text_classification/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import classify\n\ntext = \"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\"\ncategory = classify(text)\nprint(category)\n# ['The thao']\n</code></pre>"},{"location":"tutorials/text_classification/#general-classification","title":"General Classification","text":"<p>The default model classifies Vietnamese news into categories:</p> <pre><code>from underthesea import classify\n\nexamples = [\n    (\"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam th\u1eafng \u0111\u1eadm 3-0\", \"The thao\"),\n    (\"Gi\u00e1 v\u00e0ng t\u0103ng m\u1ea1nh trong tu\u1ea7n qua\", \"Kinh doanh\"),\n    (\"Qu\u1ed1c h\u1ed9i th\u00f4ng qua lu\u1eadt m\u1edbi\", \"Chinh tri Xa hoi\"),\n    (\"Ph\u00e1t hi\u1ec7n lo\u00e0i \u0111\u1ed9ng v\u1eadt m\u1edbi\", \"Khoa hoc\"),\n]\n\nfor text, expected in examples:\n    result = classify(text)\n    print(f\"{text[:30]}... -&gt; {result[0]}\")\n</code></pre>"},{"location":"tutorials/text_classification/#available-categories","title":"Available Categories","text":"Category Description <code>The thao</code> Sports <code>Kinh doanh</code> Business <code>Chinh tri Xa hoi</code> Politics &amp; Society <code>Van hoa</code> Culture <code>Khoa hoc</code> Science <code>Phap luat</code> Law <code>Suc khoe</code> Health <code>Doi song</code> Lifestyle <code>The gioi</code> World <code>Vi tinh</code> Technology"},{"location":"tutorials/text_classification/#bank-domain-classification","title":"Bank Domain Classification","text":"<p>For bank-related text, use <code>domain='bank'</code>:</p> <pre><code>from underthesea import classify\n\nexamples = [\n    \"L\u00e3i su\u1ea5t t\u1eeb BIDV r\u1ea5t \u01b0u \u0111\u00e3i\",\n    \"Nh\u00e2n vi\u00ean h\u1ed7 tr\u1ee3 r\u1ea5t nhi\u1ec7t t\u00ecnh\",\n    \"Th\u1ebb t\u00edn d\u1ee5ng c\u00f3 nhi\u1ec1u \u01b0u \u0111\u00e3i\",\n    \"Logo m\u1edbi c\u1ee7a ng\u00e2n h\u00e0ng r\u1ea5t \u0111\u1eb9p\"\n]\n\nfor text in examples:\n    result = classify(text, domain='bank')\n    print(f\"{text} -&gt; {result}\")\n</code></pre>"},{"location":"tutorials/text_classification/#bank-domain-categories","title":"Bank Domain Categories","text":"Category Description <code>INTEREST_RATE</code> Interest rate discussions <code>CUSTOMER_SUPPORT</code> Customer service feedback <code>PRODUCT</code> Product/service features <code>TRADEMARK</code> Brand perception"},{"location":"tutorials/text_classification/#prompt-based-classification","title":"Prompt-based Classification","text":"<p>Use OpenAI for flexible classification:</p> <pre><code>pip install \"underthesea[prompt]\"\nexport OPENAI_API_KEY=your_api_key\n</code></pre> <pre><code>from underthesea import classify\n\ntext = \"HLV ngo\u1ea1i \u0111\u00f2i g\u1ea7n t\u1ef7 m\u1ed7i th\u00e1ng d\u1eabn d\u1eaft tuy\u1ec3n Vi\u1ec7t Nam\"\nresult = classify(text, model='prompt')\nprint(result)\n# 'Th\u1ec3 thao'\n</code></pre>"},{"location":"tutorials/text_classification/#practical-applications","title":"Practical Applications","text":""},{"location":"tutorials/text_classification/#news-categorization-system","title":"News Categorization System","text":"<pre><code>from underthesea import classify\nfrom collections import defaultdict\n\ndef categorize_articles(articles):\n    \"\"\"Categorize a list of articles.\"\"\"\n    categorized = defaultdict(list)\n\n    for article in articles:\n        category = classify(article)[0]\n        categorized[category].append(article)\n\n    return dict(categorized)\n\narticles = [\n    \"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam chu\u1ea9n b\u1ecb cho AFF Cup\",\n    \"Ch\u1ee9ng kho\u00e1n t\u0103ng \u0111i\u1ec3m phi\u00ean \u0111\u1ea7u tu\u1ea7n\",\n    \"Ph\u00e1t hi\u1ec7n vaccine m\u1edbi ch\u1ed1ng Covid-19\",\n    \"C\u1ea7u th\u1ee7 Quang H\u1ea3i gia nh\u1eadp CLB m\u1edbi\"\n]\n\nresult = categorize_articles(articles)\nfor category, texts in result.items():\n    print(f\"\\n{category}:\")\n    for text in texts:\n        print(f\"  - {text[:40]}...\")\n</code></pre>"},{"location":"tutorials/text_classification/#content-filtering","title":"Content Filtering","text":"<pre><code>from underthesea import classify\n\ndef filter_content(texts, allowed_categories):\n    \"\"\"Filter texts by allowed categories.\"\"\"\n    return [\n        text for text in texts\n        if classify(text)[0] in allowed_categories\n    ]\n\nall_articles = [\n    \"K\u1ebft qu\u1ea3 b\u00f3ng \u0111\u00e1 h\u00f4m nay\",\n    \"Gi\u00e1 USD t\u0103ng m\u1ea1nh\",\n    \"V\u1ee5 \u00e1n h\u00ecnh s\u1ef1 \u0111\u01b0\u1ee3c x\u00e9t x\u1eed\",\n    \"\u0110\u1ed9i tuy\u1ec3n chu\u1ea9n b\u1ecb thi \u0111\u1ea5u\"\n]\n\n# Only keep sports articles\nsports_only = filter_content(all_articles, ['The thao'])\nprint(\"Sports articles:\", sports_only)\n</code></pre>"},{"location":"tutorials/text_classification/#classification-with-confidence-analysis","title":"Classification with Confidence Analysis","text":"<pre><code>from underthesea import classify\n\ndef classify_batch(texts):\n    \"\"\"Classify multiple texts with statistics.\"\"\"\n    results = []\n    for text in texts:\n        category = classify(text)[0]\n        results.append({\n            'text': text[:50] + '...' if len(text) &gt; 50 else text,\n            'category': category\n        })\n    return results\n\ntexts = [\n    \"Vi\u1ec7t Nam v\u00f4 \u0111\u1ecbch AFF Cup sau 10 n\u0103m ch\u1edd \u0111\u1ee3i\",\n    \"Ng\u00e2n h\u00e0ng Nh\u00e0 n\u01b0\u1edbc \u0111i\u1ec1u ch\u1ec9nh l\u00e3i su\u1ea5t\",\n    \"C\u00f4ng ngh\u1ec7 AI \u0111ang thay \u0111\u1ed5i th\u1ebf gi\u1edbi\"\n]\n\nresults = classify_batch(texts)\nfor r in results:\n    print(f\"{r['category']}: {r['text']}\")\n</code></pre>"},{"location":"tutorials/text_classification/#building-a-simple-recommender","title":"Building a Simple Recommender","text":"<pre><code>from underthesea import classify\n\nclass ContentRecommender:\n    def __init__(self):\n        self.user_preferences = {}\n\n    def record_read(self, user_id, article):\n        \"\"\"Record what user reads to learn preferences.\"\"\"\n        category = classify(article)[0]\n        if user_id not in self.user_preferences:\n            self.user_preferences[user_id] = {}\n        self.user_preferences[user_id][category] = \\\n            self.user_preferences[user_id].get(category, 0) + 1\n\n    def recommend(self, user_id, articles):\n        \"\"\"Recommend articles based on user preferences.\"\"\"\n        if user_id not in self.user_preferences:\n            return articles  # No history, return all\n\n        prefs = self.user_preferences[user_id]\n        favorite_category = max(prefs, key=prefs.get)\n\n        return [a for a in articles if classify(a)[0] == favorite_category]\n\n# Usage\nrecommender = ContentRecommender()\nrecommender.record_read(\"user1\", \"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam th\u1eafng l\u1edbn\")\nrecommender.record_read(\"user1\", \"C\u1ea7u th\u1ee7 ghi b\u00e0n \u0111\u1eb9p m\u1eaft\")\n\nnew_articles = [\n    \"K\u1ebft qu\u1ea3 b\u00f3ng \u0111\u00e1 h\u00f4m nay\",\n    \"Gi\u00e1 v\u00e0ng t\u0103ng cao\",\n    \"Tr\u1eadn \u0111\u1ea5u k\u1ecbch t\u00ednh\"\n]\n\nrecommended = recommender.recommend(\"user1\", new_articles)\nprint(\"Recommended:\", recommended)\n</code></pre>"},{"location":"tutorials/text_classification/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sentiment Analysis</li> <li>Explore Translation</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/translation/","title":"Translation Tutorial","text":"<p>Learn how to translate between Vietnamese and English.</p>"},{"location":"tutorials/translation/#installation","title":"Installation","text":"<p>Translation requires the deep learning dependencies:</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre>"},{"location":"tutorials/translation/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import translate\n\n# Vietnamese to English (default)\ntext = \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\nenglish = translate(text)\nprint(english)\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"tutorials/translation/#vietnamese-to-english","title":"Vietnamese to English","text":"<pre><code>from underthesea import translate\n\nexamples = [\n    \"Xin ch\u00e0o, t\u00f4i l\u00e0 sinh vi\u00ean\",\n    \"Vi\u1ec7t Nam c\u00f3 nhi\u1ec1u \u0111\u1ecba \u0111i\u1ec3m du l\u1ecbch \u0111\u1eb9p\",\n    \"\u1ea8m th\u1ef1c Vi\u1ec7t Nam n\u1ed5i ti\u1ebfng tr\u00ean th\u1ebf gi\u1edbi\"\n]\n\nfor text in examples:\n    english = translate(text)\n    print(f\"VI: {text}\")\n    print(f\"EN: {english}\\n\")\n</code></pre>"},{"location":"tutorials/translation/#english-to-vietnamese","title":"English to Vietnamese","text":"<pre><code>from underthesea import translate\n\nexamples = [\n    \"I love Vietnamese food\",\n    \"Vietnam is a beautiful country\",\n    \"Hello, how are you today?\"\n]\n\nfor text in examples:\n    vietnamese = translate(text, source_lang='en', target_lang='vi')\n    print(f\"EN: {text}\")\n    print(f\"VI: {vietnamese}\\n\")\n</code></pre>"},{"location":"tutorials/translation/#practical-applications","title":"Practical Applications","text":""},{"location":"tutorials/translation/#document-translation","title":"Document Translation","text":"<pre><code>from underthesea import sent_tokenize, translate\n\ndef translate_document(document, source_lang='vi', target_lang='en'):\n    \"\"\"Translate a document sentence by sentence.\"\"\"\n    sentences = sent_tokenize(document)\n    translations = []\n\n    for sentence in sentences:\n        if sentence.strip():\n            translated = translate(sentence, source_lang, target_lang)\n            translations.append(translated)\n\n    return ' '.join(translations)\n\ndocument = \"\"\"\nVi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia n\u1eb1m \u1edf \u0110\u00f4ng Nam \u00c1.\nTh\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam l\u00e0 H\u00e0 N\u1ed9i.\nTh\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh.\n\"\"\"\n\nenglish = translate_document(document)\nprint(english)\n</code></pre>"},{"location":"tutorials/translation/#bilingual-content-generator","title":"Bilingual Content Generator","text":"<pre><code>from underthesea import translate\n\ndef create_bilingual_content(texts, source_lang='vi'):\n    \"\"\"Create bilingual content from source texts.\"\"\"\n    target_lang = 'en' if source_lang == 'vi' else 'vi'\n\n    bilingual = []\n    for text in texts:\n        translated = translate(text, source_lang, target_lang)\n        bilingual.append({\n            'original': text,\n            'translated': translated,\n            'source_lang': source_lang,\n            'target_lang': target_lang\n        })\n\n    return bilingual\n\ntexts = [\n    \"Xin ch\u00e0o\",\n    \"C\u1ea3m \u01a1n b\u1ea1n\",\n    \"T\u1ea1m bi\u1ec7t\"\n]\n\nresult = create_bilingual_content(texts)\nfor item in result:\n    print(f\"{item['source_lang'].upper()}: {item['original']}\")\n    print(f\"{item['target_lang'].upper()}: {item['translated']}\\n\")\n</code></pre>"},{"location":"tutorials/translation/#translation-memory","title":"Translation Memory","text":"<pre><code>from underthesea import translate\n\nclass TranslationMemory:\n    \"\"\"Simple translation memory for caching translations.\"\"\"\n\n    def __init__(self):\n        self.memory = {}\n\n    def translate(self, text, source_lang='vi', target_lang='en'):\n        key = (text, source_lang, target_lang)\n\n        if key not in self.memory:\n            # Translate and cache\n            result = translate(text, source_lang, target_lang)\n            self.memory[key] = result\n\n        return self.memory[key]\n\n    def get_stats(self):\n        return {\n            'cached_translations': len(self.memory)\n        }\n\n# Usage\ntm = TranslationMemory()\n\n# First call - translates and caches\nresult1 = tm.translate(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n\n# Second call - returns from cache (faster)\nresult2 = tm.translate(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n\nprint(result1)  # Hello Vietnam\nprint(tm.get_stats())  # {'cached_translations': 1}\n</code></pre>"},{"location":"tutorials/translation/#website-localization-helper","title":"Website Localization Helper","text":"<pre><code>from underthesea import translate\n\ndef localize_ui_strings(strings, target_lang='en'):\n    \"\"\"Translate UI strings for localization.\"\"\"\n    localized = {}\n\n    for key, text in strings.items():\n        localized[key] = translate(text, 'vi', target_lang)\n\n    return localized\n\n# Vietnamese UI strings\nvi_strings = {\n    'welcome': 'Ch\u00e0o m\u1eebng b\u1ea1n \u0111\u1ebfn v\u1edbi \u1ee9ng d\u1ee5ng',\n    'login': '\u0110\u0103ng nh\u1eadp',\n    'logout': '\u0110\u0103ng xu\u1ea5t',\n    'settings': 'C\u00e0i \u0111\u1eb7t',\n    'profile': 'H\u1ed3 s\u01a1 c\u00e1 nh\u00e2n'\n}\n\n# Generate English translations\nen_strings = localize_ui_strings(vi_strings)\n\nprint(\"Localization Results:\")\nfor key in vi_strings:\n    print(f\"  {key}:\")\n    print(f\"    VI: {vi_strings[key]}\")\n    print(f\"    EN: {en_strings[key]}\")\n</code></pre>"},{"location":"tutorials/translation/#parallel-text-corpus-builder","title":"Parallel Text Corpus Builder","text":"<pre><code>from underthesea import translate\n\ndef build_parallel_corpus(source_texts, source_lang='vi', target_lang='en'):\n    \"\"\"Build a parallel text corpus for training/research.\"\"\"\n    corpus = []\n\n    for text in source_texts:\n        translated = translate(text, source_lang, target_lang)\n        corpus.append({\n            'source': text,\n            'target': translated,\n            'source_lang': source_lang,\n            'target_lang': target_lang\n        })\n\n    return corpus\n\ndef save_corpus(corpus, filename):\n    \"\"\"Save parallel corpus to file.\"\"\"\n    with open(filename, 'w', encoding='utf-8') as f:\n        for item in corpus:\n            f.write(f\"{item['source']}\\t{item['target']}\\n\")\n\n# Build corpus\ntexts = [\n    \"Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p\",\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\",\n    \"\u1ea8m th\u1ef1c Vi\u1ec7t Nam r\u1ea5t phong ph\u00fa\"\n]\n\ncorpus = build_parallel_corpus(texts)\nfor item in corpus:\n    print(f\"VI: {item['source']}\")\n    print(f\"EN: {item['target']}\\n\")\n</code></pre>"},{"location":"tutorials/translation/#tips-for-better-translations","title":"Tips for Better Translations","text":"<ol> <li>Sentence-level: Translate sentence by sentence for better quality</li> <li>Context: Provide complete sentences, not fragments</li> <li>Preprocessing: Clean and normalize text before translation</li> <li>Post-editing: Review translations for domain-specific terms</li> </ol> <pre><code>from underthesea import text_normalize, translate\n\ndef quality_translate(text, source_lang='vi', target_lang='en'):\n    \"\"\"Translate with preprocessing for better quality.\"\"\"\n    # Normalize text first\n    if source_lang == 'vi':\n        text = text_normalize(text)\n\n    # Translate\n    result = translate(text, source_lang, target_lang)\n\n    return result\n</code></pre>"},{"location":"tutorials/translation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Text-to-Speech</li> <li>Explore Language Detection</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/tts/","title":"Text-to-Speech Tutorial","text":"<p>Learn how to convert Vietnamese text to speech.</p>"},{"location":"tutorials/tts/#installation","title":"Installation","text":"<p>Text-to-speech requires additional setup:</p> <pre><code># Install dependencies\npip install \"underthesea[voice]\"\n\n# Download the TTS model\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"tutorials/tts/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea.pipeline.tts import tts\n\ntext = \"Xin ch\u00e0o Vi\u1ec7t Nam\"\ntts(text)\n# Creates sound.wav in current directory\n</code></pre>"},{"location":"tutorials/tts/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Basic usage\nunderthesea tts \"Xin ch\u00e0o Vi\u1ec7t Nam\"\n</code></pre>"},{"location":"tutorials/tts/#custom-output-file","title":"Custom Output File","text":"<pre><code>from underthesea.pipeline.tts import tts\n\n# Save to specific file\ntts(\"Xin ch\u00e0o\", outfile=\"hello.wav\")\ntts(\"T\u1ea1m bi\u1ec7t\", outfile=\"goodbye.wav\")\n\n# Play audio immediately\ntts(\"Xin ch\u00e0o\", play=True)\n</code></pre>"},{"location":"tutorials/tts/#practical-applications","title":"Practical Applications","text":""},{"location":"tutorials/tts/#audio-book-generator","title":"Audio Book Generator","text":"<pre><code>from underthesea.pipeline.tts import tts\nfrom underthesea import sent_tokenize\nimport os\n\ndef generate_audiobook(text, output_dir=\"audiobook\"):\n    \"\"\"Generate audio files for each sentence.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    sentences = sent_tokenize(text)\n    audio_files = []\n\n    for i, sentence in enumerate(sentences):\n        if sentence.strip():\n            filename = f\"{output_dir}/sentence_{i+1:03d}.wav\"\n            tts(sentence, outfile=filename)\n            audio_files.append(filename)\n            print(f\"Generated: {filename}\")\n\n    return audio_files\n\ntext = \"\"\"\nVi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia xinh \u0111\u1eb9p.\nTh\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam l\u00e0 H\u00e0 N\u1ed9i.\n\u1ea8m th\u1ef1c Vi\u1ec7t Nam n\u1ed5i ti\u1ebfng tr\u00ean th\u1ebf gi\u1edbi.\n\"\"\"\n\nfiles = generate_audiobook(text)\nprint(f\"Generated {len(files)} audio files\")\n</code></pre>"},{"location":"tutorials/tts/#vocabulary-pronunciation-guide","title":"Vocabulary Pronunciation Guide","text":"<pre><code>from underthesea.pipeline.tts import tts\nimport os\n\ndef create_pronunciation_guide(words, output_dir=\"pronunciation\"):\n    \"\"\"Create audio files for vocabulary pronunciation.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    for word in words:\n        # Clean filename\n        filename = word.replace(\" \", \"_\").lower()\n        filepath = f\"{output_dir}/{filename}.wav\"\n\n        tts(word, outfile=filepath)\n        print(f\"Created: {filepath}\")\n\nvocabulary = [\n    \"Xin ch\u00e0o\",\n    \"C\u1ea3m \u01a1n\",\n    \"T\u1ea1m bi\u1ec7t\",\n    \"Xin l\u1ed7i\",\n    \"Kh\u00f4ng sao\"\n]\n\ncreate_pronunciation_guide(vocabulary)\n</code></pre>"},{"location":"tutorials/tts/#notification-system","title":"Notification System","text":"<pre><code>from underthesea.pipeline.tts import tts\nimport subprocess\nimport platform\n\ndef speak_notification(message):\n    \"\"\"Speak a notification message.\"\"\"\n    output_file = \"/tmp/notification.wav\"\n    tts(message, outfile=output_file)\n\n    # Play audio based on platform\n    system = platform.system()\n    if system == \"Darwin\":  # macOS\n        subprocess.run([\"afplay\", output_file])\n    elif system == \"Linux\":\n        subprocess.run([\"aplay\", output_file])\n    elif system == \"Windows\":\n        import winsound\n        winsound.PlaySound(output_file, winsound.SND_FILENAME)\n\n# Usage\nspeak_notification(\"B\u1ea1n c\u00f3 m\u1ed9t tin nh\u1eafn m\u1edbi\")\nspeak_notification(\"Cu\u1ed9c h\u1ecdp s\u1ebd b\u1eaft \u0111\u1ea7u sau 5 ph\u00fat\")\n</code></pre>"},{"location":"tutorials/tts/#language-learning-app","title":"Language Learning App","text":"<pre><code>from underthesea.pipeline.tts import tts\nfrom underthesea import translate\nimport os\n\ndef create_learning_material(vietnamese_phrases, output_dir=\"learning\"):\n    \"\"\"Create learning materials with audio and translations.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    materials = []\n\n    for i, phrase in enumerate(vietnamese_phrases):\n        # Generate audio\n        audio_file = f\"{output_dir}/phrase_{i+1}.wav\"\n        tts(phrase, outfile=audio_file)\n\n        # Get translation\n        english = translate(phrase)\n\n        materials.append({\n            'vietnamese': phrase,\n            'english': english,\n            'audio': audio_file\n        })\n\n        print(f\"Created: {phrase} -&gt; {english}\")\n\n    return materials\n\nphrases = [\n    \"B\u1ea1n kh\u1ecfe kh\u00f4ng?\",\n    \"T\u00f4i r\u1ea5t vui \u0111\u01b0\u1ee3c g\u1eb7p b\u1ea1n\",\n    \"H\u00f4m nay th\u1eddi ti\u1ebft \u0111\u1eb9p qu\u00e1\"\n]\n\nmaterials = create_learning_material(phrases)\n</code></pre>"},{"location":"tutorials/tts/#batch-audio-generation","title":"Batch Audio Generation","text":"<pre><code>from underthesea.pipeline.tts import tts\nimport os\n\ndef generate_audio_batch(texts, output_dir=\"batch_audio\"):\n    \"\"\"Generate audio files in batch.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    def generate_one(args):\n        i, text = args\n        filename = f\"{output_dir}/audio_{i+1:03d}.wav\"\n        tts(text, outfile=filename)\n        return filename\n\n    # Note: For true parallel processing, you might need\n    # to handle model loading carefully\n    results = []\n    for i, text in enumerate(texts):\n        result = generate_one((i, text))\n        results.append(result)\n        print(f\"Generated: {result}\")\n\n    return results\n\ntexts = [\n    \"\u0110\u00e2y l\u00e0 c\u00e2u th\u1ee9 nh\u1ea5t\",\n    \"\u0110\u00e2y l\u00e0 c\u00e2u th\u1ee9 hai\",\n    \"\u0110\u00e2y l\u00e0 c\u00e2u th\u1ee9 ba\"\n]\n\nfiles = generate_audio_batch(texts)\n</code></pre>"},{"location":"tutorials/tts/#tips-for-better-audio","title":"Tips for Better Audio","text":"<ol> <li>Punctuation: Use proper punctuation for natural pauses</li> <li>Sentence Length: Shorter sentences produce cleaner audio</li> <li>Text Normalization: Normalize text before generating audio</li> </ol> <pre><code>from underthesea.pipeline.tts import tts\nfrom underthesea import text_normalize\n\ndef quality_tts(text, outfile=\"sound.wav\"):\n    \"\"\"Generate audio with text preprocessing.\"\"\"\n    # Normalize text first\n    normalized = text_normalize(text)\n\n    # Generate audio\n    tts(normalized, outfile=outfile)\n\nquality_tts(\"\u00d0\u00e2y l\u00e0 v\u0103n b\u1ea3n c\u1ea7n chu\u1ea9n ho\u00e1\")\n</code></pre>"},{"location":"tutorials/tts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/tts/#model-not-found","title":"Model Not Found","text":"<pre><code># Download the model\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"tutorials/tts/#no-audio-output","title":"No Audio Output","text":"<ul> <li>Check that the output file was created</li> <li>Verify audio player is installed on your system</li> <li>Check file permissions in the output directory</li> </ul>"},{"location":"tutorials/tts/#audio-quality-issues","title":"Audio Quality Issues","text":"<ul> <li>Ensure input text is proper Vietnamese</li> <li>Use complete sentences with punctuation</li> <li>Avoid very long sentences</li> </ul>"},{"location":"tutorials/tts/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Language Detection</li> <li>Explore Translation</li> <li>See the API Reference</li> </ul>"},{"location":"tutorials/word_segmentation/","title":"Word Segmentation Tutorial","text":"<p>Learn how to segment Vietnamese text into words using Underthesea.</p>"},{"location":"tutorials/word_segmentation/#introduction","title":"Introduction","text":"<p>Vietnamese text doesn't have spaces between words like English. For example:</p> <ul> <li>English: <code>\"I love Vietnam\"</code> \u2192 3 words</li> <li>Vietnamese: <code>\"T\u00f4i y\u00eau Vi\u1ec7t Nam\"</code> \u2192 4 words (not 3!)</li> </ul> <p>The phrase \"Vi\u1ec7t Nam\" is a single word (country name), but it appears as two syllables separated by a space. Word segmentation identifies the correct word boundaries.</p>"},{"location":"tutorials/word_segmentation/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# [\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n</code></pre> <p>Notice that: - \"Ch\u00e0ng trai\" (young man) is identified as one word - \"Qu\u1ea3ng Tr\u1ecb\" (province name) is identified as one word - \"kh\u1edfi nghi\u1ec7p\" (entrepreneurship) is identified as one word</p>"},{"location":"tutorials/word_segmentation/#output-formats","title":"Output Formats","text":""},{"location":"tutorials/word_segmentation/#list-format-default","title":"List Format (Default)","text":"<pre><code>words = word_tokenize(\"T\u00f4i y\u00eau Vi\u1ec7t Nam\")\nprint(words)\n# ['T\u00f4i', 'y\u00eau', 'Vi\u1ec7t Nam']\nprint(type(words))\n# &lt;class 'list'&gt;\n</code></pre>"},{"location":"tutorials/word_segmentation/#text-format","title":"Text Format","text":"<p>Use <code>format=\"text\"</code> to get a string with underscores joining multi-syllable words:</p> <pre><code>result = word_tokenize(\"T\u00f4i y\u00eau Vi\u1ec7t Nam\", format=\"text\")\nprint(result)\n# 'T\u00f4i y\u00eau Vi\u1ec7t_Nam'\n</code></pre> <p>This format is useful for: - Training machine learning models - Text preprocessing pipelines - Downstream NLP tasks</p>"},{"location":"tutorials/word_segmentation/#using-fixed-words","title":"Using Fixed Words","text":"<p>Sometimes you need specific terms to be kept together. Use the <code>fixed_words</code> parameter:</p> <pre><code>text = \"Vi\u1ec7n Nghi\u00ean C\u1ee9u chi\u1ebfn l\u01b0\u1ee3c qu\u1ed1c gia v\u1ec1 h\u1ecdc m\u00e1y\"\n\n# Without fixed_words\nresult = word_tokenize(text, format=\"text\")\nprint(result)\n# Might split \"Vi\u1ec7n Nghi\u00ean C\u1ee9u\" or \"h\u1ecdc m\u00e1y\" incorrectly\n\n# With fixed_words\nfixed_words = [\"Vi\u1ec7n Nghi\u00ean C\u1ee9u\", \"h\u1ecdc m\u00e1y\"]\nresult = word_tokenize(text, fixed_words=fixed_words, format=\"text\")\nprint(result)\n# 'Vi\u1ec7n_Nghi\u00ean_C\u1ee9u chi\u1ebfn_l\u01b0\u1ee3c qu\u1ed1c_gia v\u1ec1 h\u1ecdc_m\u00e1y'\n</code></pre> <p>This is especially useful for: - Domain-specific terminology - Organization names - Technical terms</p>"},{"location":"tutorials/word_segmentation/#practical-examples","title":"Practical Examples","text":""},{"location":"tutorials/word_segmentation/#processing-a-document","title":"Processing a Document","text":"<pre><code>from underthesea import sent_tokenize, word_tokenize\n\ndocument = \"\"\"\nVi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia n\u1eb1m \u1edf \u0110\u00f4ng Nam \u00c1. Th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam l\u00e0 H\u00e0 N\u1ed9i.\nTh\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh.\n\"\"\"\n\n# First, segment into sentences\nsentences = sent_tokenize(document)\n\n# Then, tokenize each sentence\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    print(f\"Sentence: {sentence}\")\n    print(f\"Words: {words}\")\n    print(f\"Word count: {len(words)}\\n\")\n</code></pre>"},{"location":"tutorials/word_segmentation/#building-a-word-frequency-counter","title":"Building a Word Frequency Counter","text":"<pre><code>from collections import Counter\nfrom underthesea import word_tokenize\n\ntext = \"\"\"\nVi\u1ec7t Nam c\u00f3 nhi\u1ec1u \u0111\u1ecba \u0111i\u1ec3m du l\u1ecbch \u0111\u1eb9p. Du l\u1ecbch Vi\u1ec7t Nam ng\u00e0y c\u00e0ng ph\u00e1t tri\u1ec3n.\nKh\u00e1ch du l\u1ecbch qu\u1ed1c t\u1ebf \u0111\u1ebfn Vi\u1ec7t Nam ng\u00e0y c\u00e0ng t\u0103ng.\n\"\"\"\n\n# Tokenize\nwords = word_tokenize(text)\n\n# Count frequencies\nword_counts = Counter(words)\nprint(\"Most common words:\")\nfor word, count in word_counts.most_common(5):\n    print(f\"  {word}: {count}\")\n</code></pre>"},{"location":"tutorials/word_segmentation/#preprocessing-for-machine-learning","title":"Preprocessing for Machine Learning","text":"<pre><code>from underthesea import word_tokenize\n\ndef preprocess(text):\n    \"\"\"Preprocess Vietnamese text for ML.\"\"\"\n    # Tokenize with text format\n    tokenized = word_tokenize(text, format=\"text\")\n    # Convert to lowercase\n    tokenized = tokenized.lower()\n    return tokenized\n\ntexts = [\n    \"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam th\u1eafng \u0111\u1eadm\",\n    \"Gi\u00e1 v\u00e0ng t\u0103ng m\u1ea1nh h\u00f4m nay\"\n]\n\npreprocessed = [preprocess(t) for t in texts]\nprint(preprocessed)\n# ['\u0111\u1ed9i_tuy\u1ec3n vi\u1ec7t_nam th\u1eafng \u0111\u1eadm', 'gi\u00e1 v\u00e0ng t\u0103ng m\u1ea1nh h\u00f4m_nay']\n</code></pre>"},{"location":"tutorials/word_segmentation/#performance-tips","title":"Performance Tips","text":"<ol> <li>Batch Processing: Process texts in batches for efficiency</li> <li>Caching: The model is loaded once and cached</li> <li>First Call: First call is slower due to model loading</li> </ol> <pre><code># First call - slower (model loading)\nword_tokenize(\"Hello\")\n\n# Subsequent calls - fast\nfor text in large_text_list:\n    word_tokenize(text)\n</code></pre>"},{"location":"tutorials/word_segmentation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about POS Tagging</li> <li>Explore Named Entity Recognition</li> <li>See the API Reference</li> </ul>"},{"location":"user_guide/concepts/","title":"Core Concepts","text":"<p>This page explains the key concepts and design principles behind Underthesea.</p>"},{"location":"user_guide/concepts/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>Underthesea is organized as a collection of NLP pipelines. Each pipeline handles a specific NLP task:</p> <pre><code>underthesea/\n\u2514\u2500\u2500 pipeline/\n    \u251c\u2500\u2500 sent_tokenize/    # Sentence segmentation\n    \u251c\u2500\u2500 text_normalize/   # Text normalization\n    \u251c\u2500\u2500 word_tokenize/    # Word segmentation\n    \u251c\u2500\u2500 pos_tag/          # Part-of-speech tagging\n    \u251c\u2500\u2500 chunking/         # Phrase chunking\n    \u251c\u2500\u2500 dependency_parse/ # Dependency parsing\n    \u251c\u2500\u2500 ner/              # Named entity recognition\n    \u251c\u2500\u2500 classification/   # Text classification\n    \u251c\u2500\u2500 sentiment/        # Sentiment analysis\n    \u251c\u2500\u2500 translate/        # Translation\n    \u251c\u2500\u2500 lang_detect/      # Language detection\n    \u2514\u2500\u2500 tts/              # Text-to-speech\n</code></pre>"},{"location":"user_guide/concepts/#lazy-model-loading","title":"Lazy Model Loading","text":"<p>Underthesea uses lazy loading for models to minimize startup time and memory usage:</p> <pre><code># Model is NOT loaded yet\nfrom underthesea import word_tokenize\n\n# Model is loaded on first call\nresult = word_tokenize(\"Xin ch\u00e0o\")\n\n# Subsequent calls use cached model (fast)\nresult = word_tokenize(\"Vi\u1ec7t Nam\")\n</code></pre> <p>This means:</p> <ul> <li>Fast imports: Importing the library is instant</li> <li>Memory efficient: Only load models you actually use</li> <li>First call overhead: The first call may take longer due to model loading</li> </ul>"},{"location":"user_guide/concepts/#model-types","title":"Model Types","text":"<p>Underthesea uses different types of models depending on the task:</p>"},{"location":"user_guide/concepts/#crf-models-default","title":"CRF Models (Default)","text":"<p>Conditional Random Fields (CRF) models are lightweight and fast:</p> <ul> <li>Word segmentation</li> <li>POS tagging</li> <li>Chunking</li> <li>Named entity recognition</li> <li>Text classification</li> <li>Sentiment analysis</li> </ul>"},{"location":"user_guide/concepts/#deep-learning-models","title":"Deep Learning Models","text":"<p>Transformer-based models for complex tasks:</p> <ul> <li>Dependency parsing</li> <li>Deep NER (<code>ner(..., deep=True)</code>)</li> <li>Translation</li> </ul> <p>Requires <code>pip install \"underthesea[deep]\"</code></p>"},{"location":"user_guide/concepts/#prompt-based-models","title":"Prompt-based Models","text":"<p>Uses OpenAI API for zero-shot classification:</p> <ul> <li>Text classification (<code>classify(..., model='prompt')</code>)</li> </ul> <p>Requires <code>pip install \"underthesea[prompt]\"</code></p>"},{"location":"user_guide/concepts/#fasttext-models","title":"FastText Models","text":"<p>For language identification:</p> <ul> <li>Language detection</li> </ul> <p>Requires <code>pip install \"underthesea[langdetect]\"</code></p>"},{"location":"user_guide/concepts/#output-formats","title":"Output Formats","text":"<p>Most functions support multiple output formats:</p>"},{"location":"user_guide/concepts/#list-format-default","title":"List Format (Default)","text":"<pre><code>from underthesea import word_tokenize\n\nword_tokenize(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n# ['Xin', 'ch\u00e0o', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"user_guide/concepts/#text-format","title":"Text Format","text":"<pre><code>word_tokenize(\"Xin ch\u00e0o Vi\u1ec7t Nam\", format=\"text\")\n# 'Xin ch\u00e0o Vi\u1ec7t_Nam'\n</code></pre> <p>Multi-word tokens are joined with underscores.</p>"},{"location":"user_guide/concepts/#tuple-format","title":"Tuple Format","text":"<p>For tagging tasks (POS, NER, chunking):</p> <pre><code>from underthesea import pos_tag\n\npos_tag(\"Xin ch\u00e0o\")\n# [('Xin', 'V'), ('ch\u00e0o', 'V')]\n</code></pre>"},{"location":"user_guide/concepts/#vietnamese-nlp-challenges","title":"Vietnamese NLP Challenges","text":""},{"location":"user_guide/concepts/#no-word-boundaries","title":"No Word Boundaries","text":"<p>Unlike English, Vietnamese text doesn't have spaces between words:</p> <pre><code>English: \"I love Vietnam\"     \u2192 3 words\nVietnamese: \"T\u00f4i y\u00eau Vi\u1ec7t Nam\" \u2192 4 words (not 3!)\n</code></pre> <p>Underthesea's word segmentation handles this:</p> <pre><code>word_tokenize(\"T\u00f4i y\u00eau Vi\u1ec7t Nam\")\n# ['T\u00f4i', 'y\u00eau', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"user_guide/concepts/#diacritics-and-unicode","title":"Diacritics and Unicode","text":"<p>Vietnamese uses Latin script with diacritics. Underthesea's text normalization handles common encoding issues:</p> <pre><code>from underthesea import text_normalize\n\n# Fix common Unicode issues\ntext_normalize(\"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng\")\n# \"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng\"\n</code></pre>"},{"location":"user_guide/concepts/#compound-words","title":"Compound Words","text":"<p>Many Vietnamese words consist of multiple syllables:</p> <ul> <li>\"Vi\u1ec7t Nam\" (Vietnam) - 2 syllables, 1 word</li> <li>\"kh\u1edfi nghi\u1ec7p\" (start a business) - 2 syllables, 1 word</li> <li>\"th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh\" (Ho Chi Minh City) - 5 syllables, 1 entity</li> </ul>"},{"location":"user_guide/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Optional Dependencies</li> <li>Explore Available Models</li> <li>Read the API Reference</li> </ul>"},{"location":"user_guide/models/","title":"Models","text":"<p>Underthesea provides pretrained models for various NLP tasks. This page describes the available models and how to use them.</p>"},{"location":"user_guide/models/#model-management","title":"Model Management","text":""},{"location":"user_guide/models/#list-available-models","title":"List Available Models","text":"<pre><code>underthesea list-model\n</code></pre>"},{"location":"user_guide/models/#download-a-model","title":"Download a Model","text":"<pre><code>underthesea download-model MODEL_NAME\n</code></pre>"},{"location":"user_guide/models/#model-storage-location","title":"Model Storage Location","text":"<p>Models are stored in <code>~/.underthesea/models/</code>.</p>"},{"location":"user_guide/models/#models-by-task","title":"Models by Task","text":""},{"location":"user_guide/models/#word-segmentation","title":"Word Segmentation","text":"Model Description Default <code>WS_VLSP2013_CRF</code> CRF model trained on VLSP 2013 Yes"},{"location":"user_guide/models/#pos-tagging","title":"POS Tagging","text":"Model Description Default <code>POS_VLSP2013_CRF</code> CRF model trained on VLSP 2013 Yes"},{"location":"user_guide/models/#named-entity-recognition","title":"Named Entity Recognition","text":"Model Description Default <code>NER_VLSP2016_CRF</code> CRF model trained on VLSP 2016 Yes <code>NER_VLSP2016_BERT</code> BERT model trained on VLSP 2016 <code>deep=True</code> <p>Usage:</p> <pre><code>from underthesea import ner\n\n# CRF model (default)\nner(\"Vi\u1ec7t Nam l\u00e0 qu\u1ed1c gia \u0111\u00f4ng d\u00e2n\")\n\n# BERT model (requires [deep] install)\nner(\"Vi\u1ec7t Nam l\u00e0 qu\u1ed1c gia \u0111\u00f4ng d\u00e2n\", deep=True)\n</code></pre>"},{"location":"user_guide/models/#text-classification","title":"Text Classification","text":"Model Description Domain <code>TC_VNTC_CRF</code> News classification Default <code>TC_BANK_CRF</code> Bank domain classification <code>domain='bank'</code> <p>Usage:</p> <pre><code>from underthesea import classify\n\n# General classification\nclassify(\"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i\")\n\n# Bank domain\nclassify(\"L\u00e3i su\u1ea5t t\u1eeb BIDV r\u1ea5t \u01b0u \u0111\u00e3i\", domain='bank')\n</code></pre>"},{"location":"user_guide/models/#sentiment-analysis","title":"Sentiment Analysis","text":"Model Description Domain <code>SA_GENERAL_CRF</code> General sentiment Default <code>SA_BANK_CRF</code> Bank domain sentiment <code>domain='bank'</code> <p>Usage:</p> <pre><code>from underthesea import sentiment\n\n# General sentiment\nsentiment(\"S\u1ea3n ph\u1ea9m ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t\")\n\n# Bank domain\nsentiment(\"Xem l\u1ea1i v\u1eabn th\u1ea5y t\u1ef1 h\u00e0o v\u1ec1 BIDV\", domain='bank')\n</code></pre>"},{"location":"user_guide/models/#translation","title":"Translation","text":"Model Description <code>TRANSLATION_VI_EN</code> Vietnamese to English translation <p>Usage:</p> <pre><code>from underthesea import translate\n\ntranslate(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\")\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"user_guide/models/#text-to-speech","title":"Text-to-Speech","text":"Model Description Architecture <code>VIET_TTS_V0_4_1</code> Vietnamese TTS model NAT + HiFi-GAN <p>The TTS model uses a two-stage neural architecture:</p> <ol> <li>Duration + Acoustic Model: Predicts phoneme durations and generates mel-spectrograms</li> <li>HiFi-GAN Vocoder: Converts mel-spectrograms to high-quality audio</li> </ol> <p>Requires Installation</p> <pre><code>pip install \"underthesea[voice]\"\n</code></pre> <p>Download and usage:</p> <pre><code>underthesea download-model VIET_TTS_V0_4_1\n</code></pre> <pre><code>from underthesea.pipeline.tts import tts\n\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n# Generates sound.wav\n</code></pre> <p>For detailed technical documentation, see Voice Module Technical Report.</p>"},{"location":"user_guide/models/#datasets","title":"Datasets","text":"<p>Underthesea also provides access to Vietnamese NLP datasets.</p>"},{"location":"user_guide/models/#list-available-datasets","title":"List Available Datasets","text":"<pre><code>underthesea list-data\n</code></pre>"},{"location":"user_guide/models/#download-a-dataset","title":"Download a Dataset","text":"<pre><code>underthesea download-data DATASET_NAME\n</code></pre>"},{"location":"user_guide/models/#available-datasets","title":"Available Datasets","text":"Name Type Description <code>VNTC</code> Categorized Vietnamese Text Classification corpus <code>UTS2017-BANK</code> Categorized Bank domain classification <code>UIT_ABSA_RESTAURANT</code> Sentiment Restaurant reviews <code>UIT_ABSA_HOTEL</code> Sentiment Hotel reviews <code>DI_Vietnamese-UVD</code> Dictionary Vietnamese dictionary <code>CP_Vietnamese_VLC_v2_2022</code> Plaintext Vietnamese corpus"},{"location":"user_guide/models/#dataset-storage-location","title":"Dataset Storage Location","text":"<p>Datasets are stored in <code>~/.underthesea/datasets/</code>.</p>"},{"location":"user_guide/models/#custom-models","title":"Custom Models","text":"<p>You can also use custom models by specifying the model path:</p> <pre><code>from underthesea import pos_tag\n\n# Use a custom model\npos_tag(\"Xin ch\u00e0o\", model=\"/path/to/custom/model\")\n</code></pre>"},{"location":"user_guide/models/#model-performance","title":"Model Performance","text":"<p>For benchmark results and performance comparisons, see the GitHub repository.</p>"},{"location":"user_guide/optional_deps/","title":"Optional Dependencies","text":"<p>Underthesea is designed to be lightweight by default. Advanced features require optional dependencies.</p>"},{"location":"user_guide/optional_deps/#installation-options","title":"Installation Options","text":"Install Command Features Enabled <code>pip install underthesea</code> Core NLP features <code>pip install underthesea[deep]</code> Deep learning models <code>pip install underthesea[voice]</code> Text-to-Speech <code>pip install underthesea[prompt]</code> OpenAI-based classification <code>pip install underthesea[langdetect]</code> Language detection <code>pip install underthesea[dev]</code> Development tools"},{"location":"user_guide/optional_deps/#core-package","title":"Core Package","text":"<p>The base installation includes:</p> <pre><code>pip install underthesea\n</code></pre> <p>Features:</p> <ul> <li><code>sent_tokenize</code> - Sentence segmentation</li> <li><code>text_normalize</code> - Text normalization</li> <li><code>word_tokenize</code> - Word segmentation</li> <li><code>pos_tag</code> - POS tagging</li> <li><code>chunk</code> - Chunking</li> <li><code>ner</code> - Named entity recognition (CRF)</li> <li><code>classify</code> - Text classification (CRF)</li> <li><code>sentiment</code> - Sentiment analysis</li> </ul> <p>Dependencies:</p> <ul> <li>scikit-learn</li> <li>python-crfsuite</li> <li>joblib</li> <li>PyYAML</li> </ul>"},{"location":"user_guide/optional_deps/#deep-learning-package","title":"Deep Learning Package","text":"<p>For transformer-based models:</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <p>Features:</p> <ul> <li><code>dependency_parse</code> - Dependency parsing</li> <li><code>ner(..., deep=True)</code> - Deep learning NER</li> <li><code>translate</code> - Vietnamese-English translation</li> </ul> <p>Dependencies:</p> <ul> <li>torch</li> <li>transformers</li> </ul> <p>Example:</p> <pre><code>from underthesea import dependency_parse, translate\n\n# Dependency parsing\ndependency_parse(\"T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19\")\n\n# Translation\ntranslate(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\")\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"user_guide/optional_deps/#voice-package","title":"Voice Package","text":"<p>For text-to-speech synthesis:</p> <pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre> <p>Features:</p> <ul> <li><code>tts</code> - Vietnamese text-to-speech</li> </ul> <p>Dependencies:</p> <ul> <li>jax</li> <li>jaxlib</li> <li>dm-haiku</li> <li>optax</li> <li>soundfile</li> <li>matplotlib</li> <li>playsound3</li> </ul> <p>Example:</p> <pre><code>from underthesea.pipeline.tts import tts\n\n# Generate speech\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n# Creates sound.wav in current directory\n\n# Custom output file\ntts(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\", outfile=\"hanoi.wav\")\n</code></pre>"},{"location":"user_guide/optional_deps/#prompt-package","title":"Prompt Package","text":"<p>For OpenAI-powered classification:</p> <pre><code>pip install \"underthesea[prompt]\"\nexport OPENAI_API_KEY=your_api_key\n</code></pre> <p>Features:</p> <ul> <li><code>classify(..., model='prompt')</code> - Zero-shot classification</li> </ul> <p>Dependencies:</p> <ul> <li>openai</li> </ul> <p>Example:</p> <pre><code>from underthesea import classify\n\nclassify(\"HLV ngo\u1ea1i \u0111\u00f2i g\u1ea7n t\u1ef7 m\u1ed7i th\u00e1ng d\u1eabn d\u1eaft tuy\u1ec3n Vi\u1ec7t Nam\", model='prompt')\n# 'Th\u1ec3 thao'\n</code></pre>"},{"location":"user_guide/optional_deps/#language-detection-package","title":"Language Detection Package","text":"<p>For language identification:</p> <pre><code>pip install \"underthesea[langdetect]\"\n</code></pre> <p>Features:</p> <ul> <li><code>lang_detect</code> - Language detection</li> </ul> <p>Dependencies:</p> <ul> <li>fasttext</li> </ul> <p>Example:</p> <pre><code>from underthesea import lang_detect\n\nlang_detect(\"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng\")\n# 'vi'\n\nlang_detect(\"Hello, how are you?\")\n# 'en'\n</code></pre>"},{"location":"user_guide/optional_deps/#development-package","title":"Development Package","text":"<p>For contributors and developers:</p> <pre><code>pip install \"underthesea[dev]\"\n</code></pre> <p>Features:</p> <ul> <li>Linting with Ruff</li> <li>Testing with tox</li> <li>Build tools</li> </ul> <p>Dependencies:</p> <ul> <li>ruff</li> <li>tox</li> </ul>"},{"location":"user_guide/optional_deps/#combining-packages","title":"Combining Packages","text":"<p>You can install multiple optional packages:</p> <pre><code># Deep learning + language detection\npip install \"underthesea[deep,langdetect]\"\n\n# All optional dependencies\npip install \"underthesea[deep,voice,prompt,langdetect,dev]\"\n</code></pre>"},{"location":"user_guide/optional_deps/#checking-installed-features","title":"Checking Installed Features","text":"<p>You can check which features are available:</p> <pre><code>import underthesea\n\n# Check version\nprint(underthesea.__version__)\n\n# Try importing optional features\ntry:\n    from underthesea import translate\n    print(\"Deep learning features: Available\")\nexcept ImportError:\n    print(\"Deep learning features: Not installed\")\n\ntry:\n    from underthesea import lang_detect\n    print(\"Language detection: Available\")\nexcept ImportError:\n    print(\"Language detection: Not installed\")\n</code></pre>"},{"location":"user_guide/optional_deps/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/optional_deps/#pytorch-installation-issues","title":"PyTorch Installation Issues","text":"<p>If you have trouble installing PyTorch, install it separately first:</p> <pre><code># CPU only\npip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# With CUDA support\npip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"user_guide/optional_deps/#fasttext-installation-issues","title":"FastText Installation Issues","text":"<p>On some systems, FastText may require compilation:</p> <pre><code># Install build tools first\npip install wheel setuptools\n\n# Then install fasttext\npip install fasttext\n</code></pre>"},{"location":"user_guide/optional_deps/#openai-api-key","title":"OpenAI API Key","text":"<p>For prompt-based models, set your API key:</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>Or in Python:</p> <pre><code>import os\nos.environ['OPENAI_API_KEY'] = 'sk-...'\n</code></pre>"}]}