"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[5749],{7735(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"rust-text-classifier","metadata":{"permalink":"/blog/rust-text-classifier","editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/blog/2026-02-03-rust-text-classifier.md","source":"@site/blog/2026-02-03-rust-text-classifier.md","title":"Rust-Powered Text Classification","description":"In underthesea v9.2.9, we\'ve completely rewritten the text classification pipeline using our Rust-based TextClassifier. This delivers up to 273x faster inference compared to the previous sklearn-based implementation.","date":"2026-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"rust","permalink":"/blog/tags/rust"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"classification","permalink":"/blog/tags/classification"},{"inline":true,"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":3.82,"hasTruncateMarker":true,"authors":[{"name":"Vu Anh","title":"Creator of Underthesea","url":"https://github.com/rain1024","imageURL":"https://github.com/rain1024.png","key":"rain1024","page":null}],"frontMatter":{"slug":"rust-text-classifier","title":"Rust-Powered Text Classification","authors":["rain1024"],"tags":["rust","performance","classification","nlp"]},"unlisted":false,"nextItem":{"title":"Rewriting CRF Model in Rust","permalink":"/blog/rewrite-rust-crf-model"}},"content":"In underthesea v9.2.9, we\'ve completely rewritten the text classification pipeline using our Rust-based `TextClassifier`. This delivers up to **273x faster inference** compared to the previous sklearn-based implementation.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nText classification in underthesea supports two domains:\\n- **General**: News categorization (10 categories)\\n- **Bank**: Banking intent classification (14 categories)\\n\\nPreviously, we used scikit-learn\'s `TfidfVectorizer` + `LinearSVC` loaded via joblib. While accurate, this approach had significant overhead.\\n\\n## The Architecture Change\\n\\n### Before (sklearn-based)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                               \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502TfidfVectorizer\u2500\u2500\u2500\u25b6\u2502  LinearSVC   \u2502  \u2502\\n\u2502  \u2502    Text      \u2502    \u2502  (sklearn)   \u2502    \u2502  (sklearn)   \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                   \u2502          \u2502\\n\u2502                      joblib.load()        joblib.load()     \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nLoading two separate pickle files, with Python-based vectorization and inference.\\n\\n### After (Rust-based)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                               \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502         TextClassifier           \u2502  \u2502\\n\u2502  \u2502    Text      \u2502    \u2502  TF-IDF + LinearSVC (Rust)       \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                              \u2502\\n\u2502                      single .bin file                       \u2502\\n\u2502                      underthesea-core                       \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nSingle binary model file, vectorization and inference fused in Rust.\\n\\n## Code Changes\\n\\nThe API remains unchanged:\\n\\n```python\\nfrom underthesea import classify\\n\\n# General classification\\nclassify(\\"Vi\u1ec7t Nam v\xf4 \u0111\u1ecbch AFF Cup\\")\\n# \\"The thao\\"\\n\\n# Bank domain\\nclassify(\\"L\xe3i su\u1ea5t ti\u1ebft ki\u1ec7m bao nhi\xeau?\\", domain=\\"bank\\")\\n# [\'INTEREST_RATE\']\\n```\\n\\nInternally, the implementation is much simpler:\\n\\n**Before:**\\n```python\\nimport joblib\\nfrom underthesea.pipeline.classification import bank\\n\\nvectorizer = joblib.load(\\"vectorizer.pkl\\")\\nclassifier = joblib.load(\\"classifier.pkl\\")\\nfeatures = vectorizer.transform([text])\\nprediction = classifier.predict(features)\\n```\\n\\n**After:**\\n```python\\nfrom underthesea_core import TextClassifier\\n\\nclassifier = TextClassifier.load(\\"model.bin\\")\\nprediction = classifier.predict(text)\\n```\\n\\n## Benchmark Results\\n\\nTested on the same hardware with batch inference:\\n\\n| Domain | sklearn | Rust | Speedup |\\n|--------|---------|------|---------|\\n| General | 1,228 samples/sec | 66,678 samples/sec | **54x** |\\n| Bank | 244 samples/sec | 66,678 samples/sec | **273x** |\\n\\nSingle sample latency: **4ms \u2192 0.465ms**\\n\\n## Why Is It Faster?\\n\\n### 1. Fused Pipeline\\n\\nTF-IDF vectorization and SVM inference run in a single Rust function call, eliminating Python overhead between stages.\\n\\n### 2. Optimized Sparse Operations\\n\\n```rust\\npub fn predict(&self, text: &str) -> String {\\n    // Tokenize and hash features in one pass\\n    let features = self.vectorizer.transform(text);\\n\\n    // Sparse dot product with pre-sorted indices\\n    let scores = self.svm.decision_function(&features);\\n\\n    self.classes[scores.argmax()].clone()\\n}\\n```\\n\\n### 3. Single File Model\\n\\nOne `.bin` file instead of multiple pickle files:\\n- Faster loading\\n- Atomic deployment\\n- Smaller size (JSON-based, not pickle)\\n\\n### 4. No Python GIL Contention\\n\\nRust code releases the GIL during computation, enabling true parallelism.\\n\\n## The Models\\n\\n### sen-classifier-general\\n\\nGeneral Vietnamese news classification model trained on VNTC dataset.\\n\\n**Training Data:** [VNTC](https://github.com/duyvuleo/VNTC) (Vietnamese News Text Classification)\\n- 33,759 training samples\\n- 50,373 test samples\\n- 10 news categories\\n\\n**Categories:**\\n\\n| Label | Vietnamese | English |\\n|-------|------------|---------|\\n| Chinh tri Xa hoi | Ch\xednh tr\u1ecb X\xe3 h\u1ed9i | Politics/Society |\\n| Doi song | \u0110\u1eddi s\u1ed1ng | Lifestyle |\\n| Khoa hoc | Khoa h\u1ecdc | Science |\\n| Kinh doanh | Kinh doanh | Business |\\n| Phap luat | Ph\xe1p lu\u1eadt | Law |\\n| Suc khoe | S\u1ee9c kh\u1ecfe | Health |\\n| The gioi | Th\u1ebf gi\u1edbi | World |\\n| The thao | Th\u1ec3 thao | Sports |\\n| Van hoa | V\u0103n h\xf3a | Culture |\\n| Vi tinh | Vi t\xednh | Technology |\\n\\n**Performance:**\\n- Accuracy: **92.49%**\\n- F1 (weighted): 92.40%\\n- Training time: 37.6s\\n\\n### sen-classifier-bank\\n\\nVietnamese banking intent classification model trained on UTS2017_Bank dataset.\\n\\n**Training Data:** [UTS2017_Bank](https://huggingface.co/datasets/undertheseanlp/UTS2017_Bank)\\n- 1,581 training samples\\n- 396 test samples\\n- 14 banking categories\\n\\n**Categories:**\\n\\n| Label | Description | Samples |\\n|-------|-------------|---------|\\n| CUSTOMER_SUPPORT | Customer support queries | 774 |\\n| TRADEMARK | Brand/trademark mentions | 697 |\\n| LOAN | Loan services | 73 |\\n| INTERNET_BANKING | Internet banking | 69 |\\n| CARD | Card services | 66 |\\n| INTEREST_RATE | Interest rates | 58 |\\n| PROMOTION | Promotions | 56 |\\n| DISCOUNT | Discounts | 40 |\\n| MONEY_TRANSFER | Money transfer | 37 |\\n| OTHER | Other queries | 70 |\\n| PAYMENT | Payment services | 17 |\\n| SAVING | Savings | 12 |\\n| ACCOUNT | Account services | 5 |\\n| SECURITY | Security | 3 |\\n\\n**Performance:**\\n- Accuracy: **75.76%** (+3.29% vs previous sonar_core_1)\\n- F1 (weighted): 72.70%\\n- Training time: 0.13s\\n\\n## Training Pipeline\\n\\nBoth models use a 3-stage TF-IDF + Linear SVM pipeline:\\n\\n```\\nInput Text\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  CountVectorizer                    \u2502\\n\u2502  - max_features: 20,000             \u2502\\n\u2502  - ngram_range: (1, 2)              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  TfidfTransformer                   \u2502\\n\u2502  - use_idf: True                    \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502  LinearSVC                          \u2502\\n\u2502  - C: 1.0                           \u2502\\n\u2502  - max_iter: 2000                   \u2502\\n\u2502  - loss: squared_hinge              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n    \u2193\\nPredicted Label + Confidence\\n```\\n\\n**Key design decisions:**\\n- **Syllable-level tokenization**: No word segmentation for speed\\n- **Character n-grams (1-2)**: Captures Vietnamese morphology\\n- **20K vocabulary**: Balances accuracy and model size\\n- **Linear SVM**: Fast training, works well with sparse high-dimensional data\\n\\nTraining code: [sen-1/src/scripts/train_vntc.py](https://github.com/undertheseanlp/sen-1)\\n\\n## Label Format Change\\n\\nLabels now use Title case with spaces:\\n\\n| Old | New |\\n|-----|-----|\\n| `the_thao` | `The thao` |\\n| `kinh_doanh` | `Kinh doanh` |\\n| `vi_tinh` | `Vi tinh` |\\n\\nBank domain labels remain uppercase: `INTEREST_RATE`, `MONEY_TRANSFER`, etc.\\n\\n## Simplified Codebase\\n\\nWe consolidated three separate modules into one:\\n\\n**Before:**\\n```\\nclassification/\\n\u251c\u2500\u2500 bank/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 sonar_core_1/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 vntc/\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u2514\u2500\u2500 __init__.py\\n```\\n\\n**After:**\\n```\\nclassification/\\n\u251c\u2500\u2500 __init__.py      # Everything here\\n\u2514\u2500\u2500 classification_prompt.py\\n```\\n\\n~190 lines removed, single source of truth for model URLs and loading logic.\\n\\n## Try It Out\\n\\n```bash\\npip install underthesea==9.2.9\\n```\\n\\n```python\\nfrom underthesea import classify\\n\\n# 273x faster!\\nclassify(\\"Th\u1ecb tr\u01b0\u1eddng ch\u1ee9ng kho\xe1n t\u0103ng \u0111i\u1ec3m m\u1ea1nh\\")\\n# \\"Kinh doanh\\"\\n\\nclassify.labels\\n# [\'Chinh tri Xa hoi\', \'Doi song\', \'Khoa hoc\', \'Kinh doanh\', ...]\\n\\nclassify(\\"M\u1edf th\u1ebb t\xedn d\u1ee5ng\\", domain=\\"bank\\")\\n# [\'CARD\']\\n\\nclassify.bank.labels\\n# [\'ACCOUNT\', \'CARD\', \'CUSTOMER_SUPPORT\', ...]\\n```\\n\\n## Links\\n\\n- [PR #935](https://github.com/undertheseanlp/underthesea/pull/935) - Classification pipeline refactor\\n- [Sen-1](https://github.com/undertheseanlp/sen-1) - Training code and technical report\\n- [underthesea-core](https://pypi.org/project/underthesea-core/) - Rust extension on PyPI"},{"id":"rewrite-rust-crf-model","metadata":{"permalink":"/blog/rewrite-rust-crf-model","editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/blog/2026-02-02-rewrite-rust-crf-model.md","source":"@site/blog/2026-02-02-rewrite-rust-crf-model.md","title":"Rewriting CRF Model in Rust","description":"In underthesea v9.2.5, we completed the migration from python-crfsuite to our native Rust implementation underthesea-core. This change resulted in a 20% performance improvement across all CRF-based NLP tasks, plus up to 10x faster training through systematic optimization.","date":"2026-02-02T00:00:00.000Z","tags":[{"inline":true,"label":"rust","permalink":"/blog/tags/rust"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"crf","permalink":"/blog/tags/crf"},{"inline":true,"label":"nlp","permalink":"/blog/tags/nlp"}],"readingTime":8.82,"hasTruncateMarker":true,"authors":[{"name":"Vu Anh","title":"Creator of Underthesea","url":"https://github.com/rain1024","imageURL":"https://github.com/rain1024.png","key":"rain1024","page":null}],"frontMatter":{"slug":"rewrite-rust-crf-model","title":"Rewriting CRF Model in Rust","authors":["rain1024"],"tags":["rust","performance","crf","nlp"]},"unlisted":false,"prevItem":{"title":"Rust-Powered Text Classification","permalink":"/blog/rust-text-classifier"}},"content":"In underthesea v9.2.5, we completed the migration from `python-crfsuite` to our native Rust implementation `underthesea-core`. This change resulted in a **20% performance improvement** across all CRF-based NLP tasks, plus up to **10x faster training** through systematic optimization.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nUnderthesea uses Conditional Random Fields (CRF) for several core NLP tasks:\\n- Word tokenization\\n- POS tagging\\n- Named Entity Recognition (NER)\\n- Chunking\\n\\nPreviously, we relied on `python-crfsuite`, a Python wrapper for the CRFsuite C++ library. While functional, this introduced overhead from multiple language boundaries.\\n\\n## The Architecture Change\\n\\n### Before (v9.2.1)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                                \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502 CRFFeaturizer\u2502\u2500\u2500\u2500\u25b6\u2502   Python     \u2502  \u2502\\n\u2502  \u2502   Tokens     \u2502    \u2502    (Rust)    \u2502    \u2502    List      \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                                                  \u2502          \u2502\\n\u2502                                                  \u25bc          \u2502\\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502                                          \u2502  pycrfsuite  \u2502  \u2502\\n\u2502                                          \u2502    (C++)     \u2502  \u2502\\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nThe data flow crossed multiple language boundaries:\\n1. Python \u2192 Rust (CRFFeaturizer)\\n2. Rust \u2192 Python (feature list)\\n3. Python \u2192 C++ (pycrfsuite)\\n4. C++ \u2192 Python (tags)\\n\\n### After (v9.2.5)\\n\\n```\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                        Python                                \u2502\\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\\n\u2502  \u2502    Input     \u2502\u2500\u2500\u2500\u25b6\u2502 CRFFeaturizer\u2502\u2500\u2500\u2500\u25b6\u2502  CRFTagger   \u2502  \u2502\\n\u2502  \u2502   Tokens     \u2502    \u2502    (Rust)    \u2502    \u2502    (Rust)    \u2502  \u2502\\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\\n\u2502                              \u2502                   \u2502          \u2502\\n\u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\\n\u2502                               underthesea-core              \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nNow both preprocessing and inference are in Rust within the same module, eliminating the C++ dependency entirely.\\n\\n## Code Changes\\n\\nThe change was minimal from the API perspective:\\n\\n**Before:**\\n```python\\nimport pycrfsuite\\nfrom underthesea_core import CRFFeaturizer\\n\\nclass FastCRFSequenceTagger:\\n    def load(self, base_path):\\n        estimator = pycrfsuite.Tagger()\\n        estimator.open(model_path)\\n        # ...\\n```\\n\\n**After:**\\n```python\\nfrom underthesea_core import CRFFeaturizer, CRFTagger\\n\\nclass FastCRFSequenceTagger:\\n    def load(self, base_path):\\n        estimator = CRFTagger()\\n        estimator.load(model_path)\\n        # ...\\n```\\n\\n## Inference Benchmark Results\\n\\nWe benchmarked both versions on the same hardware (AMD EPYC 7713, Linux, Python 3.12) with 100 iterations:\\n\\n| Function | v9.2.1 (pycrfsuite) | v9.2.5 (Rust) | Improvement |\\n|----------|---------------------|---------------|-------------|\\n| word_tokenize | 1.45ms | 1.18ms | **-19%** |\\n| pos_tag | 3.58ms | 2.93ms | **-18%** |\\n| ner | 9.61ms | 8.49ms | **-12%** |\\n| chunk | 6.19ms | 5.65ms | **-9%** |\\n\\n## Why Is Inference Faster?\\n\\n### 1. Unified Runtime\\n\\nBoth `CRFFeaturizer` and `CRFTagger` are now in the same Rust module. This allows:\\n- Shared memory management\\n- No intermediate Python object creation\\n- Potential for future optimizations (e.g., fusing operations)\\n\\n### 2. Optimized Viterbi Implementation\\n\\nOur Rust implementation uses pre-allocated vectors and cache-friendly memory layouts:\\n\\n```rust\\nfn viterbi(&self, attr_ids: &[Vec<u32>]) -> TaggingResult {\\n    let n = attr_ids.len();\\n    let num_labels = self.model.num_labels;\\n\\n    // Pre-allocated score matrix\\n    let mut score = vec![vec![f64::NEG_INFINITY; num_labels]; n];\\n    let mut back = vec![vec![0u32; num_labels]; n];\\n\\n    // Cache emission scores per position\\n    let emission_t = self.model.emission_scores(&attr_ids[t]);\\n\\n    // Direct memory access in inner loop\\n    for y in 0..num_labels {\\n        for y_prev in 0..num_labels {\\n            let trans = self.model.get_transition(y_prev as u32, y as u32);\\n            // ...\\n        }\\n    }\\n}\\n```\\n\\n### 3. Zero-Copy Where Possible\\n\\nPyO3 bindings allow efficient data transfer between Python and Rust without unnecessary copying.\\n\\n### 4. Removed Dependency\\n\\nRemoving `python-crfsuite` also means:\\n- Simpler installation (no C++ compiler needed)\\n- Smaller package size\\n- Fewer potential compatibility issues\\n\\n## Training Optimizations\\n\\nBeyond inference, we also optimized the CRF trainer in `underthesea-core`. The original Rust trainer was **7.2x slower** than python-crfsuite for word segmentation. Through four key optimizations, we made it competitive \u2014 and even faster for some tasks.\\n\\n### CRF Training Algorithm\\n\\nThe trainer uses Limited-memory BFGS (L-BFGS) optimization with Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) extension for L1 regularization:\\n\\n```\\nminimize: L(w) = -log P(y|x) + \u03bb\u2081\u2016w\u2016\u2081 + \u03bb\u2082\u2016w\u2016\u2082\xb2\\n```\\n\\nWhere `\u03bb\u2081 = 1.0` (L1 coefficient) and `\u03bb\u2082 = 0.001` (L2 coefficient).\\n\\nThe core computation is the forward-backward algorithm for computing:\\n1. **Partition function** Z(x) via forward pass\\n2. **Marginal probabilities** P(y_t | x) via forward-backward\\n3. **Gradient** \u2207L(w) = E_model[f] - E_empirical[f]\\n\\nComplexity per sequence: **O(n \xd7 L\xb2)** where n is the sequence length and L is the number of labels.\\n\\nFollowing CRFsuite\'s approach, we use **scaled probability space** instead of log-space:\\n\\n```rust\\n// Instead of: log_alpha[t][y] = logsumexp(log_alpha[t-1] + log_trans + log_state)\\n// We use:     alpha[t][y] = sum(alpha[t-1] * exp_trans) * exp_state * scale\\n```\\n\\nBenefits:\\n- No log/exp in inner loops\\n- Better numerical stability with scaling factors\\n- Matches CRFsuite\'s performance characteristics\\n\\n### Starting Point\\n\\n| Task | python-crfsuite | underthesea-core (original) | Slowdown |\\n|------|-----------------|----------------------------|----------|\\n| Word Segmentation | 2m 34s | 18m 33s | **7.2x slower** |\\n| POS Tagging | 4m 50s | 7m 21s | **1.5x slower** |\\n\\n### Optimization 1: Flat Data Structure for Feature Lookup\\n\\nThe original used nested vectors (`Vec<Vec<(u32, u32)>>`) for feature lookup \u2014 each inner `Vec` separately heap-allocated, causing cache misses for large feature sets (562k features).\\n\\nWe flattened into contiguous arrays with offset indexing:\\n\\n```rust\\n// Contiguous memory, excellent cache locality\\nattr_offsets: Vec<u32>               // attr_id -> start index\\nattr_features_flat: Vec<(u32, u32)>  // flattened (label_id, feature_id) pairs\\n\\n// Lookup: O(1) with sequential memory access\\nlet start = attr_offsets[attr_id];\\nlet end = attr_offsets[attr_id + 1];\\nfor i in start..end {\\n    let (label_id, feature_id) = attr_features_flat[i];\\n    // Process feature...\\n}\\n```\\n\\n**Result**:\\n\\n| Task | Before | After | Speedup |\\n|------|--------|-------|---------|\\n| Word Segmentation | 18m 33s | 1m 49s | **10.2x** |\\n| POS Tagging | 7m 21s | 5m 9s | **1.4x** |\\n\\nThe larger speedup for word segmentation (562k features vs 37k for POS) confirms the feature lookup was the bottleneck.\\n\\n### Optimization 2: Loop Unrolling for Auto-Vectorization\\n\\nThe forward-backward algorithm has O(n \xd7 L\xb2) inner loops. For POS tagging (16 labels = 256 transitions per timestep), we applied 4-way manual loop unrolling to enable SIMD auto-vectorization:\\n\\n```rust\\n// 4-way unrolled for instruction-level parallelism\\nlet chunks = num_labels / 4;\\nfor i in 0..chunks {\\n    let y = i * 4;\\n    let a0 = alpha[curr + y];\\n    let a1 = alpha[curr + y + 1];\\n    let a2 = alpha[curr + y + 2];\\n    let a3 = alpha[curr + y + 3];\\n\\n    let t0 = trans[trans_base + y];\\n    let t1 = trans[trans_base + y + 1];\\n    let t2 = trans[trans_base + y + 2];\\n    let t3 = trans[trans_base + y + 3];\\n\\n    alpha[curr + y]     = a0 + alpha_prev * t0;\\n    alpha[curr + y + 1] = a1 + alpha_prev * t1;\\n    alpha[curr + y + 2] = a2 + alpha_prev * t2;\\n    alpha[curr + y + 3] = a3 + alpha_prev * t3;\\n}\\n```\\n\\n**Result**: POS tagging (10 iterations) went from 25.7s to 17.58s \u2014 a **1.46x speedup**.\\n\\n### Optimization 3: Unsafe Bounds-Check Elimination\\n\\nWe used `unsafe` with `get_unchecked` for hot paths where indices are provably valid, eliminating Rust\'s bounds checks in tight loops:\\n\\n```rust\\n// Safe but slow: 2 bounds checks per iteration\\nfor y in 0..num_labels {\\n    gradient[feature_id] += state_mexp[base + y];\\n}\\n\\n// Unsafe but fast: 0 bounds checks\\nunsafe {\\n    for y in 0..num_labels {\\n        *gradient.get_unchecked_mut(feature_id) +=\\n            *state_mexp.get_unchecked(base + y);\\n    }\\n}\\n```\\n\\nAll `unsafe` blocks are guarded by loop bounds derived from array lengths, assertions, and algorithm invariants.\\n\\n### Optimization 4: Fused Operations\\n\\nSeparate loops for related operations cause redundant memory traversals. We fused them:\\n\\n```rust\\n// Before: 3 separate loops, 3 memory traversals\\nfor y in 0..L { alpha[y] *= exp_state[y]; }\\nfor y in 0..L { sum += alpha[y]; }\\nfor y in 0..L { alpha[y] *= scale; }\\n\\n// After: 1 fused loop + 1 normalization pass\\nlet mut sum = 0.0;\\nfor y in 0..L {\\n    let val = alpha[y] * exp_state[y];\\n    alpha[y] = val;\\n    sum += val;\\n}\\nlet scale = 1.0 / sum;\\nfor y in 0..L { alpha[y] *= scale; }\\n```\\n\\n### Cumulative Optimization Impact\\n\\n| Optimization | Word Seg Speedup | POS Tag Speedup |\\n|--------------|------------------|-----------------|\\n| Baseline (original) | 1.0x | 1.0x |\\n| + Flat data structure | **10.2x** | 1.4x |\\n| + Loop unrolling | 10.2x | **2.1x** |\\n| + Unsafe bounds elim | ~10.2x | ~2.3x |\\n| **Total** | **10.2x** | **2.3x** |\\n\\n### Training Benchmark Results (200 iterations)\\n\\n| Task | Features | Labels | python-crfsuite | underthesea-core | Result |\\n|------|----------|--------|-----------------|------------------|--------|\\n| Word Segmentation | 562,885 | 2 | 2m 2s | **1m 38s** | **1.24x faster** |\\n| POS Tagging | 626,723 | 16 | 4m 3s | 4m 14s | ~equal (4% slower) |\\n\\n### Accuracy Verification\\n\\n| Task | Metric | python-crfsuite | underthesea-core |\\n|------|--------|-----------------|------------------|\\n| Word Segmentation | Syllable Accuracy | 98.89% | **98.89%** |\\n| Word Segmentation | Word F1 | 98.00% | **98.00%** |\\n| POS Tagging | Accuracy | 95.98% | **95.97%** |\\n\\n**Accuracy is identical** \u2014 optimizations only affected performance, not correctness.\\n\\n## What Didn\'t Work\\n\\nWe evaluated several additional optimizations that did not provide significant improvements:\\n\\n- **Explicit SIMD Intrinsics (AVX2)**: The inner loops process only 2-16 labels, too small for explicit SIMD to outperform the compiler\'s auto-vectorization with loop unrolling.\\n- **Parallel Forward-Backward (Rayon)**: Thread-local gradient accumulation overhead from buffer allocation per sequence and gradient merging negated the parallelism benefits. Sequential processing with buffer reuse remains faster.\\n- **Memory Pool for Temporary Buffers**: Already implemented \u2014 the current implementation reuses buffers across sequences within each L-BFGS evaluation. Further pooling across evaluations showed minimal improvement.\\n- **Compressed Sparse Features**: The flat data structure with offset indexing already provides efficient sparse feature access \u2014 additional compression just adds decode overhead.\\n\\n## Key Insight: Different Tasks, Different Bottlenecks\\n\\n| Feature Set Size | Bottleneck | Best Optimization |\\n|------------------|------------|-------------------|\\n| Large (500k+) | Feature lookup (cache misses) | Flat data structure |\\n| Small (&lt;50k) | Forward-backward O(L\xb2) | Loop unrolling |\\n\\n### Why python-crfsuite Was Initially Faster\\n\\nCRFsuite (C implementation) already had:\\n1. **Hand-optimized sparse feature storage** \u2014 similar to our flat structure\\n2. **SIMD-vectorized matrix operations** \u2014 AVX/SSE intrinsics\\n3. **Cache-optimized memory layout** \u2014 column-major for transitions\\n4. **Decades of optimization** \u2014 mature codebase\\n\\nOur flat data structure and loop unrolling effectively replicated these advantages in Rust.\\n\\n### Lessons Learned\\n\\n1. **Profile first** \u2014 the bottleneck was different for each task\\n2. **Data structure matters** \u2014 flat arrays beat nested vectors by 10x\\n3. **Cache locality is critical** \u2014 sequential memory access enables hardware prefetching\\n4. **Unsafe Rust is justified** \u2014 when correctness is provable and performance is critical\\n5. **Incremental migration reduces risk** \u2014 migrating one task at a time allowed validation at each step\\n\\n## Migration Path\\n\\nThe migration was done incrementally across 4 releases:\\n\\n| Version | Changes |\\n|---------|---------|\\n| v9.2.2 | word_tokenize migrated |\\n| v9.2.3 | pos_tag, ner, chunking migrated |\\n| v9.2.4 | CRFTrainer migrated, removed unused files |\\n| v9.2.5 | Removed python-crfsuite dependency |\\n\\n## Model Compatibility\\n\\nThe Rust implementation can load existing `.crfsuite` model files trained by python-crfsuite. No retraining is required.\\n\\n```python\\n# Works with both old and new models\\nfrom underthesea import word_tokenize\\nword_tokenize(\\"H\xe0 N\u1ed9i l\xe0 th\u1ee7 \u0111\xf4 c\u1ee7a Vi\u1ec7t Nam\\")\\n# [\'H\xe0 N\u1ed9i\', \'l\xe0\', \'th\u1ee7 \u0111\xf4\', \'c\u1ee7a\', \'Vi\u1ec7t Nam\']\\n```\\n\\n## Conclusion\\n\\nBy rewriting our CRF implementation in Rust and unifying the pipeline, we achieved:\\n\\n- **12-19% faster inference** across all CRF-based tasks\\n- **1.24x faster training** for word segmentation (10x from original Rust implementation)\\n- **Identical accuracy** \u2014 no degradation from the migration\\n- **Simpler dependency tree** (no python-crfsuite / C++ compiler needed)\\n- **Better maintainability** with a single Rust codebase\\n\\nThe full implementation is available in [underthesea-core](https://github.com/undertheseanlp/underthesea/tree/main/extensions/underthesea_core).\\n\\n## Try It Out\\n\\n```bash\\npip install underthesea==9.2.5\\n```\\n\\n```python\\nfrom underthesea import word_tokenize, pos_tag, ner, chunk\\n\\nword_tokenize(\\"Vi\u1ec7t Nam\\")  # 20% faster!\\n```\\n\\n## Appendix\\n\\n### Hyperparameters\\n\\n| Parameter | Value | Description |\\n|-----------|-------|-------------|\\n| c1 (L1) | 1.0 | L1 regularization coefficient |\\n| c2 (L2) | 0.001 | L2 regularization coefficient |\\n| max_iterations | 200 | Maximum L-BFGS iterations |\\n| linesearch | Backtracking | Line search algorithm for OWL-QN |\\n| max_step_size | 1e20 | Allow large steps (critical for convergence) |\\n\\n### Hardware\\n\\n| Component | Specification |\\n|-----------|---------------|\\n| CPU | AMD EPYC 7713 64-Core Processor |\\n| Platform | Linux |\\n| Rust | 1.75+ (release mode with LTO) |\\n| Python | 3.12 |\\n\\n### Code References\\n\\n| File | Description |\\n|------|-------------|\\n| `underthesea_core/src/crf/trainer.rs` | Main CRF trainer implementation |\\n| `underthesea_core/src/crf/model.rs` | CRF model structure |\\n| `tre-1/scripts/train.py` | POS tagger training script |\\n| `tre-1/scripts/train_word_segmentation.py` | Word segmentation training script |\\n\\n### Detailed Benchmark Results (2026-01-31)\\n\\n**10-Iteration Tests:**\\n\\n| Task | Trainer | Training Time | Accuracy |\\n|------|---------|---------------|----------|\\n| POS Tagging | python-crfsuite | 12.96s | 78.37% |\\n| POS Tagging | underthesea-core | 18.51s | 75.42% |\\n| Word Segmentation | python-crfsuite | 6.78s | 81.44% F1 |\\n| Word Segmentation | underthesea-core | 11.99s | 82.81% F1 |\\n\\n**200-Iteration Tests:**\\n\\n| Task | Trainer | Training Time | Accuracy |\\n|------|---------|---------------|----------|\\n| POS Tagging | python-crfsuite | 243.22s | 95.98% |\\n| POS Tagging | underthesea-core | 254.07s | 95.97% |\\n| Word Segmentation | python-crfsuite | 121.69s | 98.89% / 98.00% F1 |\\n| Word Segmentation | underthesea-core | 98.34s | 98.89% / 98.00% F1 |"}]}}')}}]);