"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[8603],{9153(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"technical-reports/sent-tokenize","title":"Sentence Segmentation","description":"Overview","source":"@site/docs/technical-reports/sent-tokenize.md","sourceDirName":"technical-reports","slug":"/technical-reports/sent-tokenize","permalink":"/docs/technical-reports/sent-tokenize","draft":false,"unlisted":false,"editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/docs/technical-reports/sent-tokenize.md","tags":[],"version":"current","frontMatter":{},"sidebar":"technicalReportsSidebar","previous":{"title":"Text Normalization","permalink":"/docs/technical-reports/text-normalize"},"next":{"title":"Tagging","permalink":"/docs/technical-reports/nlp/tagging"}}');var s=t(4848),r=t(8453);const l={},d="Sentence Segmentation",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Algorithm: Punkt-style Sentence Tokenizer",id:"algorithm-punkt-style-sentence-tokenizer",level:3},{value:"Components",id:"components",level:3},{value:"Algorithm Details",id:"algorithm-details",level:2},{value:"Sentence Boundary Detection",id:"sentence-boundary-detection",level:3},{value:"Boundary Decision Rules",id:"boundary-decision-rules",level:3},{value:"Abbreviation Handling",id:"abbreviation-handling",level:3},{value:"Data Format",id:"data-format",level:2},{value:"punkt_params.json",id:"punkt_paramsjson",level:3},{value:"Usage",id:"usage",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Edge Cases",id:"edge-cases",level:3},{value:"Performance",id:"performance",level:2},{value:"Complexity",id:"complexity",level:3},{value:"Benchmarks",id:"benchmarks",level:3},{value:"Limitations",id:"limitations",level:2},{value:"Future Improvements",id:"future-improvements",level:2},{value:"References",id:"references",level:2},{value:"Changelog",id:"changelog",level:2},{value:"Version 9.1.4 (PR #879)",id:"version-914-pr-879",level:3}];function o(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sentence-segmentation",children:"Sentence Segmentation"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The sentence tokenization module in Underthesea provides Vietnamese sentence boundary detection using a custom Punkt-style algorithm. This document describes the architecture, algorithm, and the migration from NLTK dependency to a standalone implementation."}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"algorithm-punkt-style-sentence-tokenizer",children:"Algorithm: Punkt-style Sentence Tokenizer"}),"\n",(0,s.jsx)(n.p,{children:"The implementation is inspired by the Punkt sentence boundary detection algorithm developed by Kiss and Strunk (2006)."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reference:"})," Tibor Kiss and Jan Strunk. 2006. ",(0,s.jsx)(n.a,{href:"https://aclanthology.org/J06-4003/",children:"Unsupervised Multilingual Sentence Boundary Detection"})]}),"\n",(0,s.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"PunktSentenceTokenizer\n\u251c\u2500\u2500 Configuration\n\u2502   \u251c\u2500\u2500 SENT_END_CHARS: frozenset('.', '?', '!')\n\u2502   \u2514\u2500\u2500 abbrev_types: Set[str] (abbreviation dictionary)\n\u251c\u2500\u2500 Core Methods\n\u2502   \u251c\u2500\u2500 sentences_from_text(): Main tokenization entry point\n\u2502   \u251c\u2500\u2500 _is_sentence_boundary(): Boundary detection logic\n\u2502   \u2514\u2500\u2500 _get_preceding_word(): Abbreviation extraction\n\u2514\u2500\u2500 Data\n    \u2514\u2500\u2500 punkt_params.json (368 Vietnamese abbreviations)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"algorithm-details",children:"Algorithm Details"}),"\n",(0,s.jsx)(n.h3,{id:"sentence-boundary-detection",children:"Sentence Boundary Detection"}),"\n",(0,s.jsx)(n.p,{children:"The algorithm identifies sentence boundaries through a multi-step process:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Punctuation Detection"}),": Scan for sentence-ending punctuation (",(0,s.jsx)(n.code,{children:"."}),", ",(0,s.jsx)(n.code,{children:"?"}),", ",(0,s.jsx)(n.code,{children:"!"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Abbreviation Check"}),": Verify if the period follows a known abbreviation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Analysis"}),": Examine following characters to confirm boundary"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"boundary-decision-rules",children:"Boundary Decision Rules"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Condition"}),(0,s.jsx)(n.th,{children:"Is Boundary?"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'Period after abbreviation (e.g., "Dr.")'}),(0,s.jsx)(n.td,{children:"No"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Punctuation followed by uppercase letter"}),(0,s.jsx)(n.td,{children:"Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Punctuation followed by newline"}),(0,s.jsx)(n.td,{children:"Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Punctuation at end of text"}),(0,s.jsx)(n.td,{children:"Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Single letter followed by period"}),(0,s.jsx)(n.td,{children:"No (treated as abbreviation)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"abbreviation-handling",children:"Abbreviation Handling"}),"\n",(0,s.jsx)(n.p,{children:"The tokenizer maintains a comprehensive abbreviation dictionary:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Built-in Categories:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Single letters (A-Z, a-z)"}),"\n",(0,s.jsx)(n.li,{children:"Vietnamese abbreviations (TP, TS, ThS, BS, PGS, GS, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"English abbreviations (Dr, Mr, Mrs, Prof, Inc, etc.)"}),"\n",(0,s.jsx)(n.li,{children:'Domain-specific terms (e.g., ".hcm", ".hn", "g.m.t")'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained Abbreviations:"})," 368 entries extracted from Vietnamese text corpora"]}),"\n",(0,s.jsx)(n.h2,{id:"data-format",children:"Data Format"}),"\n",(0,s.jsx)(n.h3,{id:"punkt_paramsjson",children:"punkt_params.json"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "abbrev_types": [\n    ".hcm",\n    ".hn",\n    "tp",\n    "ts",\n    "ths",\n    ...\n  ],\n  "sent_starters": []\n}\n'})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Field"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"abbrev_types"}),(0,s.jsx)(n.td,{children:"List[str]"}),(0,s.jsx)(n.td,{children:"Known abbreviations that don't end sentences"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sent_starters"}),(0,s.jsx)(n.td,{children:"List[str]"}),(0,s.jsx)(n.td,{children:"Words that typically start sentences (unused)"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from underthesea import sent_tokenize\n\ntext = \"Xin ch\xe0o. T\xf4i l\xe0 sinh vi\xean.\"\nsentences = sent_tokenize(text)\n# Output: ['Xin ch\xe0o.', 'T\xf4i l\xe0 sinh vi\xean.']\n"})}),"\n",(0,s.jsx)(n.h3,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Empty text\nsent_tokenize(\"\")  # Returns: []\n\n# Single sentence without punctuation\nsent_tokenize(\"h\xf4m nay\")  # Returns: ['h\xf4m nay']\n\n# Abbreviations\nsent_tokenize(\"\xd4ng Dr. Nguy\u1ec5n \u0111\xe3 \u0111\u1ebfn.\")  # Returns: ['\xd4ng Dr. Nguy\u1ec5n \u0111\xe3 \u0111\u1ebfn.']\n\n# Multiple punctuation\nsent_tokenize(\"Th\u1eadt sao?! Tuy\u1ec7t v\u1eddi!\")  # Returns: ['Th\u1eadt sao?!', 'Tuy\u1ec7t v\u1eddi!']\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance",children:"Performance"}),"\n",(0,s.jsx)(n.h3,{id:"complexity",children:"Complexity"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Operation"}),(0,s.jsx)(n.th,{children:"Time Complexity"}),(0,s.jsx)(n.th,{children:"Space Complexity"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Tokenization"}),(0,s.jsx)(n.td,{children:"O(n)"}),(0,s.jsx)(n.td,{children:"O(n)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model Loading"}),(0,s.jsx)(n.td,{children:"O(a)"}),(0,s.jsx)(n.td,{children:"O(a)"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"n = length of input text"}),"\n",(0,s.jsx)(n.li,{children:"a = number of abbreviations (~400)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Text Length"}),(0,s.jsx)(n.th,{children:"Time (ms)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"100 chars"}),(0,s.jsx)(n.td,{children:"<0.1"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1,000 chars"}),(0,s.jsx)(n.td,{children:"~0.5"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"10,000 chars"}),(0,s.jsx)(n.td,{children:"~5"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Benchmarks run on Apple M1, Python 3.11"})}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No Probabilistic Model"}),": Unlike NLTK's Punkt, this implementation uses a fixed abbreviation dictionary rather than a probabilistic model trained on text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No Sentence Starters"}),": The ",(0,s.jsx)(n.code,{children:"sent_starters"})," parameter is preserved in JSON but not used in boundary detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Limited Ellipsis Handling"}),': Sequences like "..." are treated as single boundary markers']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"future-improvements",children:"Future Improvements"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Train a Vietnamese-specific abbreviation model"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add support for quoted speech boundary detection"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",'Handle numeric expressions (e.g., "1.000.000 \u0111\u1ed3ng")']}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add streaming/generator API for large texts"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525."}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.nltk.org/api/nltk.tokenize.punkt.html",children:"NLTK Punkt Tokenizer"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/undertheseanlp/underthesea",children:"Underthesea GitHub Repository"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"changelog",children:"Changelog"}),"\n",(0,s.jsx)(n.h3,{id:"version-914-pr-879",children:"Version 9.1.4 (PR #879)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Removed NLTK dependency"}),"\n",(0,s.jsx)(n.li,{children:"Implemented custom Punkt-style sentence tokenizer"}),"\n",(0,s.jsx)(n.li,{children:"Converted model from pickle to JSON format"}),"\n",(0,s.jsx)(n.li,{children:"Removed unused Tree/TreeSentence classes from conll.py"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},8453(e,n,t){t.d(n,{R:()=>l,x:()=>d});var i=t(6540);const s={},r=i.createContext(s);function l(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);