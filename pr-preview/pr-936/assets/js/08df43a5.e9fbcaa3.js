"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[8928],{4435(e,n,t){t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>o,frontMatter:()=>l,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"api/sent-tokenize","title":"sent_tokenize","description":"Segment text into sentences.","source":"@site/docs/api/sent-tokenize.md","sourceDirName":"api","slug":"/api/sent-tokenize","permalink":"/docs/api/sent-tokenize","draft":false,"unlisted":false,"editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/docs/api/sent-tokenize.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"API Reference","permalink":"/docs/api/"},"next":{"title":"text_normalize","permalink":"/docs/api/text-normalize"}}');var s=t(4848),r=t(8453);const l={},c="sent_tokenize",a={},h=[{value:"Usage",id:"usage",level:2},{value:"Function Signature",id:"function-signature",level:2},{value:"Parameters",id:"parameters",level:2},{value:"Returns",id:"returns",level:2},{value:"Examples",id:"examples",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Multiple Sentences",id:"multiple-sentences",level:3},{value:"Handling Abbreviations",id:"handling-abbreviations",level:3},{value:"Notes",id:"notes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sent_tokenize",children:"sent_tokenize"})}),"\n",(0,s.jsx)(n.p,{children:"Segment text into sentences."}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from underthesea import sent_tokenize\n\ntext = \'Taylor cho bi\u1ebft l\xfac \u0111\u1ea7u c\xf4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\xf4 b\u1ea1n th\xe2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\xf4i qua nhanh ch\xf3ng. Amanda c\u0169ng tho\u1ea3i m\xe1i v\u1edbi m\u1ed1i quan h\u1ec7 n\xe0y.\'\n\nsentences = sent_tokenize(text)\nprint(sentences)\n# [\n#   "Taylor cho bi\u1ebft l\xfac \u0111\u1ea7u c\xf4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\xf4 b\u1ea1n th\xe2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\xf4i qua nhanh ch\xf3ng.",\n#   "Amanda c\u0169ng tho\u1ea3i m\xe1i v\u1edbi m\u1ed1i quan h\u1ec7 n\xe0y."\n# ]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"function-signature",children:"Function Signature"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def sent_tokenize(text: str) -> list[str]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"text"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"str"})}),(0,s.jsx)(n.td,{children:"The input text to segment into sentences"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"list[str]"})}),(0,s.jsx)(n.td,{children:"A list of sentences"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from underthesea import sent_tokenize\n\ntext = \"Xin ch\xe0o. T\xf4i l\xe0 sinh vi\xean. T\xf4i h\u1ecdc \u1edf H\xe0 N\u1ed9i.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['Xin ch\xe0o.', 'T\xf4i l\xe0 sinh vi\xean.', 'T\xf4i h\u1ecdc \u1edf H\xe0 N\u1ed9i.']\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multiple-sentences",children:"Multiple Sentences"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'text = """Vi\u1ec7t Nam l\xe0 m\u1ed9t qu\u1ed1c gia. Th\u1ee7 \u0111\xf4 l\xe0 H\xe0 N\u1ed9i. Th\xe0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\xe0 TP. H\u1ed3 Ch\xed Minh."""\nsentences = sent_tokenize(text)\nprint(len(sentences))  # 3\n'})}),"\n",(0,s.jsx)(n.h3,{id:"handling-abbreviations",children:"Handling Abbreviations"}),"\n",(0,s.jsx)(n.p,{children:"The function handles common Vietnamese abbreviations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"text = \"TP. H\u1ed3 Ch\xed Minh l\xe0 th\xe0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam. D\xe2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['TP. H\u1ed3 Ch\xed Minh l\xe0 th\xe0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam.', 'D\xe2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.']\n"})}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The function uses rule-based sentence boundary detection"}),"\n",(0,s.jsx)(n.li,{children:"It handles common Vietnamese punctuation patterns"}),"\n",(0,s.jsx)(n.li,{children:'Abbreviations like "TP." (th\xe0nh ph\u1ed1) are handled correctly'}),"\n"]})]})}function o(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>l,x:()=>c});var i=t(6540);const s={},r=i.createContext(s);function l(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);