{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Underthesea","text":"<p>Underthesea is a suite of open source Python modules, datasets, and tutorials supporting research and development in Vietnamese Natural Language Processing.</p> <p>We provide an extremely easy API to quickly apply pretrained NLP models to your Vietnamese text.</p> <p>New in v9.1.5</p> <p>Conversational AI Agent is here! Use <code>agent(\"Xin ch\u00e0o\")</code> to chat with an AI assistant specialized in Vietnamese NLP.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install underthesea, simply:</p> <pre><code>pip install underthesea\n</code></pre> <p>Install with extras (note: use quotes in zsh):</p> <pre><code>pip install \"underthesea[deep]\"       # Deep learning support\npip install \"underthesea[voice]\"      # Text-to-Speech support\npip install \"underthesea[prompt]\"     # OpenAI-based classification\npip install \"underthesea[langdetect]\" # Language detection\npip install \"underthesea[agent]\"      # Conversational AI agent\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"Sentence Segmentation - Breaking text into individual sentences <pre><code>&gt;&gt;&gt; from underthesea import sent_tokenize\n&gt;&gt;&gt; text = 'Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng. Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.'\n\n&gt;&gt;&gt; sent_tokenize(text)\n[\n  \"Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng.\",\n  \"Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.\"\n]\n</code></pre> Text Normalization - Standardizing textual data representation <pre><code>&gt;&gt;&gt; from underthesea import text_normalize\n&gt;&gt;&gt; text_normalize(\"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng ph\u00f2ng th\u00ed ngh\u1ecb\u00eam ho\u00e1 h\u1ecdc\")\n\"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng ph\u00f2ng th\u00ed nghi\u1ec7m h\u00f3a h\u1ecdc\"\n</code></pre> Word Segmentation - Dividing text into individual words <pre><code>&gt;&gt;&gt; from underthesea import word_tokenize\n&gt;&gt;&gt; text = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\n\n&gt;&gt;&gt; word_tokenize(text)\n[\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n\n&gt;&gt;&gt; word_tokenize(text, format=\"text\")\n\"Ch\u00e0ng_trai 9X Qu\u1ea3ng_Tr\u1ecb kh\u1edfi_nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\n\n&gt;&gt;&gt; text = \"Vi\u1ec7n Nghi\u00ean C\u1ee9u chi\u1ebfn l\u01b0\u1ee3c qu\u1ed1c gia v\u1ec1 h\u1ecdc m\u00e1y\"\n&gt;&gt;&gt; fixed_words = [\"Vi\u1ec7n Nghi\u00ean C\u1ee9u\", \"h\u1ecdc m\u00e1y\"]\n&gt;&gt;&gt; word_tokenize(text, fixed_words=fixed_words)\n\"Vi\u1ec7n_Nghi\u00ean_C\u1ee9u chi\u1ebfn_l\u01b0\u1ee3c qu\u1ed1c_gia v\u1ec1 h\u1ecdc_m\u00e1y\"\n</code></pre> POS Tagging - Labeling words with their part-of-speech <pre><code>&gt;&gt;&gt; from underthesea import pos_tag\n&gt;&gt;&gt; pos_tag('Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t')\n[('Ch\u1ee3', 'N'),\n ('th\u1ecbt', 'N'),\n ('ch\u00f3', 'N'),\n ('n\u1ed5i ti\u1ebfng', 'A'),\n ('\u1edf', 'E'),\n ('S\u00e0i G\u00f2n', 'Np'),\n ('b\u1ecb', 'V'),\n ('truy qu\u00e9t', 'V')]\n</code></pre> Chunking - Grouping words into meaningful phrases <pre><code>&gt;&gt;&gt; from underthesea import chunk\n&gt;&gt;&gt; text = 'B\u00e1c s\u0129 b\u00e2y gi\u1edd c\u00f3 th\u1ec3 th\u1ea3n nhi\u00ean b\u00e1o tin b\u1ec7nh nh\u00e2n b\u1ecb ung th\u01b0?'\n&gt;&gt;&gt; chunk(text)\n[('B\u00e1c s\u0129', 'N', 'B-NP'),\n ('b\u00e2y gi\u1edd', 'P', 'B-NP'),\n ('c\u00f3 th\u1ec3', 'R', 'O'),\n ('th\u1ea3n nhi\u00ean', 'A', 'B-AP'),\n ('b\u00e1o', 'V', 'B-VP'),\n ('tin', 'N', 'B-NP'),\n ('b\u1ec7nh nh\u00e2n', 'N', 'B-NP'),\n ('b\u1ecb', 'V', 'B-VP'),\n ('ung th\u01b0', 'N', 'B-NP'),\n ('?', 'CH', 'O')]\n</code></pre> Dependency Parsing - Analyzing grammatical structure (requires <code>[deep]</code>) <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import dependency_parse\n&gt;&gt;&gt; text = 'T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19'\n&gt;&gt;&gt; dependency_parse(text)\n[('T\u1ed1i', 5, 'obl:tmod'),\n ('29/11', 1, 'flat:date'),\n (',', 1, 'punct'),\n ('Vi\u1ec7t Nam', 5, 'nsubj'),\n ('th\u00eam', 0, 'root'),\n ('2', 7, 'nummod'),\n ('ca', 5, 'obj'),\n ('m\u1eafc', 7, 'nmod'),\n ('Covid-19', 8, 'nummod')]\n</code></pre> Named Entity Recognition - Identifying named entities <p>CRF Model (Default)</p> <pre><code>&gt;&gt;&gt; from underthesea import ner\n&gt;&gt;&gt; text = 'Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump'\n&gt;&gt;&gt; ner(text)\n[('Ch\u01b0a', 'R', 'O', 'O'),\n ('ti\u1ebft l\u1ed9', 'V', 'B-VP', 'O'),\n ('l\u1ecbch tr\u00ecnh', 'V', 'B-VP', 'O'),\n ('t\u1edbi', 'E', 'B-PP', 'O'),\n ('Vi\u1ec7t Nam', 'Np', 'B-NP', 'B-LOC'),\n ('c\u1ee7a', 'E', 'B-PP', 'O'),\n ('T\u1ed5ng th\u1ed1ng', 'N', 'B-NP', 'O'),\n ('M\u1ef9', 'Np', 'B-NP', 'B-LOC'),\n ('Donald', 'Np', 'B-NP', 'B-PER'),\n ('Trump', 'Np', 'B-NP', 'I-PER')]\n</code></pre> <p>Deep Learning Model (requires <code>[deep]</code>)</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import ner\n&gt;&gt;&gt; text = \"B\u1ed9 C\u00f4ng Th\u01b0\u01a1ng x\u00f3a m\u1ed9t t\u1ed5ng c\u1ee5c, gi\u1ea3m nhi\u1ec1u \u0111\u1ea7u m\u1ed1i\"\n&gt;&gt;&gt; ner(text, deep=True)\n[\n  {'entity': 'B-ORG', 'word': 'B\u1ed9'},\n  {'entity': 'I-ORG', 'word': 'C\u00f4ng'},\n  {'entity': 'I-ORG', 'word': 'Th\u01b0\u01a1ng'}\n]\n</code></pre> Text Classification - Categorizing text into predefined groups <p>CRF Model (Default)</p> <pre><code>&gt;&gt;&gt; from underthesea import classify\n\n&gt;&gt;&gt; classify('HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u')\n['The thao']\n\n&gt;&gt;&gt; classify('H\u1ed9i \u0111\u1ed3ng t\u01b0 v\u1ea5n kinh doanh Asean vinh danh gi\u1ea3i th\u01b0\u1edfng qu\u1ed1c t\u1ebf')\n['Kinh doanh']\n\n&gt;&gt;&gt; classify('L\u00e3i su\u1ea5t t\u1eeb BIDV r\u1ea5t \u01b0u \u0111\u00e3i', domain='bank')\n['INTEREST_RATE']\n</code></pre> <p>Prompt-based Model (requires <code>[prompt]</code>)</p> <pre><code>pip install \"underthesea[prompt]\"\nexport OPENAI_API_KEY=YOUR_KEY\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import classify\n&gt;&gt;&gt; text = \"HLV ngo\u1ea1i \u0111\u00f2i g\u1ea7n t\u1ef7 m\u1ed7i th\u00e1ng d\u1eabn d\u1eaft tuy\u1ec3n Vi\u1ec7t Nam\"\n&gt;&gt;&gt; classify(text, model='prompt')\n'Th\u1ec3 thao'\n</code></pre> Sentiment Analysis - Determining text's emotional tone <pre><code>&gt;&gt;&gt; from underthesea import sentiment\n\n&gt;&gt;&gt; sentiment('h\u00e0ng k\u00e9m ch\u1ea5t lg,ch\u0103n \u0111\u1eafp l\u00ean d\u00ednh l\u00f4ng l\u00e1 kh\u1eafp ng\u01b0\u1eddi. th\u1ea5t v\u1ecdng')\n'negative'\n&gt;&gt;&gt; sentiment('S\u1ea3n ph\u1ea9m h\u01a1i nh\u1ecf so v\u1edbi t\u01b0\u1edfng t\u01b0\u1ee3ng nh\u01b0ng ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t, \u0111\u00f3ng g\u00f3i c\u1ea9n th\u1eadn.')\n'positive'\n\n&gt;&gt;&gt; sentiment('\u0110ky qua \u0111\u01b0\u1eddng link \u1edf b\u00e0i vi\u1ebft n\u00e0y t\u1eeb th\u1ee9 6 m\u00e0 gi\u1edd ch\u01b0a th\u1ea5y ai lhe h\u1ebft', domain='bank')\n['CUSTOMER_SUPPORT#negative']\n&gt;&gt;&gt; sentiment('Xem l\u1ea1i v\u1eabn th\u1ea5y x\u00fac \u0111\u1ed9ng v\u00e0 t\u1ef1 h\u00e0o v\u1ec1 BIDV c\u1ee7a m\u00ecnh', domain='bank')\n['TRADEMARK#positive']\n</code></pre> Translation - Translating Vietnamese to English (requires <code>[deep]</code>) <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import translate\n\n&gt;&gt;&gt; translate(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\")\n'Hanoi is the capital of Vietnam'\n\n&gt;&gt;&gt; translate(\"\u1ea8m th\u1ef1c Vi\u1ec7t Nam n\u1ed5i ti\u1ebfng tr\u00ean th\u1ebf gi\u1edbi\")\n'Vietnamese cuisine is famous around the world'\n\n&gt;&gt;&gt; translate(\"I love Vietnamese food\", source_lang='en', target_lang='vi')\n'T\u00f4i y\u00eau \u1ea9m th\u1ef1c Vi\u1ec7t Nam'\n</code></pre> Language Detection - Identifying the language of text (requires <code>[langdetect]</code>) <pre><code>pip install \"underthesea[langdetect]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import lang_detect\n\n&gt;&gt;&gt; lang_detect(\"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng khi th\u1ea5y cu\u1ed9c s\u1ed1ng h\u00f2a b\u00ecnh t\u1ea1i Vi\u1ec7t Nam\")\n'vi'\n</code></pre> Text-to-Speech - Converting text into spoken audio (requires <code>[voice]</code>) <pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea.pipeline.tts import tts\n\n&gt;&gt;&gt; tts(\"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng khi th\u1ea5y cu\u1ed9c s\u1ed1ng h\u00f2a b\u00ecnh t\u1ea1i Vi\u1ec7t Nam\")\n# A new audio file named `sound.wav` will be generated.\n</code></pre> <p>Command line usage:</p> <pre><code>underthesea tts \"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng khi th\u1ea5y cu\u1ed9c s\u1ed1ng h\u00f2a b\u00ecnh t\u1ea1i Vi\u1ec7t Nam\"\n</code></pre> Conversational AI Agent - Chat with AI for Vietnamese NLP (requires <code>[agent]</code>) <pre><code>pip install \"underthesea[agent]\"\nexport OPENAI_API_KEY=your_api_key\n# Or for Azure OpenAI:\n# export AZURE_OPENAI_API_KEY=your_key\n# export AZURE_OPENAI_ENDPOINT=https://xxx.openai.azure.com\n</code></pre> <pre><code>&gt;&gt;&gt; from underthesea import agent\n\n&gt;&gt;&gt; agent(\"Xin ch\u00e0o!\")\n'Xin ch\u00e0o! T\u00f4i c\u00f3 th\u1ec3 gi\u00fap g\u00ec cho b\u1ea1n?'\n\n&gt;&gt;&gt; agent(\"NLP l\u00e0 g\u00ec?\")\n'NLP (Natural Language Processing) l\u00e0 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean...'\n\n&gt;&gt;&gt; agent(\"Cho v\u00ed d\u1ee5 v\u1ec1 word tokenization ti\u1ebfng Vi\u1ec7t\")\n'Word tokenization trong ti\u1ebfng Vi\u1ec7t l\u00e0 qu\u00e1 tr\u00ecnh...'\n\n# Reset conversation\n&gt;&gt;&gt; agent.reset()\n</code></pre> <p>Supports both OpenAI and Azure OpenAI:</p> <pre><code># Use Azure OpenAI\n&gt;&gt;&gt; agent(\"Hello\", provider=\"azure\", model=\"my-gpt4-deployment\")\n</code></pre>"},{"location":"#vietnamese-nlp-resources","title":"Vietnamese NLP Resources","text":"<p>List available resources:</p> <pre><code>underthesea list-data\n</code></pre> Name Type License Year CP_Vietnamese_VLC_v2_2022 Plaintext Open 2023 UIT_ABSA_RESTAURANT Sentiment Open 2021 UIT_ABSA_HOTEL Sentiment Open 2021 SE_Vietnamese-UBS Sentiment Open 2020 DI_Vietnamese-UVD Dictionary Open 2020 UTS2017-BANK Categorized Open 2017 VNTC Categorized Open 2007 <p>Download resources:</p> <pre><code>underthesea download-data CP_Vietnamese_VLC_v2_2022\n</code></pre>"},{"location":"#up-coming-features","title":"Up Coming Features","text":"<ul> <li>Automatic Speech Recognition</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Facebook Page</li> <li>YouTube Channel</li> <li>Google Colab Notebook</li> </ul>"},{"location":"#support","title":"Support","text":"<p>If you found this project helpful, please consider supporting us.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Add Agent class with custom tools support using OpenAI function calling (GH-712)</li> <li>Add default tools: calculator, datetime, web_search, wikipedia, shell, python, file operations (GH-712)</li> </ul>"},{"location":"CHANGELOG/#915-2026-01-29","title":"9.1.5 - 2026-01-29","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Add Agent API with OpenAI and Azure OpenAI support (GH-745, #890)</li> <li>Add ParserTrainer for dependency parsing (GH-392, #880)</li> <li>Add POS tagger training pipeline (GH-423, #883)</li> </ul>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Add Vietnamese News Dataset (UVN) documentation (GH-885, #888, #889)</li> <li>Add UVB dataset documentation (GH-720, #887)</li> <li>Add UUD-v0.1 dataset documentation (#886)</li> <li>Add UTS Dictionary dataset documentation (GH-622, #884)</li> </ul>"},{"location":"CHANGELOG/#914-2026-01-24","title":"9.1.4 - 2026-01-24","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Implement Logistic Regression library in Rust (#878)</li> <li>Implement CRF library in Rust (#876)</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Remove NLTK dependency (#879)</li> </ul>"},{"location":"CHANGELOG/#security","title":"Security","text":"<ul> <li>Fix Dependabot security vulnerabilities (#874, #875)</li> </ul>"},{"location":"CHANGELOG/#913-2026-01-24","title":"9.1.3 - 2026-01-24","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Add dependency tree visualization (#867)</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Support PyTorch v2 for dependency parsing (#871)</li> <li>Update CP_Vietnamese-VLC README with HuggingFace dataset (#872)</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Fix ValueError when loading DependencyParser from non-existent path (#873)</li> <li>Fix KeyError in Sentence.getattr (#870)</li> <li>Fix TTS UnicodeDecodeError on Windows (#869)</li> <li>Fix underthesea[voice] installation (#868)</li> </ul>"},{"location":"CHANGELOG/#912-2026-01-24","title":"9.1.2 - 2026-01-24","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>Add <code>labels</code> property to <code>classify</code> and <code>sentiment</code> functions (#865)</li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>Fix sklearn &gt;= 1.5 compatibility for loaded models (#866)</li> </ul>"},{"location":"CHANGELOG/#911-2026-01-24","title":"9.1.1 - 2026-01-24","text":""},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li>Fix VERSION file to match pyproject.toml</li> </ul>"},{"location":"CHANGELOG/#910-2026-01-24","title":"9.1.0 - 2026-01-24","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>Vietnamese-English translation module with <code>translate()</code> function (#856)</li> <li>English to Vietnamese translation example in README (#858)</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Support Python 3.14, deprecate Python 3.9 (#862)</li> <li>Migrate from Flake8/Pylint to Ruff for linting (#857)</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>Fix missing sdist (tar.gz) on PyPI for underthesea_core (#859)</li> </ul>"},{"location":"CHANGELOG/#830-2025-09-28","title":"8.3.0 - 2025-09-28","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Train text classification model for dataset VNTC2017_BANK (#819)</li> <li>Add datasets UTS2017_Bank (#822)</li> <li>Add bank model (#824)</li> <li>Build wheels for macOS x86-64 (#820)</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Remove flake8 as runtime dependency (#818)</li> </ul>"},{"location":"CHANGELOG/#820-2025-09-21","title":"8.2.0 - 2025-09-21","text":""},{"location":"CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li>Update project structure, create extensions/lab folder (#812)</li> <li>Create Sonar Core 1 - System Card (#813)</li> <li>Update output format of model sonar_core_1 (#815)</li> </ul>"},{"location":"CHANGELOG/#810-2025-09-21","title":"8.1.0 - 2025-09-21","text":""},{"location":"CHANGELOG/#fixed_4","title":"Fixed","text":"<ul> <li>Fix missing .pkl files (#809)</li> </ul>"},{"location":"CHANGELOG/#801-2025-09-21","title":"8.0.1 - 2025-09-21","text":""},{"location":"CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>Fix missing .txt files (#806)</li> </ul>"},{"location":"CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>Update publish distribution to PyPI workflow (#805)</li> </ul>"},{"location":"CHANGELOG/#security_1","title":"Security","text":"<ul> <li>Security updates for dependencies</li> </ul>"},{"location":"CHANGELOG/#800-2025-09-20","title":"8.0.0 - 2025-09-20","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li>Underthesea Languages v2 (#748)</li> <li>Interactive Page for Most Frequently Used Vietnamese Words (#756)</li> <li>Support Python 3.12, 3.13 (#777)</li> </ul>"},{"location":"CHANGELOG/#changed_5","title":"Changed","text":"<ul> <li>Update PyO3 API usage (#768)</li> <li>Update project structure (#790)</li> </ul>"},{"location":"CHANGELOG/#fixed_6","title":"Fixed","text":"<ul> <li>Fix wrong global var in sent_tokenize (#764)</li> <li>Fix logo in Readme.rst (#761)</li> </ul>"},{"location":"CHANGELOG/#684-2024-06-22","title":"6.8.4 - 2024-06-22","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>Add lang_detect module (#733)</li> </ul>"},{"location":"CHANGELOG/#changed_6","title":"Changed","text":"<ul> <li>Optimize imports (#741)</li> <li>Remove issue-manager workflow (#726)</li> </ul>"},{"location":"CHANGELOG/#680-2023-09-23","title":"6.8.0 - 2023-09-23","text":""},{"location":"CHANGELOG/#added_9","title":"Added","text":"<ul> <li>Release Source Distribution for underthesea_core (#708)</li> <li>Create docker image for underthesea (#711)</li> </ul>"},{"location":"CHANGELOG/#changed_7","title":"Changed","text":"<ul> <li>Code refactoring (#713)</li> </ul>"},{"location":"CHANGELOG/#fixed_7","title":"Fixed","text":"<ul> <li>Fix permission errors on removing downloaded models (#715)</li> </ul>"},{"location":"CHANGELOG/#670-2023-07-28","title":"6.7.0 - 2023-07-28","text":""},{"location":"CHANGELOG/#added_10","title":"Added","text":"<ul> <li>Zero shot classification with OpenAI API (#700)</li> </ul>"},{"location":"CHANGELOG/#660-2023-07-27","title":"6.6.0 - 2023-07-27","text":""},{"location":"CHANGELOG/#fixed_8","title":"Fixed","text":"<ul> <li>Fix bug word_tokenize (#697)</li> </ul>"},{"location":"CHANGELOG/#650-2023-07-14","title":"6.5.0 - 2023-07-14","text":""},{"location":"CHANGELOG/#fixed_9","title":"Fixed","text":"<ul> <li>Fix text_normalizer token rules</li> </ul>"},{"location":"CHANGELOG/#640-2023-07-14","title":"6.4.0 - 2023-07-14","text":""},{"location":"CHANGELOG/#fixed_10","title":"Fixed","text":"<ul> <li>Fix fixed_words regex</li> </ul>"},{"location":"CHANGELOG/#630-2023-06-28","title":"6.3.0 - 2023-06-28","text":""},{"location":"CHANGELOG/#added_11","title":"Added","text":"<ul> <li>Support MacOS ARM</li> </ul>"},{"location":"CHANGELOG/#620-2023-03-04","title":"6.2.0 - 2023-03-04","text":""},{"location":"CHANGELOG/#added_12","title":"Added","text":"<ul> <li>Add Text to Speech API (#668)</li> <li>Provide training script for word segmentation, pos tagging, and NER (#666)</li> <li>Create UTS_Dictionary v1.0 datasets (#663)</li> </ul>"},{"location":"CHANGELOG/#614-2023-02-26","title":"6.1.4 - 2023-02-26","text":""},{"location":"CHANGELOG/#added_13","title":"Added","text":"<ul> <li>Support underthesea_core with Python 3.11 (#659)</li> </ul>"},{"location":"CHANGELOG/#612-2023-02-15","title":"6.1.2 - 2023-02-15","text":""},{"location":"CHANGELOG/#added_14","title":"Added","text":"<ul> <li>Add option fixed_words to tokenize and word_tokenize API (#649)</li> </ul>"},{"location":"CHANGELOG/#600-2023-01-01","title":"6.0.0 - 2023-01-01","text":""},{"location":"CHANGELOG/#changed_8","title":"Changed","text":"<ul> <li>Version bump for 2023</li> </ul>"},{"location":"CHANGELOG/#141-2022-12-17","title":"1.4.1 - 2022-12-17","text":""},{"location":"CHANGELOG/#added_15","title":"Added","text":"<ul> <li>Create underthesea app</li> <li>Add viet2ipa module</li> <li>Training NER model with VLSP2016 dataset using BERT</li> </ul>"},{"location":"CHANGELOG/#removed_1","title":"Removed","text":"<ul> <li>Remove unidecode as a dependency</li> </ul>"},{"location":"CHANGELOG/#135-2022-10-31","title":"1.3.5 - 2022-10-31","text":""},{"location":"CHANGELOG/#added_16","title":"Added","text":"<ul> <li>Add Text Normalization module</li> <li>Release underthesea_core version 0.0.5a2</li> <li>Support GLIBC_2.17</li> </ul>"},{"location":"CHANGELOG/#changed_9","title":"Changed","text":"<ul> <li>Update resources path</li> </ul>"},{"location":"CHANGELOG/#fixed_11","title":"Fixed","text":"<ul> <li>Fix function word_tokenize</li> </ul>"},{"location":"CHANGELOG/#134-2022-01-08","title":"1.3.4 - 2022-01-08","text":""},{"location":"CHANGELOG/#added_17","title":"Added","text":"<ul> <li>Demo chatbot with rasa</li> <li>Lite version of underthesea</li> <li>Add build for Windows</li> </ul>"},{"location":"CHANGELOG/#changed_10","title":"Changed","text":"<ul> <li>Increase word_tokenize speed 1.5 times</li> </ul>"},{"location":"CHANGELOG/#133-2021-09-02","title":"1.3.3 - 2021-09-02","text":""},{"location":"CHANGELOG/#changed_11","title":"Changed","text":"<ul> <li>Update torch and transformer dependency</li> </ul>"},{"location":"CHANGELOG/#132-2021-08-04","title":"1.3.2 - 2021-08-04","text":""},{"location":"CHANGELOG/#added_18","title":"Added","text":"<ul> <li>Publish two ABSA open datasets</li> <li>Add pipeline folder</li> </ul>"},{"location":"CHANGELOG/#changed_12","title":"Changed","text":"<ul> <li>Migrate from travis-ci to github actions</li> <li>Update ParserTrainer</li> </ul>"},{"location":"CHANGELOG/#131-2021-01-11","title":"1.3.1 - 2021-01-11","text":""},{"location":"CHANGELOG/#added_19","title":"Added","text":"<ul> <li>Add ClassifierTrainer</li> <li>Add 3 new datasets</li> </ul>"},{"location":"CHANGELOG/#changed_13","title":"Changed","text":"<ul> <li>Compatible with newer version of scikit-learn</li> <li>Retrain classification and sentiment models</li> </ul>"},{"location":"CHANGELOG/#130-2020-12-11","title":"1.3.0 - 2020-12-11","text":""},{"location":"CHANGELOG/#added_20","title":"Added","text":"<ul> <li>Dependency Parsing</li> </ul>"},{"location":"CHANGELOG/#removed_2","title":"Removed","text":"<ul> <li>Remove languageflow dependency</li> <li>Remove tabulate dependency</li> </ul>"},{"location":"CHANGELOG/#100-2017-03-01","title":"1.0.0 - 2017-03-01","text":""},{"location":"CHANGELOG/#added_21","title":"Added","text":"<ul> <li>First release on PyPI</li> <li>First release on ReadTheDocs</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides complete API documentation for all Underthesea functions.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":"Function Description Install <code>sent_tokenize</code> Sentence segmentation Core <code>text_normalize</code> Text normalization Core <code>word_tokenize</code> Word segmentation Core <code>pos_tag</code> Part-of-speech tagging Core <code>chunk</code> Phrase chunking Core <code>ner</code> Named entity recognition Core <code>classify</code> Text classification Core <code>sentiment</code> Sentiment analysis Core"},{"location":"api/#deep-learning-functions","title":"Deep Learning Functions","text":"Function Description Install <code>dependency_parse</code> Dependency parsing <code>[deep]</code> <code>translate</code> Vietnamese-English translation <code>[deep]</code>"},{"location":"api/#additional-functions","title":"Additional Functions","text":"Function Description Install <code>lang_detect</code> Language detection <code>[langdetect]</code> <code>tts</code> Text-to-speech <code>[voice]</code> <code>agent</code> Conversational AI agent <code>[agent]</code>"},{"location":"api/#quick-import","title":"Quick Import","text":"<p>All main functions can be imported directly from <code>underthesea</code>:</p> <pre><code>from underthesea import (\n    sent_tokenize,\n    text_normalize,\n    word_tokenize,\n    pos_tag,\n    chunk,\n    ner,\n    classify,\n    sentiment,\n    dependency_parse,  # requires [deep]\n    translate,         # requires [deep]\n    lang_detect,       # requires [langdetect]\n    agent,             # requires [agent]\n)\n</code></pre>"},{"location":"api/#common-parameters","title":"Common Parameters","text":"<p>Many functions share common parameters:</p>"},{"location":"api/#format","title":"<code>format</code>","text":"<p>Controls output format:</p> <ul> <li><code>None</code> (default): Returns a list</li> <li><code>\"text\"</code>: Returns a string with underscores joining multi-word tokens</li> </ul> <pre><code>word_tokenize(\"Vi\u1ec7t Nam\", format=None)   # ['Vi\u1ec7t Nam']\nword_tokenize(\"Vi\u1ec7t Nam\", format=\"text\") # 'Vi\u1ec7t_Nam'\n</code></pre>"},{"location":"api/#model","title":"<code>model</code>","text":"<p>Specifies which model to use:</p> <pre><code># Use default model\nner(\"text\")\n\n# Use specific model\nner(\"text\", deep=True)  # Use deep learning model\nclassify(\"text\", model='prompt')  # Use OpenAI model\n</code></pre>"},{"location":"api/#domain","title":"<code>domain</code>","text":"<p>Specifies the domain for domain-specific models:</p> <pre><code>classify(\"text\", domain='bank')\nsentiment(\"text\", domain='bank')\n</code></pre>"},{"location":"api/agent/","title":"agent","text":"<p>Conversational AI agent for Vietnamese language tasks using OpenAI or Azure OpenAI.</p>"},{"location":"api/agent/#usage","title":"Usage","text":"<pre><code>from underthesea import agent\n\nresponse = agent(\"Xin ch\u00e0o, NLP l\u00e0 g\u00ec?\")\nprint(response)\n# NLP (Natural Language Processing) l\u00e0 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean...\n</code></pre>"},{"location":"api/agent/#installation","title":"Installation","text":"<pre><code>pip install \"underthesea[agent]\"\n</code></pre>"},{"location":"api/agent/#configuration","title":"Configuration","text":""},{"location":"api/agent/#environment-variables","title":"Environment Variables","text":"OpenAIAzure OpenAI <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_MODEL=\"gpt-4o-mini\"  # optional\n</code></pre> <pre><code>export AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://xxx.openai.azure.com\"\nexport AZURE_OPENAI_DEPLOYMENT=\"my-gpt4-deployment\"  # optional\nexport AZURE_OPENAI_API_VERSION=\"2024-02-01\"  # optional\n</code></pre>"},{"location":"api/agent/#environment-variables-reference","title":"Environment Variables Reference","text":"Provider Variable Required Description OpenAI <code>OPENAI_API_KEY</code> Yes OpenAI API key OpenAI <code>OPENAI_MODEL</code> No Model name (default: <code>gpt-4o-mini</code>) Azure <code>AZURE_OPENAI_API_KEY</code> Yes Azure OpenAI API key Azure <code>AZURE_OPENAI_ENDPOINT</code> Yes Azure OpenAI endpoint URL Azure <code>AZURE_OPENAI_DEPLOYMENT</code> No Deployment name (default: <code>gpt-4o-mini</code>) Azure <code>AZURE_OPENAI_API_VERSION</code> No API version (default: <code>2024-02-01</code>)"},{"location":"api/agent/#function-signature","title":"Function Signature","text":"<pre><code>def agent(\n    message: str,\n    model: str | None = None,\n    system_prompt: str | None = None,\n    provider: str | None = None,\n    api_key: str | None = None,\n    azure_endpoint: str | None = None,\n    azure_api_version: str | None = None,\n) -&gt; str\n</code></pre>"},{"location":"api/agent/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>message</code> <code>str</code> User message to send <code>model</code> <code>str</code> <code>None</code> Model/deployment name. Falls back to env var or <code>gpt-4o-mini</code> <code>system_prompt</code> <code>str</code> <code>None</code> Custom system prompt <code>provider</code> <code>str</code> <code>None</code> Provider: <code>\"openai\"</code> or <code>\"azure\"</code>. Auto-detected if not specified <code>api_key</code> <code>str</code> <code>None</code> API key. Falls back to environment variable <code>azure_endpoint</code> <code>str</code> <code>None</code> Azure endpoint URL. Falls back to <code>AZURE_OPENAI_ENDPOINT</code> <code>azure_api_version</code> <code>str</code> <code>None</code> Azure API version. Falls back to env var or <code>2024-02-01</code>"},{"location":"api/agent/#returns","title":"Returns","text":"Type Description <code>str</code> Assistant response"},{"location":"api/agent/#methods","title":"Methods","text":""},{"location":"api/agent/#agentreset","title":"<code>agent.reset()</code>","text":"<p>Clear conversation history.</p> <pre><code>agent.reset()\n</code></pre>"},{"location":"api/agent/#agenthistory","title":"<code>agent.history</code>","text":"<p>Get conversation history (read-only copy).</p> <pre><code>history = agent.history\n# [{'role': 'user', 'content': '...'}, {'role': 'assistant', 'content': '...'}]\n</code></pre>"},{"location":"api/agent/#examples","title":"Examples","text":""},{"location":"api/agent/#basic-conversation","title":"Basic Conversation","text":"<pre><code>from underthesea import agent\n\n# First message\nresponse = agent(\"Xin ch\u00e0o!\")\nprint(response)\n# Xin ch\u00e0o! T\u00f4i c\u00f3 th\u1ec3 gi\u00fap g\u00ec cho b\u1ea1n?\n\n# Follow-up (history is maintained)\nresponse = agent(\"NLP l\u00e0 g\u00ec?\")\nprint(response)\n# NLP (Natural Language Processing) l\u00e0...\n\n# Check history\nprint(len(agent.history))  # 4 (2 user + 2 assistant messages)\n\n# Reset conversation\nagent.reset()\nprint(len(agent.history))  # 0\n</code></pre>"},{"location":"api/agent/#custom-system-prompt","title":"Custom System Prompt","text":"<pre><code>from underthesea import agent\n\nresponse = agent(\n    \"Ch\u00e0o b\u1ea1n\",\n    system_prompt=\"B\u1ea1n l\u00e0 tr\u1ee3 l\u00fd chuy\u00ean v\u1ec1 \u1ea9m th\u1ef1c Vi\u1ec7t Nam.\"\n)\n</code></pre>"},{"location":"api/agent/#custom-model","title":"Custom Model","text":"<pre><code>from underthesea import agent\n\n# Use GPT-4\nresponse = agent(\"Gi\u1ea3i th\u00edch machine learning\", model=\"gpt-4\")\n\n# Use GPT-4 Turbo\nresponse = agent(\"Gi\u1ea3i th\u00edch deep learning\", model=\"gpt-4-turbo\")\n</code></pre>"},{"location":"api/agent/#azure-openai","title":"Azure OpenAI","text":"<pre><code>from underthesea import agent\n\n# Using environment variables (recommended)\n# export AZURE_OPENAI_API_KEY=\"...\"\n# export AZURE_OPENAI_ENDPOINT=\"https://xxx.openai.azure.com\"\n# export AZURE_OPENAI_DEPLOYMENT=\"my-gpt4\"\n\nresponse = agent(\"Xin ch\u00e0o\")\n\n# Or explicit configuration\nresponse = agent(\n    \"Xin ch\u00e0o\",\n    provider=\"azure\",\n    api_key=\"your-azure-key\",\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    model=\"your-deployment-name\"\n)\n</code></pre>"},{"location":"api/agent/#vietnamese-nlp-assistant","title":"Vietnamese NLP Assistant","text":"<pre><code>from underthesea import agent, word_tokenize, pos_tag\n\n# Use agent to explain NLP concepts\ntext = \"H\u1ecdc m\u00e1y l\u00e0 g\u00ec?\"\nresponse = agent(text)\nprint(response)\n\n# Combine with other underthesea functions\ntext = \"Vi\u1ec7t Nam l\u00e0 qu\u1ed1c gia xinh \u0111\u1eb9p\"\ntokens = word_tokenize(text)\ntags = pos_tag(text)\n\n# Ask agent to explain the results\nresponse = agent(f\"Gi\u1ea3i th\u00edch k\u1ebft qu\u1ea3 POS tagging: {tags}\")\nprint(response)\n</code></pre>"},{"location":"api/agent/#llm-class","title":"LLM Class","text":"<p>For more control, use the <code>LLM</code> class directly:</p> <pre><code>from underthesea.agent import LLM\n\n# Initialize\nllm = LLM(model=\"gpt-4\")\n\n# Chat with custom messages\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a Vietnamese language expert.\"},\n    {\"role\": \"user\", \"content\": \"Explain word segmentation in Vietnamese.\"}\n]\nresponse = llm.chat(messages)\nprint(response)\n\n# Check provider\nprint(llm.provider)  # 'openai' or 'azure'\nprint(llm.model)     # 'gpt-4'\n</code></pre>"},{"location":"api/agent/#agent-class-with-tools","title":"Agent Class with Tools","text":"<p>Create custom agents with function calling support using OpenAI's tools API.</p>"},{"location":"api/agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea.agent import Agent, Tool\n\n# Define a tool as a Python function\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get current weather for a location.\"\"\"\n    return {\"location\": location, \"temp\": 25, \"condition\": \"sunny\"}\n\n# Create agent with tools\nmy_agent = Agent(\n    name=\"weather_assistant\",\n    tools=[Tool(get_weather, description=\"Get weather for a city\")],\n    instruction=\"You are a helpful weather assistant.\"\n)\n\n# Use the agent - it will automatically call tools when needed\nresponse = my_agent(\"What's the weather in Hanoi?\")\nprint(response)\n# The weather in Hanoi is 25\u00b0C and sunny.\n\n# Reset conversation\nmy_agent.reset()\n</code></pre>"},{"location":"api/agent/#agent-constructor","title":"Agent Constructor","text":"<pre><code>Agent(\n    name: str,\n    tools: list[Tool] | None = None,\n    instruction: str | None = None,\n    max_iterations: int = 10,\n)\n</code></pre> Parameter Type Default Description <code>name</code> <code>str</code> Agent name <code>tools</code> <code>list[Tool]</code> <code>None</code> List of tools available to the agent <code>instruction</code> <code>str</code> <code>\"You are a helpful assistant.\"</code> System instruction <code>max_iterations</code> <code>int</code> <code>10</code> Maximum tool calling iterations"},{"location":"api/agent/#tool-class","title":"Tool Class","text":"<p>Wrap Python functions as agent tools:</p> <pre><code>from underthesea.agent import Tool\n\ndef search(query: str, limit: int = 10) -&gt; list:\n    \"\"\"Search for items matching the query.\"\"\"\n    return [{\"title\": f\"Result for {query}\"}]\n\ntool = Tool(\n    func=search,\n    name=\"web_search\",           # Optional, defaults to function name\n    description=\"Search the web\" # Optional, defaults to docstring\n)\n\n# Convert to OpenAI format\nopenai_format = tool.to_openai_tool()\n\n# Execute directly\nresult = tool(query=\"python\", limit=5)\n</code></pre>"},{"location":"api/agent/#tool-constructor","title":"Tool Constructor","text":"<pre><code>Tool(\n    func: Callable,\n    name: str | None = None,\n    description: str | None = None,\n)\n</code></pre> Parameter Type Default Description <code>func</code> <code>Callable</code> The function to wrap <code>name</code> <code>str</code> <code>None</code> Tool name (defaults to function name) <code>description</code> <code>str</code> <code>None</code> Tool description (defaults to docstring)"},{"location":"api/agent/#supported-parameter-types","title":"Supported Parameter Types","text":"<p>The Tool class automatically extracts JSON schema from function signatures:</p> Python Type JSON Schema Type <code>str</code> <code>string</code> <code>int</code> <code>integer</code> <code>float</code> <code>number</code> <code>bool</code> <code>boolean</code> <code>list</code> <code>array</code>"},{"location":"api/agent/#multiple-tools-example","title":"Multiple Tools Example","text":"<pre><code>from underthesea.agent import Agent, Tool\n\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get current weather for a location.\"\"\"\n    return {\"location\": location, \"temp\": 25}\n\ndef search_news(query: str) -&gt; str:\n    \"\"\"Search Vietnamese news articles.\"\"\"\n    return f\"Found articles about: {query}\"\n\ndef translate_text(text: str, target_lang: str = \"en\") -&gt; str:\n    \"\"\"Translate Vietnamese text.\"\"\"\n    return f\"Translated: {text}\"\n\nagent = Agent(\n    name=\"multi_tool_agent\",\n    tools=[\n        Tool(get_weather),\n        Tool(search_news),\n        Tool(translate_text, description=\"Translate Vietnamese to other languages\"),\n    ],\n    instruction=\"You are a helpful Vietnamese assistant with access to weather, news, and translation tools.\"\n)\n\n# The agent decides which tool to use based on the query\nresponse = agent(\"Th\u1eddi ti\u1ebft \u1edf \u0110\u00e0 N\u1eb5ng th\u1ebf n\u00e0o?\")  # Uses get_weather\nresponse = agent(\"Tin t\u1ee9c v\u1ec1 AI h\u00f4m nay\")          # Uses search_news\nresponse = agent(\"D\u1ecbch 'Xin ch\u00e0o' sang ti\u1ebfng Anh\") # Uses translate_text\n</code></pre>"},{"location":"api/agent/#agent-without-tools","title":"Agent without Tools","text":"<p>Agent also works without tools as a simple conversational agent:</p> <pre><code>from underthesea.agent import Agent\n\nsimple_agent = Agent(\n    name=\"chatbot\",\n    instruction=\"You are a friendly Vietnamese chatbot.\"\n)\n\nresponse = simple_agent(\"Xin ch\u00e0o!\")\nprint(response)\n</code></pre>"},{"location":"api/agent/#default-tools","title":"Default Tools","text":"<p>Pre-built tools similar to LangChain/OpenAI tools for common tasks.</p>"},{"location":"api/agent/#tool-collections","title":"Tool Collections","text":"Collection Tools Description <code>default_tools</code> 12 tools All default tools <code>core_tools</code> 4 tools Safe utilities (datetime, calculator, string, json) <code>web_tools</code> 3 tools Web operations (search, fetch, wikipedia) <code>system_tools</code> 5 tools System operations (file, shell, python)"},{"location":"api/agent/#core-tools","title":"Core Tools","text":"Tool Description <code>current_datetime_tool</code> Get current date, time, weekday <code>calculator_tool</code> Evaluate math expressions (supports sqrt, sin, cos, log, pi, e) <code>string_length_tool</code> Count characters, words, lines in text <code>json_parse_tool</code> Parse JSON strings"},{"location":"api/agent/#web-tools","title":"Web Tools","text":"Tool Description <code>web_search_tool</code> Search the web using DuckDuckGo (no API key) <code>fetch_url_tool</code> Fetch content from a URL <code>wikipedia_tool</code> Search Wikipedia (supports Vietnamese and English)"},{"location":"api/agent/#system-tools","title":"System Tools","text":"Tool Description <code>read_file_tool</code> Read content from a file <code>write_file_tool</code> Write content to a file <code>list_directory_tool</code> List files and directories <code>shell_tool</code> Run shell commands <code>python_tool</code> Execute Python code"},{"location":"api/agent/#using-default-tools","title":"Using Default Tools","text":"<pre><code>from underthesea.agent import Agent, default_tools\n\n# Create agent with all default tools\nmy_agent = Agent(\n    name=\"assistant\",\n    tools=default_tools,\n    instruction=\"You are a helpful assistant with access to various tools.\"\n)\n\n# Agent can now use any tool automatically\nmy_agent(\"What time is it?\")           # Uses current_datetime_tool\nmy_agent(\"Calculate sqrt(144) + 10\")   # Uses calculator_tool\nmy_agent(\"Search for Python tutorials\") # Uses web_search_tool\nmy_agent(\"List files in current dir\")  # Uses list_directory_tool\n</code></pre>"},{"location":"api/agent/#using-specific-tool-collections","title":"Using Specific Tool Collections","text":"<pre><code>from underthesea.agent import Agent, core_tools, web_tools\n\n# Safe agent with only core tools (no system access)\nsafe_agent = Agent(\n    name=\"safe_assistant\",\n    tools=core_tools,\n)\n\n# Web-enabled agent\nweb_agent = Agent(\n    name=\"web_assistant\",\n    tools=core_tools + web_tools,\n)\n</code></pre>"},{"location":"api/agent/#using-tools-directly","title":"Using Tools Directly","text":"<pre><code>from underthesea.agent import calculator_tool, current_datetime_tool, wikipedia_tool\n\n# Call tools directly (without LLM)\nresult = calculator_tool(expression=\"2 ** 10\")\nprint(result)  # {'expression': '2 ** 10', 'result': 1024}\n\nnow = current_datetime_tool()\nprint(now)  # {'datetime': '...', 'date': '...', 'weekday': 'Friday', ...}\n\nwiki = wikipedia_tool(query=\"H\u00e0 N\u1ed9i\", lang=\"vi\")\nprint(wiki[\"summary\"])  # Wikipedia summary about Hanoi\n</code></pre>"},{"location":"api/agent/#notes","title":"Notes","text":"<ul> <li>The agent maintains conversation history across calls</li> <li>Use <code>agent.reset()</code> to start a new conversation</li> <li>Azure OpenAI is preferred when both credentials are available</li> <li>Default system prompt focuses on Vietnamese language and NLP tasks</li> <li>First call may be slower due to client initialization</li> <li>Agent with tools uses OpenAI's function calling API</li> <li>Tools are automatically called when the model decides they are needed</li> <li><code>max_iterations</code> prevents infinite tool calling loops</li> </ul>"},{"location":"api/chunk/","title":"chunk","text":"<p>Group words into meaningful phrases (chunking/shallow parsing).</p>"},{"location":"api/chunk/#usage","title":"Usage","text":"<pre><code>from underthesea import chunk\n\ntext = \"B\u00e1c s\u0129 b\u00e2y gi\u1edd c\u00f3 th\u1ec3 th\u1ea3n nhi\u00ean b\u00e1o tin b\u1ec7nh nh\u00e2n b\u1ecb ung th\u01b0?\"\nchunks = chunk(text)\nprint(chunks)\n# [('B\u00e1c s\u0129', 'N', 'B-NP'),\n#  ('b\u00e2y gi\u1edd', 'P', 'B-NP'),\n#  ('c\u00f3 th\u1ec3', 'R', 'O'),\n#  ('th\u1ea3n nhi\u00ean', 'A', 'B-AP'),\n#  ('b\u00e1o', 'V', 'B-VP'),\n#  ('tin', 'N', 'B-NP'),\n#  ('b\u1ec7nh nh\u00e2n', 'N', 'B-NP'),\n#  ('b\u1ecb', 'V', 'B-VP'),\n#  ('ung th\u01b0', 'N', 'B-NP'),\n#  ('?', 'CH', 'O')]\n</code></pre>"},{"location":"api/chunk/#function-signature","title":"Function Signature","text":"<pre><code>def chunk(\n    sentence: str,\n    format: str = None\n) -&gt; list[tuple[str, str, str]]\n</code></pre>"},{"location":"api/chunk/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to chunk <code>format</code> <code>str</code> <code>None</code> Output format (currently only <code>None</code> supported)"},{"location":"api/chunk/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, str, str]]</code> List of (word, POS tag, chunk tag) tuples"},{"location":"api/chunk/#chunk-tags","title":"Chunk Tags","text":"Tag Description <code>B-NP</code> Beginning of Noun Phrase <code>I-NP</code> Inside Noun Phrase <code>B-VP</code> Beginning of Verb Phrase <code>I-VP</code> Inside Verb Phrase <code>B-AP</code> Beginning of Adjective Phrase <code>I-AP</code> Inside Adjective Phrase <code>B-PP</code> Beginning of Prepositional Phrase <code>I-PP</code> Inside Prepositional Phrase <code>O</code> Outside any chunk"},{"location":"api/chunk/#examples","title":"Examples","text":""},{"location":"api/chunk/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import chunk\n\ntext = \"B\u00e1c s\u0129 b\u00e2y gi\u1edd c\u00f3 th\u1ec3 th\u1ea3n nhi\u00ean b\u00e1o tin b\u1ec7nh nh\u00e2n b\u1ecb ung th\u01b0?\"\nchunks = chunk(text)\nfor word, pos, chunk_tag in chunks:\n    print(f\"{word:15} {pos:5} {chunk_tag}\")\n</code></pre>"},{"location":"api/chunk/#extracting-noun-phrases","title":"Extracting Noun Phrases","text":"<pre><code>text = \"Sinh vi\u00ean \u0110\u1ea1i h\u1ecdc B\u00e1ch Khoa H\u00e0 N\u1ed9i \u0111\u1ea1t gi\u1ea3i nh\u1ea5t\"\nchunks = chunk(text)\n\n# Extract noun phrases\ncurrent_np = []\nnoun_phrases = []\n\nfor word, pos, chunk_tag in chunks:\n    if chunk_tag == 'B-NP':\n        if current_np:\n            noun_phrases.append(' '.join(current_np))\n        current_np = [word]\n    elif chunk_tag == 'I-NP':\n        current_np.append(word)\n    else:\n        if current_np:\n            noun_phrases.append(' '.join(current_np))\n            current_np = []\n\nif current_np:\n    noun_phrases.append(' '.join(current_np))\n\nprint(noun_phrases)\n</code></pre>"},{"location":"api/chunk/#extracting-verb-phrases","title":"Extracting Verb Phrases","text":"<pre><code>text = \"T\u00f4i \u0111ang h\u1ecdc ti\u1ebfng Vi\u1ec7t v\u00e0 s\u1ebd \u0111i du l\u1ecbch\"\nchunks = chunk(text)\n\nverb_phrases = [word for word, pos, tag in chunks if tag.endswith('VP')]\nprint(verb_phrases)\n</code></pre>"},{"location":"api/chunk/#notes","title":"Notes","text":"<ul> <li>Chunking is performed on top of word segmentation and POS tagging</li> <li>It provides shallow syntactic structure without full parsing</li> <li>Useful for extracting noun phrases, verb phrases, etc.</li> </ul>"},{"location":"api/classify/","title":"classify","text":"<p>Categorize text into predefined categories.</p>"},{"location":"api/classify/#usage","title":"Usage","text":"<pre><code>from underthesea import classify\n\ntext = \"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\"\ncategory = classify(text)\nprint(category)\n# ['The thao']\n</code></pre>"},{"location":"api/classify/#function-signature","title":"Function Signature","text":"<pre><code>def classify(\n    X: str,\n    domain: str = None,\n    model: str = None\n) -&gt; list[str] | str\n</code></pre>"},{"location":"api/classify/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> <code>str</code> The input text to classify <code>domain</code> <code>str</code> <code>None</code> Domain for classification (<code>'bank'</code>) <code>model</code> <code>str</code> <code>None</code> Model type (<code>'prompt'</code> for OpenAI)"},{"location":"api/classify/#returns","title":"Returns","text":"Type Description <code>list[str]</code> List of predicted categories"},{"location":"api/classify/#available-domains","title":"Available Domains","text":""},{"location":"api/classify/#general-default","title":"General (Default)","text":"<p>News topic classification:</p> Category Description <code>The thao</code> Sports <code>Kinh doanh</code> Business <code>Chinh tri Xa hoi</code> Politics &amp; Society <code>Van hoa</code> Culture <code>Khoa hoc</code> Science <code>Phap luat</code> Law <code>Suc khoe</code> Health <code>Doi song</code> Lifestyle <code>The gioi</code> World <code>Vi tinh</code> Technology"},{"location":"api/classify/#bank-domain","title":"Bank Domain","text":"<p>Bank-related topic classification:</p> <pre><code>classify(text, domain='bank')\n</code></pre> Category Description <code>INTEREST_RATE</code> Interest rate related <code>CUSTOMER_SUPPORT</code> Customer service <code>PRODUCT</code> Bank products <code>TRADEMARK</code> Brand/trademark"},{"location":"api/classify/#examples","title":"Examples","text":""},{"location":"api/classify/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import classify\n\n# Sports\nclassify(\"HLV \u0111\u1ea7u ti\u00ean \u1edf Premier League b\u1ecb sa th\u1ea3i sau 4 v\u00f2ng \u0111\u1ea5u\")\n# ['The thao']\n\n# Business\nclassify(\"H\u1ed9i \u0111\u1ed3ng t\u01b0 v\u1ea5n kinh doanh Asean vinh danh gi\u1ea3i th\u01b0\u1edfng qu\u1ed1c t\u1ebf\")\n# ['Kinh doanh']\n</code></pre>"},{"location":"api/classify/#bank-domain_1","title":"Bank Domain","text":"<pre><code>from underthesea import classify\n\nclassify(\"L\u00e3i su\u1ea5t t\u1eeb BIDV r\u1ea5t \u01b0u \u0111\u00e3i\", domain='bank')\n# ['INTEREST_RATE']\n\nclassify(\"Nh\u00e2n vi\u00ean h\u1ed7 tr\u1ee3 r\u1ea5t nhi\u1ec7t t\u00ecnh\", domain='bank')\n# ['CUSTOMER_SUPPORT']\n</code></pre>"},{"location":"api/classify/#prompt-based-model","title":"Prompt-based Model","text":"<p>Requires Installation</p> <pre><code>pip install \"underthesea[prompt]\"\nexport OPENAI_API_KEY=your_api_key\n</code></pre> <pre><code>from underthesea import classify\n\ntext = \"HLV ngo\u1ea1i \u0111\u00f2i g\u1ea7n t\u1ef7 m\u1ed7i th\u00e1ng d\u1eabn d\u1eaft tuy\u1ec3n Vi\u1ec7t Nam\"\nresult = classify(text, model='prompt')\nprint(result)\n# 'Th\u1ec3 thao'\n</code></pre>"},{"location":"api/classify/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>from underthesea import classify\n\ndocuments = [\n    \"\u0110\u1ed9i tuy\u1ec3n Vi\u1ec7t Nam th\u1eafng \u0111\u1eadm trong tr\u1eadn \u0111\u1ea5u\",\n    \"Gi\u00e1 v\u00e0ng t\u0103ng m\u1ea1nh trong tu\u1ea7n qua\",\n    \"Ph\u00e1t hi\u1ec7n virus m\u1edbi g\u00e2y b\u1ec7nh \u1edf ch\u00e2u Phi\"\n]\n\nfor doc in documents:\n    category = classify(doc)\n    print(f\"{doc[:30]}... -&gt; {category}\")\n</code></pre>"},{"location":"api/classify/#accessing-available-labels","title":"Accessing Available Labels","text":"<p>You can access all available category labels using the <code>labels</code> property:</p> <pre><code>from underthesea import classify\n\n# Get labels for general domain\nclassify.labels\n# ['chinh_tri_xa_hoi', 'doi_song', 'khoa_hoc', 'kinh_doanh', 'phap_luat',\n#  'suc_khoe', 'the_gioi', 'the_thao', 'van_hoa', 'vi_tinh']\n\n# Get labels for bank domain\nclassify.bank.labels\n# ['ACCOUNT', 'CARD', 'CUSTOMER_SUPPORT', 'DISCOUNT', 'INTEREST_RATE',\n#  'INTERNET_BANKING', 'LOAN', 'MONEY_TRANSFER', 'OTHER', 'PAYMENT',\n#  'PROMOTION', 'SAVING', 'SECURITY', 'TRADEMARK']\n</code></pre>"},{"location":"api/classify/#notes","title":"Notes","text":"<ul> <li>The default model is trained on Vietnamese news data</li> <li>The bank domain model is specialized for banking feedback</li> <li>Prompt-based model uses OpenAI API and requires an API key</li> <li>First call may take longer due to model loading</li> <li>Use <code>classify.labels</code> to get all available categories for the default domain</li> <li>Use <code>classify.bank.labels</code> to get all available categories for the bank domain</li> </ul>"},{"location":"api/dependency_parse/","title":"dependency_parse","text":"<p>Analyze the grammatical structure and dependencies between words.</p> <p>Requires Deep Learning</p> <p>This function requires the deep learning dependencies: <pre><code>pip install \"underthesea[deep]\"\n</code></pre></p>"},{"location":"api/dependency_parse/#usage","title":"Usage","text":"<pre><code>from underthesea import dependency_parse\n\ntext = \"T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19\"\nresult = dependency_parse(text)\nprint(result)\n# [('T\u1ed1i', 5, 'obl:tmod'),\n#  ('29/11', 1, 'flat:date'),\n#  (',', 1, 'punct'),\n#  ('Vi\u1ec7t Nam', 5, 'nsubj'),\n#  ('th\u00eam', 0, 'root'),\n#  ('2', 7, 'nummod'),\n#  ('ca', 5, 'obj'),\n#  ('m\u1eafc', 7, 'nmod'),\n#  ('Covid-19', 8, 'nummod')]\n</code></pre>"},{"location":"api/dependency_parse/#function-signature","title":"Function Signature","text":"<pre><code>def dependency_parse(sentence: str) -&gt; list[tuple[str, int, str]]\n</code></pre>"},{"location":"api/dependency_parse/#parameters","title":"Parameters","text":"Parameter Type Description <code>sentence</code> <code>str</code> The input text to parse"},{"location":"api/dependency_parse/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, int, str]]</code> List of (word, head_index, relation) tuples <p>Each tuple contains:</p> <ul> <li><code>word</code>: The word token</li> <li><code>head_index</code>: Index of the head word (0 = root)</li> <li><code>relation</code>: The dependency relation type</li> </ul>"},{"location":"api/dependency_parse/#dependency-relations","title":"Dependency Relations","text":"Relation Description <code>root</code> Root of the sentence <code>nsubj</code> Nominal subject <code>obj</code> Object <code>obl</code> Oblique nominal <code>obl:tmod</code> Temporal modifier <code>amod</code> Adjectival modifier <code>nmod</code> Nominal modifier <code>nummod</code> Numeric modifier <code>punct</code> Punctuation <code>flat:date</code> Flat date expression <code>compound</code> Compound word"},{"location":"api/dependency_parse/#examples","title":"Examples","text":""},{"location":"api/dependency_parse/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import dependency_parse\n\ntext = \"T\u1ed1i 29/11, Vi\u1ec7t Nam th\u00eam 2 ca m\u1eafc Covid-19\"\nresult = dependency_parse(text)\n\nfor i, (word, head, rel) in enumerate(result, 1):\n    print(f\"{i}\\t{word}\\t{head}\\t{rel}\")\n# 1    T\u1ed1i         5    obl:tmod\n# 2    29/11       1    flat:date\n# 3    ,           1    punct\n# 4    Vi\u1ec7t Nam    5    nsubj\n# 5    th\u00eam        0    root\n# 6    2           7    nummod\n# 7    ca          5    obj\n# 8    m\u1eafc         7    nmod\n# 9    Covid-19    8    nummod\n</code></pre>"},{"location":"api/dependency_parse/#finding-the-root","title":"Finding the Root","text":"<pre><code>text = \"T\u00f4i y\u00eau Vi\u1ec7t Nam\"\nresult = dependency_parse(text)\n\nroot = [(i, word) for i, (word, head, rel) in enumerate(result, 1) if rel == 'root']\nprint(f\"Root: {root}\")\n# Root: [(2, 'y\u00eau')]\n</code></pre>"},{"location":"api/dependency_parse/#finding-subjects-and-objects","title":"Finding Subjects and Objects","text":"<pre><code>text = \"Sinh vi\u00ean \u0111\u1ecdc s\u00e1ch \u1edf th\u01b0 vi\u1ec7n\"\nresult = dependency_parse(text)\n\nsubjects = [word for word, head, rel in result if rel == 'nsubj']\nobjects = [word for word, head, rel in result if rel == 'obj']\n\nprint(f\"Subjects: {subjects}\")\nprint(f\"Objects: {objects}\")\n</code></pre>"},{"location":"api/dependency_parse/#notes","title":"Notes","text":"<ul> <li>This function uses a transformer-based model</li> <li>First call may take longer due to model loading</li> <li>Requires significant memory for the deep learning model</li> </ul>"},{"location":"api/lang_detect/","title":"lang_detect","text":"<p>Identify the language of text.</p> <p>Requires Language Detection</p> <p>This function requires the langdetect dependencies: <pre><code>pip install \"underthesea[langdetect]\"\n</code></pre></p>"},{"location":"api/lang_detect/#usage","title":"Usage","text":"<pre><code>from underthesea import lang_detect\n\ntext = \"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng khi th\u1ea5y cu\u1ed9c s\u1ed1ng h\u00f2a b\u00ecnh t\u1ea1i Vi\u1ec7t Nam\"\nlang = lang_detect(text)\nprint(lang)\n# 'vi'\n</code></pre>"},{"location":"api/lang_detect/#function-signature","title":"Function Signature","text":"<pre><code>def lang_detect(text: str) -&gt; str\n</code></pre>"},{"location":"api/lang_detect/#parameters","title":"Parameters","text":"Parameter Type Description <code>text</code> <code>str</code> The input text to analyze"},{"location":"api/lang_detect/#returns","title":"Returns","text":"Type Description <code>str</code> ISO 639-1 language code"},{"location":"api/lang_detect/#supported-languages","title":"Supported Languages","text":"<p>The function can detect 176 languages. Common codes:</p> Code Language <code>vi</code> Vietnamese <code>en</code> English <code>zh</code> Chinese <code>ja</code> Japanese <code>ko</code> Korean <code>fr</code> French <code>de</code> German <code>es</code> Spanish <code>ru</code> Russian <code>th</code> Thai"},{"location":"api/lang_detect/#examples","title":"Examples","text":""},{"location":"api/lang_detect/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import lang_detect\n\n# Vietnamese\nlang_detect(\"C\u1ef1u binh M\u1ef9 tr\u1ea3 nh\u1eadt k\u00fd nh\u1eb9 l\u00f2ng\")\n# 'vi'\n\n# English\nlang_detect(\"Hello, how are you today?\")\n# 'en'\n\n# Chinese\nlang_detect(\"\u4f60\u597d\uff0c\u4eca\u5929\u600e\u4e48\u6837\uff1f\")\n# 'zh'\n\n# Japanese\nlang_detect(\"\u3053\u3093\u306b\u3061\u306f\u3001\u5143\u6c17\u3067\u3059\u304b\uff1f\")\n# 'ja'\n</code></pre>"},{"location":"api/lang_detect/#detecting-multiple-texts","title":"Detecting Multiple Texts","text":"<pre><code>from underthesea import lang_detect\n\ntexts = [\n    \"Xin ch\u00e0o Vi\u1ec7t Nam\",\n    \"Hello World\",\n    \"Bonjour le monde\",\n    \"Hallo Welt\"\n]\n\nfor text in texts:\n    lang = lang_detect(text)\n    print(f\"{text} -&gt; {lang}\")\n# Xin ch\u00e0o Vi\u1ec7t Nam -&gt; vi\n# Hello World -&gt; en\n# Bonjour le monde -&gt; fr\n# Hallo Welt -&gt; de\n</code></pre>"},{"location":"api/lang_detect/#filtering-by-language","title":"Filtering by Language","text":"<pre><code>from underthesea import lang_detect\n\ndocuments = [\n    \"Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p\",\n    \"This is an English sentence\",\n    \"H\u00f4m nay tr\u1eddi \u0111\u1eb9p qu\u00e1\",\n    \"The weather is nice today\"\n]\n\n# Filter Vietnamese documents\nvietnamese_docs = [doc for doc in documents if lang_detect(doc) == 'vi']\nprint(vietnamese_docs)\n# ['Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p', 'H\u00f4m nay tr\u1eddi \u0111\u1eb9p qu\u00e1']\n</code></pre>"},{"location":"api/lang_detect/#language-statistics","title":"Language Statistics","text":"<pre><code>from collections import Counter\nfrom underthesea import lang_detect\n\ndocuments = [\n    \"Xin ch\u00e0o\",\n    \"Hello\",\n    \"T\u1ea1m bi\u1ec7t\",\n    \"Goodbye\",\n    \"C\u1ea3m \u01a1n\",\n    \"Merci\"\n]\n\nlangs = [lang_detect(doc) for doc in documents]\ndistribution = Counter(langs)\nprint(distribution)\n# Counter({'vi': 3, 'en': 2, 'fr': 1})\n</code></pre>"},{"location":"api/lang_detect/#notes","title":"Notes","text":"<ul> <li>Uses FastText's language identification model</li> <li>Works best with longer text (at least a few words)</li> <li>Very short text may be less accurate</li> <li>First call may take longer due to model loading</li> </ul>"},{"location":"api/ner/","title":"ner","text":"<p>Identify and classify named entities in text.</p>"},{"location":"api/ner/#usage","title":"Usage","text":"<pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\nprint(entities)\n# [('Ch\u01b0a', 'R', 'O', 'O'),\n#  ('ti\u1ebft l\u1ed9', 'V', 'B-VP', 'O'),\n#  ('l\u1ecbch tr\u00ecnh', 'V', 'B-VP', 'O'),\n#  ('t\u1edbi', 'E', 'B-PP', 'O'),\n#  ('Vi\u1ec7t Nam', 'Np', 'B-NP', 'B-LOC'),\n#  ('c\u1ee7a', 'E', 'B-PP', 'O'),\n#  ('T\u1ed5ng th\u1ed1ng', 'N', 'B-NP', 'O'),\n#  ('M\u1ef9', 'Np', 'B-NP', 'B-LOC'),\n#  ('Donald', 'Np', 'B-NP', 'B-PER'),\n#  ('Trump', 'Np', 'B-NP', 'I-PER')]\n</code></pre>"},{"location":"api/ner/#function-signature","title":"Function Signature","text":"<pre><code>def ner(\n    sentence: str,\n    format: str = None,\n    deep: bool = False\n) -&gt; list[tuple] | list[dict]\n</code></pre>"},{"location":"api/ner/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text <code>format</code> <code>str</code> <code>None</code> Output format <code>deep</code> <code>bool</code> <code>False</code> Use deep learning model (requires <code>[deep]</code> install)"},{"location":"api/ner/#returns","title":"Returns","text":""},{"location":"api/ner/#crf-model-default","title":"CRF Model (default)","text":"Type Description <code>list[tuple[str, str, str, str]]</code> List of (word, POS, chunk, entity) tuples"},{"location":"api/ner/#deep-learning-model-deeptrue","title":"Deep Learning Model (<code>deep=True</code>)","text":"Type Description <code>list[dict]</code> List of dictionaries with <code>entity</code> and <code>word</code> keys"},{"location":"api/ner/#entity-types","title":"Entity Types","text":"Tag Description <code>PER</code> Person <code>LOC</code> Location <code>ORG</code> Organization <code>O</code> Not an entity <p>Tags use BIO format:</p> <ul> <li><code>B-XXX</code>: Beginning of entity</li> <li><code>I-XXX</code>: Inside entity</li> <li><code>O</code>: Outside (not an entity)</li> </ul>"},{"location":"api/ner/#examples","title":"Examples","text":""},{"location":"api/ner/#basic-usage-crf-model","title":"Basic Usage (CRF Model)","text":"<pre><code>from underthesea import ner\n\ntext = \"Ch\u01b0a ti\u1ebft l\u1ed9 l\u1ecbch tr\u00ecnh t\u1edbi Vi\u1ec7t Nam c\u1ee7a T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump\"\nentities = ner(text)\n\n# Extract only named entities\nfor word, pos, chunk, entity in entities:\n    if entity != 'O':\n        print(f\"{word}: {entity}\")\n# Vi\u1ec7t Nam: B-LOC\n# M\u1ef9: B-LOC\n# Donald: B-PER\n# Trump: I-PER\n</code></pre>"},{"location":"api/ner/#deep-learning-model","title":"Deep Learning Model","text":"<p>Requires Installation</p> <pre><code>pip install \"underthesea[deep]\"\n</code></pre> <pre><code>from underthesea import ner\n\ntext = \"B\u1ed9 C\u00f4ng Th\u01b0\u01a1ng x\u00f3a m\u1ed9t t\u1ed5ng c\u1ee5c, gi\u1ea3m nhi\u1ec1u \u0111\u1ea7u m\u1ed1i\"\nentities = ner(text, deep=True)\nprint(entities)\n# [\n#   {'entity': 'B-ORG', 'word': 'B\u1ed9'},\n#   {'entity': 'I-ORG', 'word': 'C\u00f4ng'},\n#   {'entity': 'I-ORG', 'word': 'Th\u01b0\u01a1ng'}\n# ]\n</code></pre>"},{"location":"api/ner/#extracting-entities-by-type","title":"Extracting Entities by Type","text":"<pre><code>text = \"\u00d4ng Nguy\u1ec5n V\u0103n A t\u1eeb H\u00e0 N\u1ed9i \u0111\u1ebfn c\u00f4ng ty ABC\"\nentities = ner(text)\n\npersons = []\nlocations = []\norgs = []\n\nfor word, pos, chunk, entity in entities:\n    if entity.endswith('PER'):\n        persons.append(word)\n    elif entity.endswith('LOC'):\n        locations.append(word)\n    elif entity.endswith('ORG'):\n        orgs.append(word)\n\nprint(f\"Persons: {persons}\")\nprint(f\"Locations: {locations}\")\nprint(f\"Organizations: {orgs}\")\n</code></pre>"},{"location":"api/ner/#combining-multi-word-entities","title":"Combining Multi-word Entities","text":"<pre><code>text = \"T\u1ed5ng th\u1ed1ng M\u1ef9 Donald Trump th\u0103m Vi\u1ec7t Nam\"\nentities = ner(text)\n\n# Combine B-/I- tags into full entities\ncurrent_entity = []\ncurrent_type = None\nfull_entities = []\n\nfor word, pos, chunk, entity in entities:\n    if entity.startswith('B-'):\n        if current_entity:\n            full_entities.append((' '.join(current_entity), current_type))\n        current_entity = [word]\n        current_type = entity[2:]\n    elif entity.startswith('I-'):\n        current_entity.append(word)\n    else:\n        if current_entity:\n            full_entities.append((' '.join(current_entity), current_type))\n            current_entity = []\n            current_type = None\n\nif current_entity:\n    full_entities.append((' '.join(current_entity), current_type))\n\nprint(full_entities)\n# [('Donald Trump', 'PER'), ('Vi\u1ec7t Nam', 'LOC')]\n</code></pre>"},{"location":"api/ner/#notes","title":"Notes","text":"<ul> <li>The CRF model is fast and lightweight</li> <li>The deep learning model provides better accuracy but requires more resources</li> <li>First call with <code>deep=True</code> may take longer due to model loading</li> </ul>"},{"location":"api/pos_tag/","title":"pos_tag","text":"<p>Label words with their part-of-speech tags.</p>"},{"location":"api/pos_tag/#usage","title":"Usage","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nprint(tagged)\n# [('Ch\u1ee3', 'N'), ('th\u1ecbt', 'N'), ('ch\u00f3', 'N'), ('n\u1ed5i ti\u1ebfng', 'A'),\n#  ('\u1edf', 'E'), ('S\u00e0i G\u00f2n', 'Np'), ('b\u1ecb', 'V'), ('truy qu\u00e9t', 'V')]\n</code></pre>"},{"location":"api/pos_tag/#function-signature","title":"Function Signature","text":"<pre><code>def pos_tag(\n    sentence: str,\n    format: str = None,\n    model: str = None\n) -&gt; list[tuple[str, str]]\n</code></pre>"},{"location":"api/pos_tag/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to tag <code>format</code> <code>str</code> <code>None</code> Output format (currently only <code>None</code> supported) <code>model</code> <code>str</code> <code>None</code> Path to custom model"},{"location":"api/pos_tag/#returns","title":"Returns","text":"Type Description <code>list[tuple[str, str]]</code> List of (word, POS tag) tuples"},{"location":"api/pos_tag/#pos-tags","title":"POS Tags","text":"Tag Description Example <code>N</code> Noun ch\u1ee3, th\u1ecbt, ch\u00f3 <code>Np</code> Proper noun S\u00e0i G\u00f2n, Vi\u1ec7t Nam <code>V</code> Verb b\u1ecb, truy qu\u00e9t <code>A</code> Adjective n\u1ed5i ti\u1ebfng, \u0111\u1eb9p <code>P</code> Pronoun t\u00f4i, b\u1ea1n, n\u00f3 <code>R</code> Adverb r\u1ea5t, \u0111ang, s\u1ebd <code>E</code> Preposition \u1edf, trong, tr\u00ean <code>C</code> Conjunction v\u00e0, ho\u1eb7c, nh\u01b0ng <code>M</code> Number m\u1ed9t, hai, ba <code>L</code> Determiner c\u00e1c, nh\u1eefng, m\u1ecdi <code>X</code> Unknown - <code>CH</code> Punctuation . , ? !"},{"location":"api/pos_tag/#examples","title":"Examples","text":""},{"location":"api/pos_tag/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import pos_tag\n\ntext = \"Ch\u1ee3 th\u1ecbt ch\u00f3 n\u1ed5i ti\u1ebfng \u1edf S\u00e0i G\u00f2n b\u1ecb truy qu\u00e9t\"\ntagged = pos_tag(text)\nfor word, tag in tagged:\n    print(f\"{word}: {tag}\")\n# Ch\u1ee3: N\n# th\u1ecbt: N\n# ch\u00f3: N\n# n\u1ed5i ti\u1ebfng: A\n# \u1edf: E\n# S\u00e0i G\u00f2n: Np\n# b\u1ecb: V\n# truy qu\u00e9t: V\n</code></pre>"},{"location":"api/pos_tag/#filtering-by-pos-tag","title":"Filtering by POS Tag","text":"<pre><code>text = \"T\u00f4i y\u00eau Vi\u1ec7t Nam v\u00ec Vi\u1ec7t Nam r\u1ea5t \u0111\u1eb9p\"\ntagged = pos_tag(text)\n\n# Get all nouns\nnouns = [word for word, tag in tagged if tag in ('N', 'Np')]\nprint(nouns)\n# ['Vi\u1ec7t Nam', 'Vi\u1ec7t Nam']\n\n# Get all verbs\nverbs = [word for word, tag in tagged if tag == 'V']\nprint(verbs)\n# ['y\u00eau']\n</code></pre>"},{"location":"api/pos_tag/#processing-multiple-sentences","title":"Processing Multiple Sentences","text":"<pre><code>sentences = [\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\",\n    \"Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t\"\n]\n\nfor sentence in sentences:\n    tagged = pos_tag(sentence)\n    print(tagged)\n</code></pre>"},{"location":"api/pos_tag/#notes","title":"Notes","text":"<ul> <li>Word segmentation is performed automatically before POS tagging</li> <li>The model is trained on Vietnamese treebank data</li> <li>Proper nouns (Np) include names, locations, organizations</li> </ul>"},{"location":"api/sent_tokenize/","title":"sent_tokenize","text":"<p>Segment text into sentences.</p>"},{"location":"api/sent_tokenize/#usage","title":"Usage","text":"<pre><code>from underthesea import sent_tokenize\n\ntext = 'Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng. Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.'\n\nsentences = sent_tokenize(text)\nprint(sentences)\n# [\n#   \"Taylor cho bi\u1ebft l\u00fac \u0111\u1ea7u c\u00f4 c\u1ea3m th\u1ea5y ng\u1ea1i v\u1edbi c\u00f4 b\u1ea1n th\u00e2n Amanda nh\u01b0ng r\u1ed3i m\u1ecdi th\u1ee9 tr\u00f4i qua nhanh ch\u00f3ng.\",\n#   \"Amanda c\u0169ng tho\u1ea3i m\u00e1i v\u1edbi m\u1ed1i quan h\u1ec7 n\u00e0y.\"\n# ]\n</code></pre>"},{"location":"api/sent_tokenize/#function-signature","title":"Function Signature","text":"<pre><code>def sent_tokenize(text: str) -&gt; list[str]\n</code></pre>"},{"location":"api/sent_tokenize/#parameters","title":"Parameters","text":"Parameter Type Description <code>text</code> <code>str</code> The input text to segment into sentences"},{"location":"api/sent_tokenize/#returns","title":"Returns","text":"Type Description <code>list[str]</code> A list of sentences"},{"location":"api/sent_tokenize/#examples","title":"Examples","text":""},{"location":"api/sent_tokenize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sent_tokenize\n\ntext = \"Xin ch\u00e0o. T\u00f4i l\u00e0 sinh vi\u00ean. T\u00f4i h\u1ecdc \u1edf H\u00e0 N\u1ed9i.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['Xin ch\u00e0o.', 'T\u00f4i l\u00e0 sinh vi\u00ean.', 'T\u00f4i h\u1ecdc \u1edf H\u00e0 N\u1ed9i.']\n</code></pre>"},{"location":"api/sent_tokenize/#multiple-sentences","title":"Multiple Sentences","text":"<pre><code>text = \"\"\"Vi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia. Th\u1ee7 \u0111\u00f4 l\u00e0 H\u00e0 N\u1ed9i. Th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 TP. H\u1ed3 Ch\u00ed Minh.\"\"\"\nsentences = sent_tokenize(text)\nprint(len(sentences))  # 3\n</code></pre>"},{"location":"api/sent_tokenize/#handling-abbreviations","title":"Handling Abbreviations","text":"<p>The function handles common Vietnamese abbreviations:</p> <pre><code>text = \"TP. H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam. D\u00e2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n# ['TP. H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t Vi\u1ec7t Nam.', 'D\u00e2n s\u1ed1 kho\u1ea3ng 9 tri\u1ec7u ng\u01b0\u1eddi.']\n</code></pre>"},{"location":"api/sent_tokenize/#notes","title":"Notes","text":"<ul> <li>The function uses rule-based sentence boundary detection</li> <li>It handles common Vietnamese punctuation patterns</li> <li>Abbreviations like \"TP.\" (th\u00e0nh ph\u1ed1) are handled correctly</li> </ul>"},{"location":"api/sentiment/","title":"sentiment","text":"<p>Analyze the sentiment of text.</p>"},{"location":"api/sentiment/#usage","title":"Usage","text":"<pre><code>from underthesea import sentiment\n\ntext = \"S\u1ea3n ph\u1ea9m h\u01a1i nh\u1ecf so v\u1edbi t\u01b0\u1edfng t\u01b0\u1ee3ng nh\u01b0ng ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t\"\nresult = sentiment(text)\nprint(result)\n# 'positive'\n</code></pre>"},{"location":"api/sentiment/#function-signature","title":"Function Signature","text":"<pre><code>def sentiment(\n    X: str,\n    domain: str = 'general'\n) -&gt; str | list[str]\n</code></pre>"},{"location":"api/sentiment/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> <code>str</code> The input text to analyze <code>domain</code> <code>str</code> <code>'general'</code> Domain for analysis (<code>'general'</code> or <code>'bank'</code>)"},{"location":"api/sentiment/#returns","title":"Returns","text":""},{"location":"api/sentiment/#general-domain","title":"General Domain","text":"Type Description <code>str</code> Sentiment label: <code>'positive'</code>, <code>'negative'</code>, or <code>'neutral'</code>"},{"location":"api/sentiment/#bank-domain","title":"Bank Domain","text":"Type Description <code>list[str]</code> List of aspect-sentiment pairs (e.g., <code>['ASPECT#sentiment']</code>)"},{"location":"api/sentiment/#examples","title":"Examples","text":""},{"location":"api/sentiment/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sentiment\n\n# Positive sentiment\nsentiment(\"S\u1ea3n ph\u1ea9m ch\u1ea5t l\u01b0\u1ee3ng t\u1ed1t, \u0111\u00f3ng g\u00f3i c\u1ea9n th\u1eadn\")\n# 'positive'\n\n# Negative sentiment\nsentiment(\"h\u00e0ng k\u00e9m ch\u1ea5t lg, ch\u0103n \u0111\u1eafp l\u00ean d\u00ednh l\u00f4ng l\u00e1 kh\u1eafp ng\u01b0\u1eddi. th\u1ea5t v\u1ecdng\")\n# 'negative'\n</code></pre>"},{"location":"api/sentiment/#bank-domain_1","title":"Bank Domain","text":"<p>The bank domain provides aspect-based sentiment analysis:</p> <pre><code>from underthesea import sentiment\n\n# Customer support aspect\nsentiment(\"\u0110ky qua \u0111\u01b0\u1eddng link \u1edf b\u00e0i vi\u1ebft n\u00e0y t\u1eeb th\u1ee9 6 m\u00e0 gi\u1edd ch\u01b0a th\u1ea5y ai lhe h\u1ebft\", domain='bank')\n# ['CUSTOMER_SUPPORT#negative']\n\n# Trademark aspect\nsentiment(\"Xem l\u1ea1i v\u1eabn th\u1ea5y x\u00fac \u0111\u1ed9ng v\u00e0 t\u1ef1 h\u00e0o v\u1ec1 BIDV c\u1ee7a m\u00ecnh\", domain='bank')\n# ['TRADEMARK#positive']\n</code></pre>"},{"location":"api/sentiment/#bank-domain-aspects","title":"Bank Domain Aspects","text":"Aspect Description <code>INTEREST_RATE</code> Interest rate related <code>CUSTOMER_SUPPORT</code> Customer service quality <code>PRODUCT</code> Product/service quality <code>TRADEMARK</code> Brand perception"},{"location":"api/sentiment/#processing-reviews","title":"Processing Reviews","text":"<pre><code>from underthesea import sentiment\n\nreviews = [\n    \"D\u1ecbch v\u1ee5 tuy\u1ec7t v\u1eddi, nh\u00e2n vi\u00ean nhi\u1ec7t t\u00ecnh\",\n    \"Giao h\u00e0ng ch\u1eadm, \u0111\u00f3ng g\u00f3i kh\u00f4ng c\u1ea9n th\u1eadn\",\n    \"S\u1ea3n ph\u1ea9m b\u00ecnh th\u01b0\u1eddng, kh\u00f4ng c\u00f3 g\u00ec \u0111\u1eb7c bi\u1ec7t\"\n]\n\nfor review in reviews:\n    result = sentiment(review)\n    print(f\"{review[:30]}... -&gt; {result}\")\n# D\u1ecbch v\u1ee5 tuy\u1ec7t v\u1eddi, nh\u00e2n vi\u00ean n... -&gt; positive\n# Giao h\u00e0ng ch\u1eadm, \u0111\u00f3ng g\u00f3i kh\u00f4ng... -&gt; negative\n# S\u1ea3n ph\u1ea9m b\u00ecnh th\u01b0\u1eddng, kh\u00f4ng c\u00f3... -&gt; neutral\n</code></pre>"},{"location":"api/sentiment/#counting-sentiment-distribution","title":"Counting Sentiment Distribution","text":"<pre><code>from collections import Counter\nfrom underthesea import sentiment\n\nreviews = [\n    \"S\u1ea3n ph\u1ea9m t\u1ed1t\",\n    \"Kh\u00f4ng h\u00e0i l\u00f2ng\",\n    \"R\u1ea5t th\u00edch\",\n    \"T\u1ec7 qu\u00e1\",\n    \"B\u00ecnh th\u01b0\u1eddng\"\n]\n\nsentiments = [sentiment(r) for r in reviews]\ndistribution = Counter(sentiments)\nprint(distribution)\n# Counter({'positive': 2, 'negative': 2, 'neutral': 1})\n</code></pre>"},{"location":"api/sentiment/#accessing-available-labels","title":"Accessing Available Labels","text":"<p>You can access all available sentiment labels using the <code>labels</code> property:</p> <pre><code>from underthesea import sentiment\n\n# Get labels for general domain\nsentiment.labels\n# ['positive', 'negative']\n\n# Get labels for bank domain\nsentiment.bank.labels\n# ['ACCOUNT#negative', 'CARD#negative', 'CARD#neutral', 'CARD#positive',\n#  'CUSTOMER_SUPPORT#negative', 'CUSTOMER_SUPPORT#neutral', 'CUSTOMER_SUPPORT#positive',\n#  'DISCOUNT#negative', 'DISCOUNT#neutral', 'DISCOUNT#positive', ...]\n</code></pre>"},{"location":"api/sentiment/#notes","title":"Notes","text":"<ul> <li>The general domain model classifies into positive/negative</li> <li>The bank domain model provides aspect-based sentiment</li> <li>First call may take longer due to model loading</li> <li>Use <code>sentiment.labels</code> to get all available labels for the general domain</li> <li>Use <code>sentiment.bank.labels</code> to get all available labels for the bank domain</li> </ul>"},{"location":"api/text_normalize/","title":"text_normalize","text":"<p>Normalize Vietnamese text by fixing common encoding and diacritic issues.</p>"},{"location":"api/text_normalize/#usage","title":"Usage","text":"<pre><code>from underthesea import text_normalize\n\ntext = \"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng ph\u00f2ng th\u00ed ngh\u1ecb\u00eam ho\u00e1 h\u1ecdc\"\nnormalized = text_normalize(text)\nprint(normalized)\n# \"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng ph\u00f2ng th\u00ed nghi\u1ec7m h\u00f3a h\u1ecdc\"\n</code></pre>"},{"location":"api/text_normalize/#function-signature","title":"Function Signature","text":"<pre><code>def text_normalize(text: str, tokenizer: str = 'underthesea') -&gt; str\n</code></pre>"},{"location":"api/text_normalize/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The input text to normalize <code>tokenizer</code> <code>str</code> <code>'underthesea'</code> The tokenizer to use"},{"location":"api/text_normalize/#returns","title":"Returns","text":"Type Description <code>str</code> The normalized text"},{"location":"api/text_normalize/#examples","title":"Examples","text":""},{"location":"api/text_normalize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import text_normalize\n\n# Fix diacritic issues\ntext_normalize(\"\u00d0\u1ea3m ba\u1ecf ch\u1ea5t l\u1ef1\u01a1ng\")\n# \"\u0110\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng\"\n</code></pre>"},{"location":"api/text_normalize/#common-fixes","title":"Common Fixes","text":"<pre><code># Fix \u0110/\u00d0 confusion\ntext_normalize(\"\u00d0\u1ea1i h\u1ecdc\")\n# \"\u0110\u1ea1i h\u1ecdc\"\n\n# Fix vowel diacritics\ntext_normalize(\"ho\u00e1 h\u1ecdc\")\n# \"h\u00f3a h\u1ecdc\"\n\n# Fix tone marks\ntext_normalize(\"ngh\u1ecb\u00eam\")\n# \"nghi\u1ec7m\"\n</code></pre>"},{"location":"api/text_normalize/#full-text-normalization","title":"Full Text Normalization","text":"<pre><code>text = \"\u00d0\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5 v\u1ec1 vi\u1ec7c chu\u1ea9n ho\u00e1 v\u0103n b\u1ea3n ti\u1ebfng Vi\u1ec7t\"\nnormalized = text_normalize(text)\nprint(normalized)\n# \"\u0110\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5 v\u1ec1 vi\u1ec7c chu\u1ea9n h\u00f3a v\u0103n b\u1ea3n ti\u1ebfng Vi\u1ec7t\"\n</code></pre>"},{"location":"api/text_normalize/#common-issues-fixed","title":"Common Issues Fixed","text":"Issue Example Fixed \u0110/\u00d0 confusion <code>\u00d0\u1ea1i</code> <code>\u0110\u1ea1i</code> Old-style diacritics <code>ho\u00e1</code> <code>h\u00f3a</code> Incorrect vowel composition <code>l\u1ef1\u01a1ng</code> <code>l\u01b0\u1ee3ng</code> Unicode normalization Various NFC form"},{"location":"api/text_normalize/#notes","title":"Notes","text":"<ul> <li>This function is useful for preprocessing Vietnamese text</li> <li>It handles common encoding issues from legacy systems</li> <li>The output is in Unicode NFC normalized form</li> </ul>"},{"location":"api/translate/","title":"translate","text":"<p>Translate text between Vietnamese and English.</p> <p>Requires Deep Learning</p> <p>This function requires the deep learning dependencies: <pre><code>pip install \"underthesea[deep]\"\n</code></pre></p>"},{"location":"api/translate/#usage","title":"Usage","text":"<pre><code>from underthesea import translate\n\n# Vietnamese to English (default)\ntext = \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\nenglish = translate(text)\nprint(english)\n# 'Hanoi is the capital of Vietnam'\n</code></pre>"},{"location":"api/translate/#function-signature","title":"Function Signature","text":"<pre><code>def translate(\n    text: str,\n    source_lang: str = 'vi',\n    target_lang: str = 'en'\n) -&gt; str\n</code></pre>"},{"location":"api/translate/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The input text to translate <code>source_lang</code> <code>str</code> <code>'vi'</code> Source language code <code>target_lang</code> <code>str</code> <code>'en'</code> Target language code"},{"location":"api/translate/#returns","title":"Returns","text":"Type Description <code>str</code> The translated text"},{"location":"api/translate/#supported-languages","title":"Supported Languages","text":"Code Language <code>vi</code> Vietnamese <code>en</code> English"},{"location":"api/translate/#examples","title":"Examples","text":""},{"location":"api/translate/#vietnamese-to-english","title":"Vietnamese to English","text":"<pre><code>from underthesea import translate\n\n# Basic translation\ntranslate(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\")\n# 'Hanoi is the capital of Vietnam'\n\ntranslate(\"\u1ea8m th\u1ef1c Vi\u1ec7t Nam n\u1ed5i ti\u1ebfng tr\u00ean th\u1ebf gi\u1edbi\")\n# 'Vietnamese cuisine is famous around the world'\n\ntranslate(\"Xin ch\u00e0o, t\u00f4i l\u00e0 sinh vi\u00ean\")\n# 'Hello, I am a student'\n</code></pre>"},{"location":"api/translate/#english-to-vietnamese","title":"English to Vietnamese","text":"<pre><code>from underthesea import translate\n\ntranslate(\"I love Vietnamese food\", source_lang='en', target_lang='vi')\n# 'T\u00f4i y\u00eau \u1ea9m th\u1ef1c Vi\u1ec7t Nam'\n\ntranslate(\"Vietnam is a beautiful country\", source_lang='en', target_lang='vi')\n# 'Vi\u1ec7t Nam l\u00e0 m\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc xinh \u0111\u1eb9p'\n</code></pre>"},{"location":"api/translate/#translating-multiple-sentences","title":"Translating Multiple Sentences","text":"<pre><code>from underthesea import translate\n\nsentences = [\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\",\n    \"Vi\u1ec7t Nam c\u00f3 nhi\u1ec1u \u0111\u1ecba \u0111i\u1ec3m du l\u1ecbch \u0111\u1eb9p\",\n    \"\u1ea8m th\u1ef1c Vi\u1ec7t Nam r\u1ea5t phong ph\u00fa\"\n]\n\nfor sentence in sentences:\n    english = translate(sentence)\n    print(f\"{sentence}\")\n    print(f\"-&gt; {english}\\n\")\n</code></pre>"},{"location":"api/translate/#handling-long-text","title":"Handling Long Text","text":"<p>For long documents, consider splitting into sentences first:</p> <pre><code>from underthesea import sent_tokenize, translate\n\ntext = \"\"\"Vi\u1ec7t Nam l\u00e0 m\u1ed9t qu\u1ed1c gia. Th\u1ee7 \u0111\u00f4 l\u00e0 H\u00e0 N\u1ed9i.\nTh\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t l\u00e0 TP. H\u1ed3 Ch\u00ed Minh.\"\"\"\n\nsentences = sent_tokenize(text)\ntranslations = [translate(s) for s in sentences]\n\nfor vi, en in zip(sentences, translations):\n    print(f\"VI: {vi}\")\n    print(f\"EN: {en}\\n\")\n</code></pre>"},{"location":"api/translate/#notes","title":"Notes","text":"<ul> <li>Uses a transformer-based neural machine translation model</li> <li>First call may take longer due to model loading</li> <li>Works best with well-formed sentences</li> <li>Long texts should be split into sentences for better results</li> </ul>"},{"location":"api/tts/","title":"tts","text":"<p>Convert Vietnamese text to speech.</p> <p>Requires Additional Setup</p> <p>This function requires extra dependencies and model download: <pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre></p>"},{"location":"api/tts/#usage","title":"Usage","text":"<pre><code>from underthesea.pipeline.tts import tts\n\ntext = \"Xin ch\u00e0o Vi\u1ec7t Nam\"\ntts(text)\n# Generates sound.wav in current directory\n</code></pre>"},{"location":"api/tts/#function-signature","title":"Function Signature","text":"<pre><code>def tts(text: str, outfile: str = \"sound.wav\", play: bool = False) -&gt; tuple[int, np.ndarray]\n</code></pre>"},{"location":"api/tts/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text</code> <code>str</code> The Vietnamese text to convert to speech <code>outfile</code> <code>str</code> <code>\"sound.wav\"</code> Output audio file path <code>play</code> <code>bool</code> <code>False</code> Whether to play audio after generation"},{"location":"api/tts/#returns","title":"Returns","text":"Type Description <code>tuple[int, np.ndarray]</code> Sample rate (16000) and audio waveform array"},{"location":"api/tts/#examples","title":"Examples","text":""},{"location":"api/tts/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea.pipeline.tts import tts\n\n# Generate speech\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n# Creates sound.wav\n\n# Custom output file\ntts(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\", outfile=\"hanoi.wav\")\n\n# Generate and play immediately\ntts(\"Xin ch\u00e0o\", play=True)\n</code></pre>"},{"location":"api/tts/#command-line-usage","title":"Command Line Usage","text":"<pre><code>underthesea tts \"Xin ch\u00e0o Vi\u1ec7t Nam\"\n# Creates sound.wav and plays it\n</code></pre>"},{"location":"api/tts/#generating-multiple-audio-files","title":"Generating Multiple Audio Files","text":"<pre><code>from underthesea.pipeline.tts import tts\n\nsentences = [\n    (\"Xin ch\u00e0o\", \"hello.wav\"),\n    (\"T\u1ea1m bi\u1ec7t\", \"goodbye.wav\"),\n    (\"C\u1ea3m \u01a1n\", \"thanks.wav\")\n]\n\nfor text, filename in sentences:\n    tts(text, outfile=filename)\n    print(f\"Generated: {filename}\")\n</code></pre>"},{"location":"api/tts/#playing-audio-with-external-library","title":"Playing Audio (with external library)","text":"<pre><code>from underthesea.pipeline.tts import tts\nimport subprocess\n\n# Generate audio\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n\n# Play audio (macOS)\nsubprocess.run([\"afplay\", \"sound.wav\"])\n\n# Play audio (Linux with aplay)\n# subprocess.run([\"aplay\", \"sound.wav\"])\n</code></pre>"},{"location":"api/tts/#notes","title":"Notes","text":"<ul> <li>Uses the VietTTS model for high-quality Vietnamese speech synthesis</li> <li>Output format is WAV audio at 16kHz sample rate</li> <li>First call may take longer due to model loading</li> <li>Requires downloading the TTS model before first use</li> <li>Credits: Based on NTT123/vietTTS</li> </ul>"},{"location":"api/tts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/tts/#model-not-found","title":"Model Not Found","text":"<p>If you get a model not found error:</p> <pre><code>underthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"api/tts/#audio-quality-issues","title":"Audio Quality Issues","text":"<ul> <li>Ensure input text is in Vietnamese</li> <li>Longer sentences produce smoother audio</li> <li>Punctuation affects prosody</li> </ul>"},{"location":"api/word_tokenize/","title":"word_tokenize","text":"<p>Segment Vietnamese text into words.</p>"},{"location":"api/word_tokenize/#usage","title":"Usage","text":"<pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# [\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n</code></pre>"},{"location":"api/word_tokenize/#function-signature","title":"Function Signature","text":"<pre><code>def word_tokenize(\n    sentence: str,\n    format: str = None,\n    use_token_normalize: bool = True,\n    fixed_words: list = None\n) -&gt; list[str] | str\n</code></pre>"},{"location":"api/word_tokenize/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>sentence</code> <code>str</code> The input text to tokenize <code>format</code> <code>str</code> <code>None</code> Output format: <code>None</code> for list, <code>\"text\"</code> for string <code>use_token_normalize</code> <code>bool</code> <code>True</code> Whether to normalize tokens <code>fixed_words</code> <code>list</code> <code>None</code> List of words that should not be split"},{"location":"api/word_tokenize/#returns","title":"Returns","text":"Type Description <code>list[str]</code> List of words (when <code>format=None</code>) <code>str</code> Space-separated string with underscores (when <code>format=\"text\"</code>)"},{"location":"api/word_tokenize/#examples","title":"Examples","text":""},{"location":"api/word_tokenize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import word_tokenize\n\ntext = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nwords = word_tokenize(text)\nprint(words)\n# [\"Ch\u00e0ng trai\", \"9X\", \"Qu\u1ea3ng Tr\u1ecb\", \"kh\u1edfi nghi\u1ec7p\", \"t\u1eeb\", \"n\u1ea5m\", \"s\u00f2\"]\n</code></pre>"},{"location":"api/word_tokenize/#text-format","title":"Text Format","text":"<pre><code>text = \"Ch\u00e0ng trai 9X Qu\u1ea3ng Tr\u1ecb kh\u1edfi nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\nresult = word_tokenize(text, format=\"text\")\nprint(result)\n# \"Ch\u00e0ng_trai 9X Qu\u1ea3ng_Tr\u1ecb kh\u1edfi_nghi\u1ec7p t\u1eeb n\u1ea5m s\u00f2\"\n</code></pre>"},{"location":"api/word_tokenize/#fixed-words","title":"Fixed Words","text":"<p>Use <code>fixed_words</code> to ensure certain words are kept together:</p> <pre><code>text = \"Vi\u1ec7n Nghi\u00ean C\u1ee9u chi\u1ebfn l\u01b0\u1ee3c qu\u1ed1c gia v\u1ec1 h\u1ecdc m\u00e1y\"\nfixed_words = [\"Vi\u1ec7n Nghi\u00ean C\u1ee9u\", \"h\u1ecdc m\u00e1y\"]\nresult = word_tokenize(text, fixed_words=fixed_words, format=\"text\")\nprint(result)\n# \"Vi\u1ec7n_Nghi\u00ean_C\u1ee9u chi\u1ebfn_l\u01b0\u1ee3c qu\u1ed1c_gia v\u1ec1 h\u1ecdc_m\u00e1y\"\n</code></pre>"},{"location":"api/word_tokenize/#processing-multiple-sentences","title":"Processing Multiple Sentences","text":"<pre><code>sentences = [\n    \"T\u00f4i y\u00eau Vi\u1ec7t Nam\",\n    \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam\"\n]\n\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    print(words)\n# ['T\u00f4i', 'y\u00eau', 'Vi\u1ec7t Nam']\n# ['H\u00e0 N\u1ed9i', 'l\u00e0', 'th\u1ee7 \u0111\u00f4', 'c\u1ee7a', 'Vi\u1ec7t Nam']\n</code></pre>"},{"location":"api/word_tokenize/#notes","title":"Notes","text":"<ul> <li>Vietnamese word segmentation is challenging because spaces don't always indicate word boundaries</li> <li>The function uses a CRF model trained on Vietnamese text</li> <li>Multi-syllable words are joined (e.g., \"Vi\u1ec7t Nam\" is one word, not two)</li> <li>Use <code>fixed_words</code> parameter for domain-specific terminology</li> </ul>"},{"location":"contribute/SPONSORS/","title":"Sponsors","text":"<p>\ud83d\udc4b If you are a fan of the project or a company that relies on Underthesea, you might want to consider sponsoring \ud83d\udcb0. This will help us devote more time to answering questions \ud83e\udd14 and doing feature development \ud83d\ude80. </p> <p>We thank those who support \ud83d\udc9d Underthesea! </p>"},{"location":"contribute/SPONSORS/#corporate","title":"Corporate \ud83c\udfe2","text":"<p>\ud83c\udf0a Underthesea has been dedicatedly working on Open Source Vietnamese Natural Lanugage Processing for the past six years. Our team of developers and researchers have been exploring various AI techniques to create tools that can simplify and automate complex tasks, thereby empowering businesses to thrive in the AI age.</p> <p>\ud83d\udcaa As we continue to build and refine our projects, we need support from corporate sponsors to help us achieve our goals. Sponsorship is an essential part of our growth strategy, and it will allow us to accelerate our research and development efforts, expand our team, and bring our technology to market.</p> <p>Join us in Advancing Vietnamese AI Technology:   \ud83e\udd1d Become a Corporate Sponsor Today </p>"},{"location":"contribute/SPONSORS/#individuals","title":"Individuals \ud83e\uddb8\u200d\u2642\ufe0f\ud83e\uddb8\u200d\u2640\ufe0f","text":"<ul> <li>Nguyen Xuan Duc </li> <li>Nguyen Huu Thanh</li> <li>Nguyen Thanh Duc</li> <li>Pham Hong Quang</li> <li>Thang Pham Ngoc</li> <li>Hoai-Thu Vuong</li> </ul>"},{"location":"contribute/SUPPORT_US/","title":"\ud83d\udc9d Support Us","text":"<p>\ud83c\udf0a Underthesea has been dedicatedly working on Open Source Vietnamese Natural Lanugage Processing for the past six years. Our team of developers and researchers have been exploring various AI techniques to create tools that can simplify and automate complex tasks, thereby empowering businesses to thrive in the AI age.</p> <p>If you found this project helpful and would like to support our work, you can just buy us a coffee \u2615.</p> <p>Your support is our biggest encouragement \ud83c\udf81!</p> <p></p>"},{"location":"datasets/UTS_VLC/","title":"UTS_VLC","text":"<p>Vietnamese Legal Corpus (2026 Edition)</p> <p>Vietnam's system of legal documents - Updated with laws passed through 2022-2025 (15th National Assembly).</p>"},{"location":"datasets/UTS_VLC/#huggingface-dataset","title":"HuggingFace Dataset","text":"<p>Dataset: undertheseanlp/UTS_VLC</p>"},{"location":"datasets/UTS_VLC/#splits","title":"Splits","text":"Split Documents Description 2021 110 Original corpus with full text 2023 208 Expanded corpus with processed text 2026 318 Latest corpus with 2022-2025 laws"},{"location":"datasets/UTS_VLC/#features","title":"Features","text":"Feature Type Description id string Document identifier (slug) filename string Source filename title string Vietnamese title type string Document type: \"code\", \"law\", or \"constitution\" content string Full text content content_length int32 Character count"},{"location":"datasets/UTS_VLC/#usage","title":"Usage","text":""},{"location":"datasets/UTS_VLC/#load-from-huggingface","title":"Load from HuggingFace","text":"<pre><code>from datasets import load_dataset\n\n# Load all splits\nds = load_dataset(\"undertheseanlp/UTS_VLC\")\n\n# Load specific split\nds_2026 = load_dataset(\"undertheseanlp/UTS_VLC\", split=\"2026\")\n\n# Filter by type\ncodes = ds_2026.filter(lambda x: x[\"type\"] == \"code\")\nlaws = ds_2026.filter(lambda x: x[\"type\"] == \"law\")\n\n# Search by title\nland_law = ds_2026.filter(lambda x: \"\u0110\u1ea5t \u0111ai\" in x[\"title\"])\n</code></pre>"},{"location":"datasets/UTS_VLC/#load-from-underthesea","title":"Load from Underthesea","text":"<pre><code>from underthesea import download_data\n\n# Download the corpus\ndownload_data(\"CP_Vietnamese_VLC_2026\")\n</code></pre>"},{"location":"datasets/UTS_VLC/#statistics","title":"Statistics","text":"Metric 2021 2023 2026 Documents 110 208 318 Total Characters ~14M ~21M ~24M Codes 6 6 6 Laws 104 202 312"},{"location":"datasets/UTS_VLC/#document-types","title":"Document Types","text":"<ul> <li>Constitution (Hi\u1ebfn ph\u00e1p): 1 document (2013 Constitution)</li> <li>Codes (B\u1ed9 lu\u1eadt): 6 major codes</li> <li>Civil Code 2015 (B\u1ed9 lu\u1eadt D\u00e2n s\u1ef1)</li> <li>Civil Procedure Code 2015 (B\u1ed9 lu\u1eadt T\u1ed1 t\u1ee5ng D\u00e2n s\u1ef1)</li> <li>Criminal Code 2015 (B\u1ed9 lu\u1eadt H\u00ecnh s\u1ef1)</li> <li>Criminal Procedure Code 2015 (B\u1ed9 lu\u1eadt T\u1ed1 t\u1ee5ng H\u00ecnh s\u1ef1)</li> <li>Maritime Code 2015 (B\u1ed9 lu\u1eadt H\u00e0ng h\u1ea3i)</li> <li>Labor Code 2019 (B\u1ed9 lu\u1eadt Lao \u0111\u1ed9ng)</li> <li>Laws (Lu\u1eadt): 300+ laws covering various domains</li> </ul>"},{"location":"datasets/UTS_VLC/#recent-laws-2022-2025","title":"Recent Laws (2022-2025)","text":"<p>The 2026 split includes laws passed by the 15th National Assembly:</p> <ul> <li>2022: 8 laws (Inspection, Anti-Money Laundering, IP, Insurance, Cinema, etc.)</li> <li>2023: 12 laws (E-Transactions, Consumer Protection, Bidding, Telecoms, Housing, etc.)</li> <li>2024: 24 laws (Land, Credit Institutions, Roads, Data, Electricity, Social Insurance, etc.)</li> <li>2025: 50+ laws including landmark legislation:</li> <li>AI Law (Lu\u1eadt Tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o) - First AI law in Vietnam</li> <li>E-commerce Law (Lu\u1eadt Th\u01b0\u01a1ng m\u1ea1i \u0111i\u1ec7n t\u1eed)</li> <li>Digital Transformation Law (Lu\u1eadt Chuy\u1ec3n \u0111\u1ed5i s\u1ed1)</li> <li>Personal Data Protection Law (Lu\u1eadt B\u1ea3o v\u1ec7 d\u1eef li\u1ec7u c\u00e1 nh\u00e2n)</li> </ul>"},{"location":"datasets/UTS_VLC/#references","title":"References","text":"<ul> <li>HuggingFace Dataset</li> <li>Vietnam's System of Legal Documents</li> <li>Thu Vien Phap Luat - Official Vietnamese legal document database</li> <li>National Assembly of Vietnam</li> </ul>"},{"location":"datasets/UUD-v0.1/","title":"UUD-v0.1","text":"<p>Universal Dependency Dataset for Vietnamese</p> <p>Vietnamese Universal Dependency dataset following Universal Dependencies annotation guidelines. Machine-generated using Underthesea NLP toolkit.</p>"},{"location":"datasets/UUD-v0.1/#huggingface-dataset","title":"HuggingFace Dataset","text":"<p>Dataset: undertheseanlp/UDD-v0.1</p>"},{"location":"datasets/UUD-v0.1/#summary","title":"Summary","text":"Metric Value Sentences 3,000 Tokens 64,814 Avg sentence length 21.60 Max sentence length 65 Avg tree depth 6.77 Max tree depth 21 Source Vietnamese Legal Corpus (UTS_VLC) Validation 0 errors (passes all UD checks)"},{"location":"datasets/UUD-v0.1/#features","title":"Features","text":"Field Type Description <code>sent_id</code> string Sentence identifier <code>text</code> string Original sentence text <code>tokens</code> list[string] Tokenized words <code>lemmas</code> list[string] Lemmatized forms <code>upos</code> list[string] Universal POS tags <code>xpos</code> list[string] Language-specific POS tags <code>feats</code> list[string] Morphological features <code>head</code> list[string] Head token indices <code>deprel</code> list[string] Dependency relations <code>deps</code> list[string] Enhanced dependencies <code>misc</code> list[string] Miscellaneous annotations"},{"location":"datasets/UUD-v0.1/#usage","title":"Usage","text":""},{"location":"datasets/UUD-v0.1/#load-from-huggingface","title":"Load from HuggingFace","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"undertheseanlp/UDD-v0.1\")\nprint(dataset[\"train\"][0])\n</code></pre>"},{"location":"datasets/UUD-v0.1/#clone-and-run-scripts","title":"Clone and Run Scripts","text":"<pre><code># Clone the dataset repository\ngit clone https://huggingface.co/datasets/undertheseanlp/UDD-v0.1\ncd UDD-v0.1\n\n# Install dependencies with uv\nuv sync\n\n# Fetch sentences from UTS_VLC\nuv run python scripts/fetch_data.py\n\n# Convert to UD format\nuv run python scripts/convert_to_ud.py\n\n# Run statistics\nuv run python scripts/statistics.py\n\n# Upload to HuggingFace\nuv run python scripts/upload_to_hf.py\n</code></pre>"},{"location":"datasets/UUD-v0.1/#upos-distribution","title":"UPOS Distribution","text":"Tag Count Percent NOUN 21,599 33.32% VERB 15,793 24.37% PUNCT 6,391 9.86% ADP 6,309 9.73% CCONJ 2,942 4.54% AUX 2,665 4.11% ADV 2,518 3.88% ADJ 2,254 3.48% NUM 1,444 2.23% DET 1,350 2.08% PRON 1,128 1.74% PROPN 318 0.49%"},{"location":"datasets/UUD-v0.1/#top-dependency-relations","title":"Top Dependency Relations","text":"Relation Count Percent obj 6,448 9.95% punct 6,391 9.86% nmod 5,870 9.06% case 5,853 9.03% conj 4,920 7.59% compound 3,314 5.11% root 3,000 4.63% acl:subj 2,889 4.46% nsubj 2,869 4.43% nmod:poss 1,656 2.56%"},{"location":"datasets/UUD-v0.1/#root-upos-distribution","title":"Root UPOS Distribution","text":"UPOS Count Percent VERB 2,220 74.00% NOUN 639 21.30% ADJ 63 2.10% ADP 41 1.37% AUX 17 0.57% PROPN 14 0.47%"},{"location":"datasets/UUD-v0.1/#scripts","title":"Scripts","text":"<p>The dataset repository includes scripts for data processing:</p> Script Description <code>scripts/fetch_data.py</code> Fetch sentences from UTS_VLC corpus <code>scripts/convert_to_ud.py</code> Convert to UD format with syntax fixes <code>scripts/statistics.py</code> Compute dataset statistics <code>scripts/upload_to_hf.py</code> Upload to HuggingFace Hub"},{"location":"datasets/UUD-v0.1/#related-datasets","title":"Related Datasets","text":"<p>Other Vietnamese dependency treebanks include: UD_Vietnamese-VTB - the official Vietnamese treebank in Universal Dependencies, converted from VietTreebank constituent treebank created by VLSP project (UD v1.4+); VnDT - the first Vietnamese dependency treebank with 10,200 sentences automatically converted from VietTreebank and manually edited (2013, revised 2016); BKTreebank - a dependency treebank with 6,900 sentences featuring custom POS tagset and dependency relations designed specifically for Vietnamese linguistic characteristics (LREC 2018); and VLSP shared task data - training and test data from VLSP dependency parsing shared tasks with 8,152 sentences following Universal Dependencies v2 annotation scheme (2019-2020), where top models achieved 76.27% LAS and 84.65% UAS using PhoBERT+ELMO/Biaffine architecture.</p> Dataset Sentences Tokens Domain Annotation Format Available UUD-v0.1 3,000 64,814 Legal Machine-generated CoNLL-U HuggingFace UD_Vietnamese-VTB 3,323 58,069 News (Tuoi Tre) Manual CoNLL-U UD, GitHub, HuggingFace VnDT 10,200 ~170K News (Tuoi Tre) Semi-automatic CoNLL GitHub BKTreebank 6,900 ~115K Mixed Manual CoNLL ACL VLSP 2020 8,152 ~140K Mixed Manual CoNLL-U VLSP"},{"location":"datasets/UUD-v0.1/#references","title":"References","text":"<ul> <li>HuggingFace Dataset</li> <li>Universal Dependencies</li> <li>UD Vietnamese Guidelines</li> <li>UTS_VLC Dataset - Source corpus</li> <li>VLSP - Vietnamese Language and Speech Processing</li> </ul>"},{"location":"datasets/UVB/","title":"UVB","text":"<p>Underthesea Vietnamese Books Dataset (2026 Edition)</p> <p>A collection of 447 Vietnamese books with full text content and Goodreads metadata for NLP research.</p>"},{"location":"datasets/UVB/#huggingface-dataset","title":"HuggingFace Dataset","text":"<p>Dataset: undertheseanlp/UVB-v0.1</p>"},{"location":"datasets/UVB/#features","title":"Features","text":"Feature Type Description id string Unique identifier (e.g., vn_000001) title string Book title author string Author name content string Full text content of the book genres list[string] Book genres from Goodreads first_publish string First publication year goodreads_id string Goodreads book ID goodreads_url string Goodreads URL goodreads_rating float Goodreads rating (1-5) goodreads_num_ratings int Number of ratings"},{"location":"datasets/UVB/#usage","title":"Usage","text":""},{"location":"datasets/UVB/#load-from-huggingface","title":"Load from HuggingFace","text":"<pre><code>from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"undertheseanlp/UVB-v0.1\")\n\n# Access the data\nfor item in dataset[\"train\"]:\n    print(f\"Title: {item['title']}\")\n    print(f\"Author: {item['author']}\")\n    print(f\"Content: {item['content'][:200]}...\")\n    print(f\"Genres: {item['genres']}\")\n    print(f\"First publish: {item['first_publish']}\")\n    break\n\n# Filter by genre\nfiction = dataset[\"train\"].filter(lambda x: \"Fiction\" in (x.get(\"genres\") or []))\nnon_fiction = dataset[\"train\"].filter(lambda x: \"Non Fiction\" in (x.get(\"genres\") or []))\n</code></pre>"},{"location":"datasets/UVB/#statistics","title":"Statistics","text":"Metric Value Total books 447 Books with genres 230 (51.5%) Books with publication year 421 (94.2%) Total size ~209 MB"},{"location":"datasets/UVB/#top-genres","title":"Top Genres","text":"Genre Count Non Fiction 76 Fiction 62 Romance 37 Classics 30 Novels 27 Philosophy 25 Self Help 25 Literature 24 History 22 Childrens 20"},{"location":"datasets/UVB/#publication-year-distribution","title":"Publication Year Distribution","text":"Period Count Before 1900 6 1900-1950 12 1951-1980 38 1981-2000 82 2001-2010 134 2011+ 149"},{"location":"datasets/UVB/#source-data","title":"Source Data","text":"<ul> <li>Vietnamese books: tmnam20/Vietnamese-Book-Corpus</li> <li>Goodreads metadata: BrightData/Goodreads-Books</li> </ul>"},{"location":"datasets/UVB/#processing-scripts","title":"Processing Scripts","text":"<p>Scripts included in the dataset repository:</p> <ul> <li><code>scripts/map_goodreads.py</code> - Map Vietnamese books to Goodreads entries using fuzzy matching</li> <li><code>scripts/add_genres.py</code> - Fetch genres from Goodreads pages</li> <li><code>scripts/add_publish_date.py</code> - Fetch first publication year from Goodreads</li> </ul>"},{"location":"datasets/UVB/#sample-books","title":"Sample Books","text":""},{"location":"datasets/UVB/#fiction","title":"Fiction","text":"Title Author Year Rating THE COMPLETE SHERLOCK HOLMES Arthur Conan Doyle 1983 4.01 L\u0128NH NAM CH\u00cdCH QU\u00c1I Tr\u1ea7n Th\u1ebf Ph\u00e1p 1492 3.80 1Q84 Haruki Murakami 2009 4.10 David Copperfield Charles Dickens 2009 4.17 S\u1ed0NG M\u00d2N Nam Cao 2008 4.23"},{"location":"datasets/UVB/#non-fiction","title":"Non Fiction","text":"Title Author Year Rating THE TIBETAN BOOK OF LIVING AND DYING Sogyal Rinpoche 1992 4.21 VI\u1ec6T NAM PHONG T\u1ee4C Phan K\u1ebf B\u00ednh 1972 4.09 T\u1ef0 H\u1eccC M\u1ed8T NHU C\u1ea6U C\u1ee6A TH\u1edcI \u0110\u1ea0I Nguy\u1ec5n Hi\u1ebfn L\u00ea 2007 4.27 THE INFORMATION James Gleick 2011 4.03 S\u1eec K\u00dd T\u01af M\u00c3 THI\u00caN T\u01b0 M\u00e3 Thi\u00ean - 4.21"},{"location":"datasets/UVB/#citation","title":"Citation","text":"<pre><code>@misc{uvb_dataset,\n  title={UVB: Underthesea Vietnamese Books Dataset},\n  author={Underthesea NLP},\n  year={2026},\n  publisher={HuggingFace},\n  url={https://huggingface.co/datasets/undertheseanlp/UVB-v0.1}\n}\n</code></pre>"},{"location":"datasets/UVB/#references","title":"References","text":"<ul> <li>HuggingFace Dataset</li> <li>GitHub Issue #720</li> <li>Goodreads</li> </ul>"},{"location":"datasets/UVN/","title":"Vietnamese News Dataset","text":"<p>A dataset of Vietnamese news articles collected from 6 major Vietnamese newspapers for NLP research.</p>"},{"location":"datasets/UVN/#dataset-description","title":"Dataset Description","text":"<ul> <li>Homepage: https://github.com/undertheseanlp/underthesea</li> <li>Repository: https://huggingface.co/datasets/undertheseanlp/UVN-1</li> <li>Point of Contact: Vu Anh (anhv.ict91@gmail.com)</li> </ul>"},{"location":"datasets/UVN/#dataset-summary","title":"Dataset Summary","text":"<p>This dataset contains 3,268 Vietnamese news articles covering various topics including politics, business, sports, entertainment, education, health, and technology. It is designed for Vietnamese NLP research tasks such as:</p> <ul> <li>Text classification (news categorization)</li> <li>Language modeling</li> <li>Text generation</li> <li>Named entity recognition</li> <li>Keyword extraction</li> </ul>"},{"location":"datasets/UVN/#languages","title":"Languages","text":"<p>Vietnamese (vi)</p>"},{"location":"datasets/UVN/#dataset-structure","title":"Dataset Structure","text":""},{"location":"datasets/UVN/#data-instance","title":"Data Instance","text":"<pre><code>{\n  \"id\": \"article-00001\",\n  \"source\": \"vnexpress\",\n  \"url\": \"https://vnexpress.net/example-123.html\",\n  \"category\": \"Th\u1eddi s\u1ef1\",\n  \"title\": \"Ti\u00eau \u0111\u1ec1 b\u00e0i b\u00e1o\",\n  \"content\": \"N\u1ed9i dung b\u00e0i b\u00e1o...\",\n  \"publish_date\": \"2026-01-28\"\n}\n</code></pre>"},{"location":"datasets/UVN/#data-fields","title":"Data Fields","text":"Field Type Description <code>id</code> string Unique identifier (article-XXXXX) <code>source</code> string News source <code>url</code> string Original article URL <code>category</code> string News category <code>title</code> string Article title <code>content</code> string Full article content <code>publish_date</code> string Publication date (yyyy-mm-dd)"},{"location":"datasets/UVN/#data-splits","title":"Data Splits","text":"Split Count train 3,268"},{"location":"datasets/UVN/#sources","title":"Sources","text":"Source Website Count VnExpress vnexpress.net 1,000 D\u00e2n Tr\u00ed dantri.com.vn 976 Thanh Ni\u00ean thanhnien.vn 579 Tu\u1ed5i Tr\u1ebb tuoitre.vn 210 Ti\u1ec1n Phong tienphong.vn 240 Ng\u01b0\u1eddi Lao \u0110\u1ed9ng nld.com.vn 263"},{"location":"datasets/UVN/#categories","title":"Categories","text":"<ul> <li>Th\u1eddi s\u1ef1 (Politics)</li> <li>Th\u1ebf gi\u1edbi (World)</li> <li>Kinh doanh (Business)</li> <li>Gi\u1ea3i tr\u00ed (Entertainment)</li> <li>Th\u1ec3 thao (Sports)</li> <li>Ph\u00e1p lu\u1eadt (Law)</li> <li>Gi\u00e1o d\u1ee5c (Education)</li> <li>S\u1ee9c kh\u1ecfe (Health)</li> <li>\u0110\u1eddi s\u1ed1ng (Lifestyle)</li> <li>Khoa h\u1ecdc (Science/Technology)</li> </ul>"},{"location":"datasets/UVN/#usage","title":"Usage","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"undertheseanlp/UVN-1\")\n\n# Access articles\nfor article in dataset[\"train\"]:\n    print(article[\"id\"], article[\"title\"])\n    break\n\n# Filter by source\nvnexpress = dataset[\"train\"].filter(lambda x: x[\"source\"] == \"vnexpress\")\n\n# Filter by category\nsports = dataset[\"train\"].filter(lambda x: x[\"category\"] == \"Th\u1ec3 thao\")\n</code></pre>"},{"location":"datasets/UVN/#considerations","title":"Considerations","text":""},{"location":"datasets/UVN/#biases","title":"Biases","text":"<ul> <li>Reflects editorial perspectives of mainstream Vietnamese news outlets</li> <li>May be biased toward urban and national news coverage</li> </ul>"},{"location":"datasets/UVN/#limitations","title":"Limitations","text":"<ul> <li>Articles from 2025-2026 time period</li> <li>Uneven distribution across sources due to website structure differences</li> </ul>"},{"location":"datasets/UVN/#license","title":"License","text":"<p>CC-BY-NC-4.0. For research and educational purposes only.</p>"},{"location":"datasets/UVN/#citation","title":"Citation","text":"<pre><code>@misc{vietnamese_news_dataset,\n  title={Vietnamese News Dataset},\n  author={Underthesea NLP},\n  year={2026},\n  publisher={HuggingFace},\n  url={https://huggingface.co/datasets/undertheseanlp/UVN-1}\n}\n</code></pre>"},{"location":"datasets/UVW/","title":"UVW","text":"<p>Underthesea Vietnamese Wikipedia Dataset (2026 Edition)</p> <p>A high-quality, cleaned dataset of 1.1M Vietnamese Wikipedia articles enriched with Wikidata metadata for NLP research.</p>"},{"location":"datasets/UVW/#huggingface-dataset","title":"HuggingFace Dataset","text":"<p>Dataset: undertheseanlp/UVW-2026</p>"},{"location":"datasets/UVW/#features","title":"Features","text":"Feature Type Description id string Unique identifier (URL-safe title) title string Article title content string Cleaned article text num_chars int32 Character count num_sentences int32 Sentence count quality_score int32 Quality score (1-10) wikidata_id string Wikidata Q-identifier main_category string Primary category from Wikidata P31"},{"location":"datasets/UVW/#usage","title":"Usage","text":""},{"location":"datasets/UVW/#load-from-huggingface","title":"Load from HuggingFace","text":"<pre><code>from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"undertheseanlp/UVW-2026\")\n\n# Access the data\ntrain = dataset[\"train\"]\nprint(train[0])\n\n# Filter high-quality articles (score &gt;= 7)\nhigh_quality = train.filter(lambda x: x[\"quality_score\"] &gt;= 7)\n\n# Filter by category\npeople = train.filter(lambda x: x[\"main_category\"] == \"ng\u01b0\u1eddi\")\n</code></pre>"},{"location":"datasets/UVW/#statistics","title":"Statistics","text":"Metric Value Total articles 1,118,224 Train split 894,579 (80%) Validation split 111,822 (10%) Test split 111,823 (10%) Wikidata coverage 99.4% Category coverage 97.0% Unique categories 11,549 Avg. characters 1,190 Avg. sentences 10"},{"location":"datasets/UVW/#quality-score-distribution","title":"Quality Score Distribution","text":"Score Count Percentage 1 134 0.0% 2 376 0.0% 3 28,267 2.5% 4 607,081 54.3% 5 208,304 18.6% 6 134,385 12.0% 7 70,345 6.3% 8 57,054 5.1% 9 9,649 0.9% 10 2,629 0.2%"},{"location":"datasets/UVW/#top-categories","title":"Top Categories","text":"Category (Vietnamese) Count Percentage \u0111\u01a1n v\u1ecb ph\u00e2n lo\u1ea1i (taxon) 618,281 55.3% ng\u01b0\u1eddi (human) 78,191 7.0% x\u00e3 c\u1ee7a Ph\u00e1p 35,635 3.2% khu \u0111\u1ecbnh c\u01b0 20,276 1.8% ti\u1ec3u h\u00e0nh tinh 17,891 1.6% x\u00e3 c\u1ee7a Vi\u1ec7t Nam 7,088 0.6% \u0111\u00f4 th\u1ecb c\u1ee7a \u00dd 6,700 0.6% trang \u0111\u1ecbnh h\u01b0\u1edbng Wikimedia 6,202 0.6%"},{"location":"datasets/UVW/#quality-scoring","title":"Quality Scoring","text":"<p>Articles are scored 1-10 based on:</p> Component Weight Criteria Length 40% Character count (200 - 100,000 optimal) Sentences 30% Sentence count (3 - 1,000 optimal) Density 30% Avg sentence length (80-150 chars optimal) Wikidata bonus +0.5 Has wikidata_id Category bonus +0.5 Has main_category Markup penalty -1 to -3 Remaining Wikipedia markup"},{"location":"datasets/UVW/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<ol> <li>Download - Vietnamese Wikipedia XML dump from Wikimedia</li> <li>Extract - Parse XML and extract article content</li> <li>Clean - Remove Wikipedia markup (templates, refs, links, tables)</li> <li>Normalize - Apply Unicode NFC normalization</li> <li>Score - Calculate quality metrics for each article</li> <li>Enrich - Add Wikidata IDs and categories via Wikidata API</li> <li>Filter - Remove special pages, redirects, disambiguation, short articles</li> <li>Split - Create train/validation/test splits (80/10/10, seed=42)</li> </ol>"},{"location":"datasets/UVW/#removed-content","title":"Removed Content","text":"<ul> <li>Wikipedia templates (<code>{{...}}</code>)</li> <li>References and citations (<code>&lt;ref&gt;...&lt;/ref&gt;</code>)</li> <li>HTML tags and comments</li> <li>Category links (<code>[[Th\u1ec3 lo\u1ea1i:...]]</code>)</li> <li>File/image links (<code>[[T\u1eadp tin:...]]</code>, <code>[[File:...]]</code>)</li> <li>Interwiki links</li> <li>Tables (<code>{| ... |}</code>)</li> <li>Infoboxes and navigation templates</li> </ul>"},{"location":"datasets/UVW/#sample-articles","title":"Sample Articles","text":"Title Category Quality Wikidata Vi\u1ec7t Nam qu\u1ed1c gia c\u00f3 ch\u1ee7 quy\u1ec1n 9 Q881 H\u00e0 N\u1ed9i th\u1ee7 \u0111\u00f4 9 Q1858 Nguy\u1ec5n Du ng\u01b0\u1eddi 8 Q332972 S\u00f4ng M\u00ea K\u00f4ng s\u00f4ng 8 Q3056359 Ph\u1edf m\u00f3n \u0103n 7 Q217666"},{"location":"datasets/UVW/#citation","title":"Citation","text":"<pre><code>@dataset{uvw2026,\n  title = {UVW 2026: Underthesea Vietnamese Wikipedia Dataset},\n  author = {Underthesea NLP},\n  year = {2026},\n  publisher = {Hugging Face},\n  url = {https://huggingface.co/datasets/undertheseanlp/UVW-2026},\n  note = {Vietnamese Wikipedia articles enriched with Wikidata metadata}\n}\n</code></pre>"},{"location":"datasets/UVW/#references","title":"References","text":"<ul> <li>HuggingFace Dataset</li> <li>GitHub Repository</li> <li>GitHub Issue #896</li> <li>Vietnamese Wikipedia</li> <li>Wikidata</li> </ul>"},{"location":"developer/architecture/","title":"Architecture","text":"<p>This document describes the internal architecture of Underthesea.</p>"},{"location":"developer/architecture/#overview","title":"Overview","text":"<p>Underthesea is organized as a collection of NLP pipelines, each handling a specific task.</p> <pre><code>underthesea/\n\u251c\u2500\u2500 pipeline/              # Main NLP modules\n\u2502   \u251c\u2500\u2500 sent_tokenize/     # Sentence segmentation\n\u2502   \u251c\u2500\u2500 text_normalize/    # Text normalization\n\u2502   \u251c\u2500\u2500 word_tokenize/     # Word segmentation\n\u2502   \u251c\u2500\u2500 pos_tag/           # POS tagging\n\u2502   \u251c\u2500\u2500 chunking/          # Phrase chunking\n\u2502   \u251c\u2500\u2500 dependency_parse/  # Dependency parsing\n\u2502   \u251c\u2500\u2500 ner/               # Named entity recognition\n\u2502   \u251c\u2500\u2500 classification/    # Text classification\n\u2502   \u251c\u2500\u2500 sentiment/         # Sentiment analysis\n\u2502   \u251c\u2500\u2500 translate/         # Translation\n\u2502   \u251c\u2500\u2500 lang_detect/       # Language detection\n\u2502   \u2514\u2500\u2500 tts/               # Text-to-speech\n\u251c\u2500\u2500 models/                # Model implementations\n\u251c\u2500\u2500 datasets/              # Built-in datasets\n\u251c\u2500\u2500 corpus/                # Corpus handling\n\u251c\u2500\u2500 resources/             # Static resources\n\u2514\u2500\u2500 cli.py                 # CLI interface\n</code></pre>"},{"location":"developer/architecture/#pipeline-module-structure","title":"Pipeline Module Structure","text":"<p>Each pipeline module follows a consistent pattern:</p> <pre><code>pipeline/word_tokenize/\n\u251c\u2500\u2500 __init__.py            # Main API function\n\u251c\u2500\u2500 model.py               # Model implementation\n\u251c\u2500\u2500 feature.py             # Feature extraction\n\u2514\u2500\u2500 default_model/         # Default model files\n</code></pre>"},{"location":"developer/architecture/#main-api-__init__py","title":"Main API (<code>__init__.py</code>)","text":"<pre><code># Lazy loading pattern\n_model = None\n\ndef word_tokenize(sentence, format=None):\n    global _model\n    if _model is None:\n        _model = load_model()\n    return _model.predict(sentence, format)\n</code></pre>"},{"location":"developer/architecture/#model-implementation","title":"Model Implementation","text":"<pre><code>class CRFModel:\n    def __init__(self, model_path):\n        self.model = load_crf(model_path)\n\n    def predict(self, text):\n        features = extract_features(text)\n        return self.model.tag(features)\n</code></pre>"},{"location":"developer/architecture/#lazy-loading","title":"Lazy Loading","text":"<p>Models are loaded on first use to minimize startup time:</p> <pre><code># At import time - no model loaded\nfrom underthesea import word_tokenize\n\n# First call - model loaded and cached\nresult = word_tokenize(\"text\")\n\n# Subsequent calls - uses cached model\nresult = word_tokenize(\"more text\")\n</code></pre> <p>Benefits: - Fast import time - Memory efficiency (only used models loaded) - Simple API</p>"},{"location":"developer/architecture/#model-types","title":"Model Types","text":""},{"location":"developer/architecture/#crf-models","title":"CRF Models","text":"<p>Used for: word segmentation, POS tagging, chunking, NER, classification, sentiment</p> <pre><code># Uses python-crfsuite\nimport pycrfsuite\n\nclass CRFTagger:\n    def __init__(self, model_path):\n        self.tagger = pycrfsuite.Tagger()\n        self.tagger.open(model_path)\n\n    def tag(self, features):\n        return self.tagger.tag(features)\n</code></pre>"},{"location":"developer/architecture/#deep-learning-models","title":"Deep Learning Models","text":"<p>Used for: dependency parsing, deep NER, translation</p> <pre><code># Uses transformers\nfrom transformers import AutoModel, AutoTokenizer\n\nclass TransformerModel:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n</code></pre>"},{"location":"developer/architecture/#fasttext-models","title":"FastText Models","text":"<p>Used for: language detection</p> <pre><code>import fasttext\n\nclass LangDetector:\n    def __init__(self, model_path):\n        self.model = fasttext.load_model(model_path)\n\n    def detect(self, text):\n        prediction = self.model.predict(text)\n        return prediction[0][0].replace('__label__', '')\n</code></pre>"},{"location":"developer/architecture/#feature-extraction","title":"Feature Extraction","text":"<p>Features are extracted for CRF models:</p> <pre><code>def extract_features(sentence):\n    features = []\n    for i, word in enumerate(sentence):\n        word_features = {\n            'word': word,\n            'is_upper': word.isupper(),\n            'is_title': word.istitle(),\n            'prev_word': sentence[i-1] if i &gt; 0 else 'BOS',\n            'next_word': sentence[i+1] if i &lt; len(sentence)-1 else 'EOS',\n        }\n        features.append(word_features)\n    return features\n</code></pre>"},{"location":"developer/architecture/#resource-management","title":"Resource Management","text":""},{"location":"developer/architecture/#model-storage","title":"Model Storage","text":"<p>Models are stored in <code>~/.underthesea/models/</code>:</p> <pre><code>~/.underthesea/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 WS_VLSP2013_CRF/\n\u2502   \u251c\u2500\u2500 POS_VLSP2013_CRF/\n\u2502   \u2514\u2500\u2500 NER_VLSP2016_BERT/\n\u2514\u2500\u2500 datasets/\n    \u251c\u2500\u2500 VNTC/\n    \u2514\u2500\u2500 UTS2017-BANK/\n</code></pre>"},{"location":"developer/architecture/#model-download","title":"Model Download","text":"<pre><code>def download_model(model_name):\n    url = get_model_url(model_name)\n    local_path = get_local_path(model_name)\n\n    if not os.path.exists(local_path):\n        download_file(url, local_path)\n        extract_archive(local_path)\n\n    return local_path\n</code></pre>"},{"location":"developer/architecture/#rust-extension","title":"Rust Extension","text":"<p>Performance-critical code uses the Rust extension:</p> <pre><code>extensions/underthesea_core/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 lib.rs             # Rust implementation\n\u251c\u2500\u2500 Cargo.toml             # Rust dependencies\n\u2514\u2500\u2500 pyproject.toml         # Python binding config\n</code></pre> <p>Built with maturin:</p> <pre><code>cd extensions/underthesea_core\nmaturin develop\n</code></pre>"},{"location":"developer/architecture/#cli-architecture","title":"CLI Architecture","text":"<p>The CLI uses Click:</p> <pre><code># cli.py\nimport click\n\n@click.group()\ndef cli():\n    pass\n\n@cli.command()\ndef list_data():\n    \"\"\"List available datasets.\"\"\"\n    for dataset in get_datasets():\n        print(dataset)\n\n@cli.command()\n@click.argument('text')\ndef tts(text):\n    \"\"\"Convert text to speech.\"\"\"\n    from underthesea.pipeline.tts import tts\n    tts(text)\n</code></pre>"},{"location":"developer/architecture/#optional-dependencies","title":"Optional Dependencies","text":"<p>Optional features are guarded:</p> <pre><code>def translate(text):\n    try:\n        from transformers import AutoModel\n    except ImportError:\n        raise ImportError(\n            \"Translation requires deep learning dependencies. \"\n            \"Install with: pip install 'underthesea[deep]'\"\n        )\n    # ... translation logic\n</code></pre>"},{"location":"developer/architecture/#testing-architecture","title":"Testing Architecture","text":"<pre><code>tests/\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 word_tokenize/\n\u2502   \u2502   \u2514\u2500\u2500 test_word_tokenize.py\n\u2502   \u251c\u2500\u2500 pos_tag/\n\u2502   \u2502   \u2514\u2500\u2500 test_pos_tag.py\n\u2502   \u2514\u2500\u2500 ner/\n\u2502       \u2514\u2500\u2500 test_ner.py\n\u2514\u2500\u2500 conftest.py            # Pytest fixtures\n</code></pre>"},{"location":"developer/architecture/#extending-underthesea","title":"Extending Underthesea","text":""},{"location":"developer/architecture/#adding-a-new-pipeline","title":"Adding a New Pipeline","text":"<ol> <li>Create directory: <code>underthesea/pipeline/new_task/</code></li> <li>Implement <code>__init__.py</code> with main API</li> <li>Add model implementation</li> <li>Export from <code>underthesea/__init__.py</code></li> <li>Add tests in <code>tests/pipeline/new_task/</code></li> <li>Add documentation</li> </ol>"},{"location":"developer/architecture/#adding-a-new-model","title":"Adding a New Model","text":"<ol> <li>Train the model using appropriate toolkit</li> <li>Save model files</li> <li>Update model registry</li> <li>Add download logic</li> <li>Test with existing pipeline</li> </ol>"},{"location":"developer/contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Underthesea! This guide will help you get started.</p>"},{"location":"developer/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"developer/contributing/#bug-reports","title":"Bug Reports","text":"<ul> <li>Check existing GitHub Issues first</li> <li>Include Python version, OS, and Underthesea version</li> <li>Provide minimal code to reproduce the issue</li> <li>Include full error traceback</li> </ul>"},{"location":"developer/contributing/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Reference the issue number in your PR</li> <li>Include tests that demonstrate the fix</li> <li>Update documentation if needed</li> </ul>"},{"location":"developer/contributing/#new-features","title":"New Features","text":"<ul> <li>Open an issue to discuss the feature first</li> <li>Follow the existing code style</li> <li>Add tests and documentation</li> </ul>"},{"location":"developer/contributing/#documentation","title":"Documentation","text":"<ul> <li>Fix typos and improve clarity</li> <li>Add examples and tutorials</li> <li>Translate documentation</li> </ul>"},{"location":"developer/contributing/#development-setup","title":"Development Setup","text":""},{"location":"developer/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"developer/contributing/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/undertheseanlp/underthesea.git\ncd underthesea\n\n# Create virtual environment with uv\nuv venv\nsource .venv/bin/activate\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"developer/contributing/#macos-arm64-apple-silicon","title":"macOS ARM64 (Apple Silicon)","text":"<p>Build the Rust extension:</p> <pre><code>cd extensions/underthesea_core\nuv pip install maturin\nmaturin develop\ncd ../..\n</code></pre>"},{"location":"developer/contributing/#code-style","title":"Code Style","text":""},{"location":"developer/contributing/#linting","title":"Linting","text":"<p>We use Ruff for linting:</p> <pre><code># Check for issues\nruff check underthesea/\n\n# Auto-fix issues\nruff check underthesea/ --fix\n</code></pre>"},{"location":"developer/contributing/#configuration","title":"Configuration","text":"<p>Ruff configuration is in <code>pyproject.toml</code>.</p>"},{"location":"developer/contributing/#testing","title":"Testing","text":""},{"location":"developer/contributing/#test-categories","title":"Test Categories","text":"Command Description <code>tox -e lint</code> Linting with Ruff <code>tox -e core</code> Core module tests <code>tox -e deep</code> Deep learning tests <code>tox -e prompt</code> Prompt model tests <code>tox -e langdetect</code> Language detection tests"},{"location":"developer/contributing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Word tokenization tests\nuv run python -m unittest discover tests.pipeline.word_tokenize\n\n# POS tagging tests\nuv run python -m unittest discover tests.pipeline.pos_tag\n\n# NER tests\nuv run python -m unittest tests.pipeline.ner.test_ner\n\n# Classification tests\nuv run python -m unittest tests.pipeline.classification.test_bank\n\n# Translation tests\nuv run python -m unittest discover tests.pipeline.translate\n</code></pre>"},{"location":"developer/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Mirror the source structure</li> <li>Use Python's <code>unittest</code> framework</li> <li>Include both positive and edge case tests</li> </ul> <pre><code>import unittest\nfrom underthesea import word_tokenize\n\nclass TestWordTokenize(unittest.TestCase):\n    def test_basic(self):\n        result = word_tokenize(\"Xin ch\u00e0o Vi\u1ec7t Nam\")\n        self.assertEqual(result, ['Xin', 'ch\u00e0o', 'Vi\u1ec7t Nam'])\n\n    def test_empty_string(self):\n        result = word_tokenize(\"\")\n        self.assertEqual(result, [])\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"developer/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"developer/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Update your branch: Rebase on latest <code>main</code></li> <li>Run linting: <code>ruff check underthesea/</code></li> <li>Run tests: <code>tox -e core</code></li> <li>Update docs: If adding features</li> </ol>"},{"location":"developer/contributing/#pr-guidelines","title":"PR Guidelines","text":"<ul> <li>Use clear, descriptive titles</li> <li>Reference related issues</li> <li>Describe changes and motivation</li> <li>Include test results</li> <li>Add screenshots for UI changes</li> </ul>"},{"location":"developer/contributing/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Related Issues\nFixes #123\n\n## Changes\n- Added X feature\n- Fixed Y bug\n- Updated Z documentation\n\n## Testing\n- [ ] Linting passes\n- [ ] Unit tests pass\n- [ ] Manual testing done\n\n## Documentation\n- [ ] Updated relevant docs\n- [ ] Added docstrings\n</code></pre>"},{"location":"developer/contributing/#project-structure","title":"Project Structure","text":"<pre><code>underthesea/\n\u251c\u2500\u2500 underthesea/           # Main package\n\u2502   \u251c\u2500\u2500 pipeline/          # NLP modules\n\u2502   \u251c\u2500\u2500 models/            # Model implementations\n\u2502   \u251c\u2500\u2500 datasets/          # Built-in datasets\n\u2502   \u251c\u2500\u2500 corpus/            # Corpus handling\n\u2502   \u2514\u2500\u2500 cli.py             # CLI commands\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 extensions/            # Rust extension, apps\n\u2514\u2500\u2500 pyproject.toml         # Project configuration\n</code></pre>"},{"location":"developer/contributing/#cli-commands","title":"CLI Commands","text":"<pre><code># List available data\nunderthesea list-data\n\n# List available models\nunderthesea list-model\n\n# Download data\nunderthesea download-data VNTC\n</code></pre>"},{"location":"developer/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> <li>Facebook Community</li> </ul>"},{"location":"developer/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers feel welcome</li> </ul>"},{"location":"developer/releasing/","title":"Releasing","text":"<p>This document describes the release process for Underthesea.</p>"},{"location":"developer/releasing/#version-scheme","title":"Version Scheme","text":"<p>Underthesea follows Semantic Versioning:</p> <ul> <li>MAJOR.MINOR.PATCH (e.g., 9.0.0)</li> <li>MAJOR: Breaking changes</li> <li>MINOR: New features, backward compatible</li> <li>PATCH: Bug fixes, backward compatible</li> </ul>"},{"location":"developer/releasing/#release-checklist","title":"Release Checklist","text":""},{"location":"developer/releasing/#pre-release","title":"Pre-release","text":"<ul> <li>[ ] All tests passing</li> <li>[ ] Documentation updated</li> <li>[ ] CHANGELOG updated</li> <li>[ ] Version bumped in <code>pyproject.toml</code></li> <li>[ ] Version bumped in <code>__init__.py</code></li> </ul>"},{"location":"developer/releasing/#release-steps","title":"Release Steps","text":"<ol> <li>Update version</li> </ol> <pre><code># underthesea/__init__.py\n__version__ = \"9.1.0\"\n</code></pre> <pre><code># pyproject.toml\n[project]\nversion = \"9.1.0\"\n</code></pre> <ol> <li>Update CHANGELOG</li> </ol> <p>Add entry to <code>docs/history.md</code>:</p> <pre><code>## 9.1.0 (2024-XX-XX)\n\n### New Features\n- Added X feature\n\n### Bug Fixes\n- Fixed Y issue\n\n### Documentation\n- Updated Z docs\n</code></pre> <ol> <li>Run tests</li> </ol> <pre><code>tox -e lint\ntox -e core\n</code></pre> <ol> <li>Create release commit</li> </ol> <pre><code>git add -A\ngit commit -m \"Release version 9.1.0\"\ngit tag v9.1.0\n</code></pre> <ol> <li>Push to GitHub</li> </ol> <pre><code>git push origin main\ngit push origin v9.1.0\n</code></pre> <ol> <li>Publish to PyPI</li> </ol> <p>GitHub Actions will automatically publish when a tag is pushed.</p> <p>Or manually:</p> <pre><code>uv pip install build twine\npython -m build\ntwine upload dist/*\n</code></pre> <ol> <li> <p>Create GitHub Release</p> </li> <li> <p>Go to Releases</p> </li> <li>Click \"Draft a new release\"</li> <li>Select the tag</li> <li>Add release notes</li> <li>Publish</li> </ol>"},{"location":"developer/releasing/#versioning-guidelines","title":"Versioning Guidelines","text":""},{"location":"developer/releasing/#when-to-bump-major","title":"When to bump MAJOR","text":"<ul> <li>Removing a public function</li> <li>Changing function signatures</li> <li>Dropping Python version support</li> <li>Breaking changes to output format</li> </ul>"},{"location":"developer/releasing/#when-to-bump-minor","title":"When to bump MINOR","text":"<ul> <li>Adding new functions</li> <li>Adding new parameters (with defaults)</li> <li>New model support</li> <li>New optional features</li> </ul>"},{"location":"developer/releasing/#when-to-bump-patch","title":"When to bump PATCH","text":"<ul> <li>Bug fixes</li> <li>Performance improvements</li> <li>Documentation updates</li> <li>Dependency updates</li> </ul>"},{"location":"developer/releasing/#hotfix-process","title":"Hotfix Process","text":"<p>For urgent bug fixes:</p> <ol> <li> <p>Create branch from latest release tag:    <pre><code>git checkout -b hotfix/9.0.1 v9.0.0\n</code></pre></p> </li> <li> <p>Fix the bug and add tests</p> </li> <li> <p>Bump PATCH version</p> </li> <li> <p>Merge to main and tag:    <pre><code>git checkout main\ngit merge hotfix/9.0.1\ngit tag v9.0.1\ngit push origin main v9.0.1\n</code></pre></p> </li> </ol>"},{"location":"developer/releasing/#release-automation","title":"Release Automation","text":"<p>GitHub Actions workflow (<code>.github/workflows/publish.yml</code>):</p> <pre><code>name: Publish to PyPI\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n      - run: pip install build twine\n      - run: python -m build\n      - run: twine upload dist/*\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}\n</code></pre>"},{"location":"developer/releasing/#post-release","title":"Post-release","text":"<ul> <li>[ ] Verify package on PyPI</li> <li>[ ] Test installation: <code>pip install underthesea==9.1.0</code></li> <li>[ ] Update ReadTheDocs if needed</li> <li>[ ] Announce on social media</li> <li>[ ] Close related GitHub issues</li> </ul>"},{"location":"technical_reports/dependency_parsing/","title":"Dependency Parsing Technical Report","text":""},{"location":"technical_reports/dependency_parsing/#overview","title":"Overview","text":"<p>The dependency parsing module in Underthesea provides Vietnamese dependency parsing using a Biaffine Neural Dependency Parser. This document describes the architecture, training process, and recent updates for PyTorch v2 compatibility.</p>"},{"location":"technical_reports/dependency_parsing/#architecture","title":"Architecture","text":""},{"location":"technical_reports/dependency_parsing/#model-biaffine-dependency-parser","title":"Model: Biaffine Dependency Parser","text":"<p>The implementation is based on the Deep Biaffine Attention architecture proposed by Dozat and Manning (2017).</p> <p>Reference: Timothy Dozat and Christopher D. Manning. 2017. Deep Biaffine Attention for Neural Dependency Parsing</p>"},{"location":"technical_reports/dependency_parsing/#components","title":"Components","text":"<pre><code>DependencyParser\n\u251c\u2500\u2500 Embeddings\n\u2502   \u251c\u2500\u2500 word_embed: nn.Embedding (word embeddings)\n\u2502   \u251c\u2500\u2500 feat_embed: CharLSTM | BertEmbedding | nn.Embedding (feature embeddings)\n\u2502   \u2514\u2500\u2500 embed_dropout: IndependentDropout\n\u251c\u2500\u2500 Encoder\n\u2502   \u251c\u2500\u2500 lstm: BiLSTM (3-layer bidirectional LSTM)\n\u2502   \u2514\u2500\u2500 lstm_dropout: SharedDropout\n\u251c\u2500\u2500 MLP Layers\n\u2502   \u251c\u2500\u2500 mlp_arc_d/h: MLP (arc head/dependent)\n\u2502   \u2514\u2500\u2500 mlp_rel_d/h: MLP (relation head/dependent)\n\u2514\u2500\u2500 Biaffine Attention\n    \u251c\u2500\u2500 arc_attn: Biaffine (arc scoring)\n    \u2514\u2500\u2500 rel_attn: Biaffine (relation scoring)\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#default-hyperparameters","title":"Default Hyperparameters","text":"Parameter Value Description n_embed 50 Word embedding dimension n_feat_embed 100 Feature embedding dimension n_char_embed 50 Character embedding dimension n_lstm_hidden 400 BiLSTM hidden size n_lstm_layers 3 Number of BiLSTM layers n_mlp_arc 500 Arc MLP output size n_mlp_rel 100 Relation MLP output size embed_dropout 0.33 Embedding dropout lstm_dropout 0.33 LSTM dropout mlp_dropout 0.33 MLP dropout"},{"location":"technical_reports/dependency_parsing/#file-structure","title":"File Structure","text":"<pre><code>underthesea/\n\u251c\u2500\u2500 pipeline/dependency_parse/\n\u2502   \u251c\u2500\u2500 __init__.py          # Main API: dependency_parse()\n\u2502   \u2514\u2500\u2500 visualize.py         # Visualization utilities\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 dependency_parser.py # DependencyParser model\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 base.py              # BiLSTM, MLP, Biaffine, CharLSTM\n\u2502   \u251c\u2500\u2500 bert.py              # BERT embeddings\n\u2502   \u2514\u2500\u2500 embeddings.py        # FieldEmbeddings, CharacterEmbeddings\n\u251c\u2500\u2500 trainers/\n\u2502   \u2514\u2500\u2500 dependency_parser_trainer.py\n\u251c\u2500\u2500 transforms/\n\u2502   \u2514\u2500\u2500 conll.py             # CoNLL format handling\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 sp_field.py          # Field, SubwordField\n    \u251c\u2500\u2500 sp_vocab.py          # Vocabulary\n    \u251c\u2500\u2500 sp_data.py           # Dataset\n    \u251c\u2500\u2500 sp_metric.py         # AttachmentMetric (UAS, LAS)\n    \u2514\u2500\u2500 sp_alg.py            # Eisner, MST algorithms\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#performance","title":"Performance","text":""},{"location":"technical_reports/dependency_parsing/#benchmark-results-on-vlsp2020-dp","title":"Benchmark Results on VLSP2020-DP","text":"Model UAS LAS UCM LCM MaltParser (baseline) 75.41% 66.11% - - Biaffine Attention (v1) 87.28% 72.63% 30.67% 6.98% vi-dp-v1a1 (current) 87.10% 80.00% - -"},{"location":"technical_reports/dependency_parsing/#metrics-description","title":"Metrics Description","text":"Metric Description UAS (Unlabeled Attachment Score) Percentage of tokens with correct head LAS (Labeled Attachment Score) Percentage of tokens with correct head AND relation label UCM (Unlabeled Complete Match) Percentage of sentences with ALL heads correct LCM (Labeled Complete Match) Percentage of sentences with ALL heads and labels correct"},{"location":"technical_reports/dependency_parsing/#training-history","title":"Training History","text":"<p>MaltParser Baseline (2020) <pre><code>Metric     | Precision |    Recall |  F1 Score\n-----------+-----------+-----------+-----------\nUAS        |     75.41 |     75.41 |     75.41\nLAS        |     66.11 |     66.11 |     66.11\nCLAS       |     62.70 |     62.17 |     62.43\n</code></pre></p> <p>Biaffine Attention v1 (240 epochs, 2020) <pre><code>2020-11-29 23:05:58 Epoch 240 saved\ndev:   - UCM: 30.67% LCM:  6.98% UAS: 87.28% LAS: 72.63%\ntest:  - UCM: 30.67% LCM:  6.98% UAS: 87.28% LAS: 72.63%\nTraining time: 33m 46s, 5.96s/epoch\n</code></pre></p>"},{"location":"technical_reports/dependency_parsing/#comparison-with-other-methods","title":"Comparison with Other Methods","text":"Method Year UAS LAS Notes MST Parser (N.L. Minh et al.) 2008 - - 450 sentences corpus MaltParser (N.T. Luong et al.) 2013 - - Vietnamese treebank MaltParser (baseline) 2020 75.41% 66.11% VLSP2020-DP Biaffine Attention 2020 87.28% 72.63% VLSP2020-DP NLP@UIT (VLSP2019 winner) 2019 - - Ensemble model"},{"location":"technical_reports/dependency_parsing/#pretrained-models","title":"Pretrained Models","text":"Model Description UAS LAS Source vi-dp-v1a1 Default Vietnamese dependency parser 87.10% 80.00% Trained on VLSP2020-DP <p>Models are automatically downloaded from the Underthesea resources repository.</p>"},{"location":"technical_reports/dependency_parsing/#training","title":"Training","text":""},{"location":"technical_reports/dependency_parsing/#dataset","title":"Dataset","text":"<ul> <li>VLSP2020-DP: Vietnamese dependency parsing dataset from VLSP 2020 shared task</li> <li>VLSP2020_DP_SAMPLE: Small sample dataset for testing (auto-downloadable)</li> </ul>"},{"location":"technical_reports/dependency_parsing/#training-script","title":"Training Script","text":"<pre><code>from underthesea.datasets.vlsp2020_dp import VLSP2020_DP_SAMPLE\nfrom underthesea.models.dependency_parser import DependencyParser\nfrom underthesea.modules.embeddings import FieldEmbeddings, CharacterEmbeddings\nfrom underthesea.trainers.dependency_parser_trainer import DependencyParserTrainer\n\n# Load corpus\ncorpus = VLSP2020_DP_SAMPLE()\n\n# Initialize parser\nembeddings = [FieldEmbeddings(), CharacterEmbeddings()]\nparser = DependencyParser(embeddings=embeddings, init_pre_train=True)\n\n# Train\ntrainer = DependencyParserTrainer(parser, corpus)\ntrainer.train(\n    base_path=\"path/to/save/model\",\n    max_epochs=100,\n    lr=2e-3,\n    mu=0.9,\n    batch_size=5000\n)\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#training-parameters","title":"Training Parameters","text":"Parameter Default Description lr 2e-3 Learning rate mu 0.9 Adam beta1 nu 0.9 Adam beta2 epsilon 1e-12 Adam epsilon clip 5.0 Gradient clipping decay 0.75 LR decay rate decay_steps 5000 LR decay steps patience 100 Early stopping patience batch_size 5000 Batch size (tokens) buckets 1000 Number of length buckets"},{"location":"technical_reports/dependency_parsing/#usage","title":"Usage","text":""},{"location":"technical_reports/dependency_parsing/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import dependency_parse\n\nresult = dependency_parse(\"T\u00f4i l\u00e0 sinh vi\u00ean Vi\u1ec7t Nam\")\n# Output: [('T\u00f4i', 3, 'nsubj'), ('l\u00e0', 3, 'cop'), ('sinh vi\u00ean', 0, 'root'), ('Vi\u1ec7t Nam', 3, 'compound')]\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#output-format","title":"Output Format","text":"<p>Each tuple contains: - word: The token - head: Index of the head token (0 = root) - relation: Dependency relation label</p>"},{"location":"technical_reports/dependency_parsing/#visualization","title":"Visualization","text":"<pre><code>from underthesea.pipeline.dependency_parse import render, display\n\n# Render as SVG\nsvg = render(\"T\u00f4i y\u00eau Vi\u1ec7t Nam\")\n\n# Display in notebook\ndisplay(\"T\u00f4i y\u00eau Vi\u1ec7t Nam\")\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#pytorch-v2-compatibility-gh-706","title":"PyTorch v2 Compatibility (GH-706)","text":""},{"location":"technical_reports/dependency_parsing/#issue","title":"Issue","text":"<p>The dependency parsing module was incompatible with PyTorch v2.0+ due to: 1. Deprecated <code>weights_only</code> parameter in <code>torch.load</code> 2. Deprecated <code>apply_permutation</code> function 3. Deprecated non-tuple sequence indexing</p>"},{"location":"technical_reports/dependency_parsing/#fixes-applied","title":"Fixes Applied","text":""},{"location":"technical_reports/dependency_parsing/#1-torchload-with-weights_onlyfalse","title":"1. torch.load with weights_only=False","text":"<p>PyTorch 2.0+ requires explicit <code>weights_only</code> parameter. Since models contain pickled Python objects (transforms, vocabularies), we use <code>weights_only=False</code>:</p> <pre><code># Before\nstate = torch.load(path)\n\n# After\nstate = torch.load(path, map_location='cpu', weights_only=False)\n</code></pre> <p>Files modified: - <code>underthesea/models/dependency_parser.py</code> - <code>underthesea/models/model.py</code> - <code>underthesea/modules/nn.py</code></p>"},{"location":"technical_reports/dependency_parsing/#2-replace-deprecated-apply_permutation","title":"2. Replace deprecated apply_permutation","text":"<pre><code># Before\nfrom torch.nn.modules.rnn import apply_permutation\nh = apply_permutation(hx[0], permutation)\n\n# After\nh = hx[0].index_select(0, permutation)\n</code></pre> <p>File modified: <code>underthesea/modules/base.py</code></p>"},{"location":"technical_reports/dependency_parsing/#3-fix-non-tuple-sequence-indexing","title":"3. Fix non-tuple sequence indexing","text":"<pre><code># Before\nout_tensor[i][[slice(0, i) for i in tensor.size()]] = tensor\n\n# After\nout_tensor[i][tuple(slice(0, s) for s in tensor.size())] = tensor\n</code></pre> <p>File modified: <code>underthesea/utils/sp_fn.py</code></p>"},{"location":"technical_reports/dependency_parsing/#4-additional-fixes","title":"4. Additional Fixes","text":"<ul> <li>Added <code>self.bert</code> attribute storage in <code>DependencyParser.__init__</code> for training compatibility</li> <li>Fixed <code>n_rels</code> parameter in <code>_init_model_with_state_dict</code> (was incorrectly using <code>n_feats</code>)</li> <li>Fixed device import in <code>DependencyParserTrainer</code></li> <li>Added first-epoch model saving to ensure training always produces a model</li> </ul>"},{"location":"technical_reports/dependency_parsing/#testing","title":"Testing","text":"<p>CI test added to verify training works with PyTorch v2:</p> <pre><code># Run training test\ntox -e train-dep\n\n# Or directly\npython -m unittest tests.pipeline.dependency_parse.test_train\n</code></pre>"},{"location":"technical_reports/dependency_parsing/#dependency-relations","title":"Dependency Relations","text":"<p>Common Vietnamese dependency relations:</p> Relation Description Example root Root of sentence Main verb nsubj Nominal subject T\u00f4i (I) -&gt; \u0103n (eat) obj Direct object c\u01a1m (rice) &lt;- \u0103n (eat) cop Copula l\u00e0 (is) -&gt; noun compound Compound Vi\u1ec7t Nam &lt;- sinh vi\u00ean nmod Nominal modifier c\u1ee7a (of) relations amod Adjectival modifier \u0111\u1eb9p (beautiful) -&gt; noun advmod Adverbial modifier r\u1ea5t (very) -&gt; adj punct Punctuation . , ! ?"},{"location":"technical_reports/dependency_parsing/#references","title":"References","text":"<ol> <li>Dozat, T., &amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. ICLR 2017.</li> <li>VLSP 2020 Shared Task: Vietnamese Dependency Parsing</li> <li>Underthesea GitHub Repository</li> </ol>"},{"location":"technical_reports/dependency_parsing/#changelog","title":"Changelog","text":""},{"location":"technical_reports/dependency_parsing/#version-913-pr-871","title":"Version 9.1.3 (PR #871)","text":"<ul> <li>Added PyTorch v2.0+ support</li> <li>Fixed deprecated API usage</li> <li>Added training CI test (train-dep)</li> <li>Re-enabled dependency_parse tests in CI</li> </ul>"},{"location":"technical_reports/sent_tokenize/","title":"Sentence Tokenization Technical Report","text":""},{"location":"technical_reports/sent_tokenize/#overview","title":"Overview","text":"<p>The sentence tokenization module in Underthesea provides Vietnamese sentence boundary detection using a custom Punkt-style algorithm. This document describes the architecture, algorithm, and the migration from NLTK dependency to a standalone implementation.</p>"},{"location":"technical_reports/sent_tokenize/#architecture","title":"Architecture","text":""},{"location":"technical_reports/sent_tokenize/#algorithm-punkt-style-sentence-tokenizer","title":"Algorithm: Punkt-style Sentence Tokenizer","text":"<p>The implementation is inspired by the Punkt sentence boundary detection algorithm developed by Kiss and Strunk (2006).</p> <p>Reference: Tibor Kiss and Jan Strunk. 2006. Unsupervised Multilingual Sentence Boundary Detection</p>"},{"location":"technical_reports/sent_tokenize/#components","title":"Components","text":"<pre><code>PunktSentenceTokenizer\n\u251c\u2500\u2500 Configuration\n\u2502   \u251c\u2500\u2500 SENT_END_CHARS: frozenset('.', '?', '!')\n\u2502   \u2514\u2500\u2500 abbrev_types: Set[str] (abbreviation dictionary)\n\u251c\u2500\u2500 Core Methods\n\u2502   \u251c\u2500\u2500 sentences_from_text(): Main tokenization entry point\n\u2502   \u251c\u2500\u2500 _is_sentence_boundary(): Boundary detection logic\n\u2502   \u2514\u2500\u2500 _get_preceding_word(): Abbreviation extraction\n\u2514\u2500\u2500 Data\n    \u2514\u2500\u2500 punkt_params.json (368 Vietnamese abbreviations)\n</code></pre>"},{"location":"technical_reports/sent_tokenize/#algorithm-details","title":"Algorithm Details","text":""},{"location":"technical_reports/sent_tokenize/#sentence-boundary-detection","title":"Sentence Boundary Detection","text":"<p>The algorithm identifies sentence boundaries through a multi-step process:</p> <ol> <li>Punctuation Detection: Scan for sentence-ending punctuation (<code>.</code>, <code>?</code>, <code>!</code>)</li> <li>Abbreviation Check: Verify if the period follows a known abbreviation</li> <li>Context Analysis: Examine following characters to confirm boundary</li> </ol>"},{"location":"technical_reports/sent_tokenize/#boundary-decision-rules","title":"Boundary Decision Rules","text":"Condition Is Boundary? Period after abbreviation (e.g., \"Dr.\") No Punctuation followed by uppercase letter Yes Punctuation followed by newline Yes Punctuation at end of text Yes Single letter followed by period No (treated as abbreviation)"},{"location":"technical_reports/sent_tokenize/#abbreviation-handling","title":"Abbreviation Handling","text":"<p>The tokenizer maintains a comprehensive abbreviation dictionary:</p> <p>Built-in Categories: - Single letters (A-Z, a-z) - Vietnamese abbreviations (TP, TS, ThS, BS, PGS, GS, etc.) - English abbreviations (Dr, Mr, Mrs, Prof, Inc, etc.) - Domain-specific terms (e.g., \".hcm\", \".hn\", \"g.m.t\")</p> <p>Pre-trained Abbreviations: 368 entries extracted from Vietnamese text corpora</p>"},{"location":"technical_reports/sent_tokenize/#data-format","title":"Data Format","text":""},{"location":"technical_reports/sent_tokenize/#punkt_paramsjson","title":"punkt_params.json","text":"<pre><code>{\n  \"abbrev_types\": [\n    \".hcm\",\n    \".hn\",\n    \"tp\",\n    \"ts\",\n    \"ths\",\n    ...\n  ],\n  \"sent_starters\": []\n}\n</code></pre> Field Type Description abbrev_types List[str] Known abbreviations that don't end sentences sent_starters List[str] Words that typically start sentences (unused)"},{"location":"technical_reports/sent_tokenize/#usage","title":"Usage","text":""},{"location":"technical_reports/sent_tokenize/#basic-usage","title":"Basic Usage","text":"<pre><code>from underthesea import sent_tokenize\n\ntext = \"Xin ch\u00e0o. T\u00f4i l\u00e0 sinh vi\u00ean.\"\nsentences = sent_tokenize(text)\n# Output: ['Xin ch\u00e0o.', 'T\u00f4i l\u00e0 sinh vi\u00ean.']\n</code></pre>"},{"location":"technical_reports/sent_tokenize/#edge-cases","title":"Edge Cases","text":"<pre><code># Empty text\nsent_tokenize(\"\")  # Returns: []\n\n# Single sentence without punctuation\nsent_tokenize(\"h\u00f4m nay\")  # Returns: ['h\u00f4m nay']\n\n# Abbreviations\nsent_tokenize(\"\u00d4ng Dr. Nguy\u1ec5n \u0111\u00e3 \u0111\u1ebfn.\")  # Returns: ['\u00d4ng Dr. Nguy\u1ec5n \u0111\u00e3 \u0111\u1ebfn.']\n\n# Multiple punctuation\nsent_tokenize(\"Th\u1eadt sao?! Tuy\u1ec7t v\u1eddi!\")  # Returns: ['Th\u1eadt sao?!', 'Tuy\u1ec7t v\u1eddi!']\n</code></pre>"},{"location":"technical_reports/sent_tokenize/#performance","title":"Performance","text":""},{"location":"technical_reports/sent_tokenize/#complexity","title":"Complexity","text":"Operation Time Complexity Space Complexity Tokenization O(n) O(n) Model Loading O(a) O(a) <p>Where: - n = length of input text - a = number of abbreviations (~400)</p>"},{"location":"technical_reports/sent_tokenize/#benchmarks","title":"Benchmarks","text":"Text Length Time (ms) 100 chars &lt;0.1 1,000 chars ~0.5 10,000 chars ~5 <p>Benchmarks run on Apple M1, Python 3.11</p>"},{"location":"technical_reports/sent_tokenize/#limitations","title":"Limitations","text":"<ol> <li>No Probabilistic Model: Unlike NLTK's Punkt, this implementation uses a fixed abbreviation dictionary rather than a probabilistic model trained on text</li> <li>No Sentence Starters: The <code>sent_starters</code> parameter is preserved in JSON but not used in boundary detection</li> <li>Limited Ellipsis Handling: Sequences like \"...\" are treated as single boundary markers</li> </ol>"},{"location":"technical_reports/sent_tokenize/#future-improvements","title":"Future Improvements","text":"<ul> <li>[ ] Train a Vietnamese-specific abbreviation model</li> <li>[ ] Add support for quoted speech boundary detection</li> <li>[ ] Handle numeric expressions (e.g., \"1.000.000 \u0111\u1ed3ng\")</li> <li>[ ] Add streaming/generator API for large texts</li> </ul>"},{"location":"technical_reports/sent_tokenize/#references","title":"References","text":"<ol> <li>Kiss, T., &amp; Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525.</li> <li>NLTK Punkt Tokenizer</li> <li>Underthesea GitHub Repository</li> </ol>"},{"location":"technical_reports/sent_tokenize/#changelog","title":"Changelog","text":""},{"location":"technical_reports/sent_tokenize/#version-914-pr-879","title":"Version 9.1.4 (PR #879)","text":"<ul> <li>Removed NLTK dependency</li> <li>Implemented custom Punkt-style sentence tokenizer</li> <li>Converted model from pickle to JSON format</li> <li>Removed unused Tree/TreeSentence classes from conll.py</li> </ul>"},{"location":"technical_reports/voice/","title":"Voice Module: Technical Report","text":"<p>This document provides a technical overview of the AI models used in the underthesea voice (text-to-speech) module.</p>"},{"location":"technical_reports/voice/#overview","title":"Overview","text":"<p>The voice module implements a neural text-to-speech (TTS) system for Vietnamese. It is based on VietTTS by NTT123 and uses a two-stage architecture:</p> <ol> <li>Text-to-Mel: Converts text/phonemes to mel-spectrogram</li> <li>Mel-to-Wave (Vocoder): Converts mel-spectrogram to audio waveform</li> </ol> <pre><code>Text \u2192 [Text Normalization] \u2192 [Duration Model] \u2192 [Acoustic Model] \u2192 Mel \u2192 [HiFi-GAN] \u2192 Audio\n</code></pre>"},{"location":"technical_reports/voice/#installation","title":"Installation","text":"<pre><code>pip install \"underthesea[voice]\"\nunderthesea download-model VIET_TTS_V0_4_1\n</code></pre>"},{"location":"technical_reports/voice/#model-architecture","title":"Model Architecture","text":""},{"location":"technical_reports/voice/#1-duration-model","title":"1. Duration Model","text":"<p>The Duration Model predicts the duration (in seconds) for each phoneme in the input sequence.</p> <p>Architecture:</p> Component Description Token Encoder Embedding + 3\u00d7 Conv1D + Bidirectional LSTM Projection Linear \u2192 GELU \u2192 Linear \u2192 Softplus <p>Parameters:</p> Parameter Value Vocabulary Size 256 LSTM Dimension 256 Dropout Rate 0.5 <p>Input: Phoneme sequence with lengths Output: Duration for each phoneme (in seconds)</p>"},{"location":"technical_reports/voice/#2-acoustic-model","title":"2. Acoustic Model","text":"<p>The Acoustic Model generates mel-spectrograms from phonemes and their predicted durations.</p> <p>Architecture:</p> Component Description Token Encoder Embedding + 3\u00d7 Conv1D + Bidirectional LSTM Upsampling Gaussian attention-based upsampling PreNet 2\u00d7 Linear (256 dim) with dropout Decoder 2\u00d7 LSTM with skip connections PostNet 5\u00d7 Conv1D with batch normalization Projection Linear to mel dimension <p>Parameters:</p> Parameter Value Encoder Dimension 256 Decoder Dimension 512 PostNet Dimension 512 Mel Dimension 80 <p>Key Features:</p> <ul> <li>Gaussian Upsampling: Uses soft attention to upsample encoder outputs to match target frame length</li> <li>Autoregressive Decoder: Generates mel frames sequentially with teacher forcing during training</li> <li>Zoneout Regularization: Applies zoneout to LSTM states during training for better generalization</li> <li>PostNet Refinement: Residual convolutional network refines the predicted mel-spectrogram</li> </ul>"},{"location":"technical_reports/voice/#3-hifi-gan-vocoder","title":"3. HiFi-GAN Vocoder","text":"<p>The vocoder converts mel-spectrograms to raw audio waveforms using the HiFi-GAN architecture.</p> <p>Architecture:</p> Component Description Conv Pre Conv1D (7 kernel) Upsampling Multiple Conv1DTranspose layers Multi-Receptive Field Fusion (MRF) ResBlocks with varying kernel sizes and dilations Conv Post Conv1D (7 kernel) + Tanh <p>Key Features:</p> <ul> <li>Multi-Scale Upsampling: Progressive upsampling from mel frame rate to audio sample rate</li> <li>Multi-Receptive Field Fusion: Combines outputs from residual blocks with different receptive fields</li> <li>Leaky ReLU Activation: Uses leaky ReLU with slope 0.1 throughout</li> </ul> <p>ResBlock Types:</p> <ul> <li>ResBlock1: 3 dilated convolutions (dilation: 1, 3, 5) with residual connections</li> <li>ResBlock2: 2 dilated convolutions (dilation: 1, 3) with residual connections</li> </ul>"},{"location":"technical_reports/voice/#audio-configuration","title":"Audio Configuration","text":"Parameter Value Sample Rate 16,000 Hz FFT Size 1,024 Mel Channels 80 Frequency Range 0 - 8,000 Hz"},{"location":"technical_reports/voice/#text-processing-pipeline","title":"Text Processing Pipeline","text":""},{"location":"technical_reports/voice/#1-text-normalization","title":"1. Text Normalization","text":"<p>The input text is normalized before synthesis:</p> <pre><code># Normalization steps:\n1. Unicode NFKC normalization\n2. Lowercase conversion\n3. Punctuation \u2192 silence markers\n4. Multiple spaces \u2192 single space\n</code></pre>"},{"location":"technical_reports/voice/#2-phoneme-conversion","title":"2. Phoneme Conversion","text":"<p>Text is converted to phonemes using a lexicon lookup:</p> <ul> <li>Vietnamese characters are mapped to phoneme sequences</li> <li>Special tokens: <code>sil</code> (silence), <code>sp</code> (short pause), <code></code> (word boundary)</li> <li>Unknown words are processed character-by-character</li> </ul>"},{"location":"technical_reports/voice/#model-files","title":"Model Files","text":"<p>The <code>VIET_TTS_V0_4_1</code> model package includes:</p> File Description <code>lexicon.txt</code> Word-to-phoneme mapping <code>duration_latest_ckpt.pickle</code> Duration model weights <code>acoustic_latest_ckpt.pickle</code> Acoustic model weights <code>hk_hifi.pickle</code> HiFi-GAN vocoder weights <code>config.json</code> HiFi-GAN configuration"},{"location":"technical_reports/voice/#framework-dependencies","title":"Framework Dependencies","text":"<p>The voice module uses JAX ecosystem:</p> Library Purpose JAX Numerical computation and automatic differentiation JAXlib JAX backend (CPU/GPU/TPU support) dm-haiku Neural network library for JAX Optax Gradient processing and optimization"},{"location":"technical_reports/voice/#usage-example","title":"Usage Example","text":"<pre><code>from underthesea.pipeline.tts import tts\n\n# Basic usage\ntts(\"Xin ch\u00e0o Vi\u1ec7t Nam\")  # Creates sound.wav\n\n# Custom output file\ntts(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4\", outfile=\"output.wav\")\n\n# With playback\ntts(\"\u0110\u00e2y l\u00e0 m\u1ed9t v\u00ed d\u1ee5\", play=True)\n</code></pre>"},{"location":"technical_reports/voice/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>First Call Latency: Model loading on first call may take several seconds</li> <li>JAX Compilation: JIT compilation occurs on first inference, subsequent calls are faster</li> <li>Text Length: Maximum recommended text length is 500 characters</li> <li>Memory Usage: GPU memory usage depends on input text length</li> </ul>"},{"location":"technical_reports/voice/#limitations","title":"Limitations","text":"<ul> <li>Single Speaker: Current model supports only one voice</li> <li>Vietnamese Only: Designed specifically for Vietnamese language</li> <li>Prosody: Limited control over prosody and emotion</li> <li>Real-time: Not optimized for real-time streaming</li> </ul>"},{"location":"technical_reports/voice/#references","title":"References","text":"<ul> <li>VietTTS - Original implementation by NTT123</li> <li>HiFi-GAN - Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</li> <li>Non-Attentive Tacotron - Robust and Controllable Neural TTS Synthesis</li> <li>dm-haiku - JAX neural network library by DeepMind</li> </ul>"},{"location":"technical_reports/agents/","title":"Agents Module: Technical Report","text":"<p>This document provides a technical overview of the agent module in underthesea, including architecture, tools, and comparison with other popular agent frameworks.</p>"},{"location":"technical_reports/agents/#overview","title":"Overview","text":"<p>The agent module provides conversational AI capabilities with support for:</p> <ol> <li>Simple Agent (<code>agent</code>): Singleton instance for quick conversational AI</li> <li>Custom Agent (<code>Agent</code>): Class-based agent with custom tools support</li> <li>Tool System (<code>Tool</code>): Function-to-tool wrapper with OpenAI function calling</li> <li>LLM Client (<code>LLM</code>): Provider-agnostic LLM client (OpenAI, Azure OpenAI)</li> </ol> <pre><code>User Message \u2192 [Agent] \u2192 [LLM] \u2192 Tool Calls? \u2192 [Tool Execution] \u2192 Response\n                  \u2191                    \u2193\n                  \u2514\u2500\u2500 History \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"technical_reports/agents/#installation","title":"Installation","text":"<pre><code>pip install \"underthesea[agent]\"\n\n# Set API key\nexport OPENAI_API_KEY=\"sk-...\"\n# Or for Azure OpenAI:\nexport AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://xxx.openai.azure.com\"\n</code></pre>"},{"location":"technical_reports/agents/#architecture","title":"Architecture","text":""},{"location":"technical_reports/agents/#module-structure","title":"Module Structure","text":"<pre><code>underthesea/agent/\n\u251c\u2500\u2500 __init__.py          # Exports: agent, Agent, Tool, LLM, default_tools\n\u251c\u2500\u2500 agent.py             # _AgentInstance (singleton) and Agent class\n\u251c\u2500\u2500 llm.py               # LLM client for OpenAI/Azure\n\u251c\u2500\u2500 tools.py             # Tool class for function wrapping\n\u2514\u2500\u2500 default_tools.py     # Pre-built utility tools\n</code></pre>"},{"location":"technical_reports/agents/#component-diagram","title":"Component Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Agent                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   LLM    \u2502  \u2502  Tools   \u2502  \u2502 History  \u2502  \u2502 Instruction \u2502 \u2502\n\u2502  \u2502 (client) \u2502  \u2502  (list)  \u2502  \u2502  (list)  \u2502  \u2502   (str)     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502             \u2502             \u2502                         \u2502\n\u2502       \u25bc             \u25bc             \u25bc                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    __call__()                         \u2502  \u2502\n\u2502  \u2502  1. Add user message to history                       \u2502  \u2502\n\u2502  \u2502  2. If tools: _call_with_tools()                      \u2502  \u2502\n\u2502  \u2502     - Loop: LLM \u2192 Tool calls? \u2192 Execute \u2192 Repeat      \u2502  \u2502\n\u2502  \u2502  3. Else: Simple chat completion                      \u2502  \u2502\n\u2502  \u2502  4. Add assistant response to history                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"technical_reports/agents/#core-components","title":"Core Components","text":""},{"location":"technical_reports/agents/#1-llm-client","title":"1. LLM Client","text":"<p>The <code>LLM</code> class provides a unified interface for OpenAI and Azure OpenAI.</p> <p>Provider Detection:</p> Priority Condition Provider 1 <code>AZURE_OPENAI_API_KEY</code> + <code>AZURE_OPENAI_ENDPOINT</code> set Azure 2 <code>OPENAI_API_KEY</code> set OpenAI 3 Neither set Error <p>Parameters:</p> Parameter Type Default Description <code>provider</code> <code>str</code> Auto-detect <code>\"openai\"</code> or <code>\"azure\"</code> <code>model</code> <code>str</code> <code>gpt-4o-mini</code> Model/deployment name <code>api_key</code> <code>str</code> From env API key <code>azure_endpoint</code> <code>str</code> From env Azure endpoint URL <code>azure_api_version</code> <code>str</code> <code>2024-02-01</code> Azure API version"},{"location":"technical_reports/agents/#2-tool-class","title":"2. Tool Class","text":"<p>The <code>Tool</code> class wraps Python functions for OpenAI function calling.</p> <p>Automatic Schema Extraction:</p> <pre><code>def get_weather(location: str, unit: str = \"celsius\") -&gt; dict:\n    \"\"\"Get weather for a location.\"\"\"\n    return {\"temp\": 25, \"unit\": unit}\n\ntool = Tool(get_weather)\n# Extracts:\n# - name: \"get_weather\"\n# - description: \"Get weather for a location.\"\n# - parameters: {\"location\": {\"type\": \"string\", \"required\": true},\n#                \"unit\": {\"type\": \"string\", \"required\": false}}\n</code></pre> <p>Type Mapping:</p> Python Type JSON Schema Type <code>str</code> <code>string</code> <code>int</code> <code>integer</code> <code>float</code> <code>number</code> <code>bool</code> <code>boolean</code> <code>list</code> <code>array</code>"},{"location":"technical_reports/agents/#3-agent-class","title":"3. Agent Class","text":"<p>The <code>Agent</code> class supports custom tools via OpenAI function calling.</p> <p>Constructor:</p> Parameter Type Default Description <code>name</code> <code>str</code> Required Agent identifier <code>tools</code> <code>list[Tool]</code> <code>[]</code> Available tools <code>instruction</code> <code>str</code> <code>\"You are a helpful assistant.\"</code> System prompt <code>max_iterations</code> <code>int</code> <code>10</code> Max tool calling loops <p>Tool Calling Flow:</p> <pre><code>1. User sends message\n2. Agent builds messages: [system, ...history, user]\n3. Send to LLM with tools\n4. If LLM returns tool_calls:\n   a. Execute each tool\n   b. Add tool results to messages\n   c. Go to step 3 (repeat until no tool calls or max_iterations)\n5. Return final assistant message\n</code></pre>"},{"location":"technical_reports/agents/#default-tools","title":"Default Tools","text":""},{"location":"technical_reports/agents/#tool-collections","title":"Tool Collections","text":"Collection Count Description <code>default_tools</code> 12 All default tools <code>core_tools</code> 4 Safe utilities (no external calls) <code>web_tools</code> 3 Web/network operations <code>system_tools</code> 5 File and system operations"},{"location":"technical_reports/agents/#core-tools","title":"Core Tools","text":"Tool Function Description <code>current_datetime_tool</code> <code>get_current_datetime()</code> Returns date, time, weekday, timestamp <code>calculator_tool</code> <code>calculator(expression)</code> Evaluates math (sqrt, sin, cos, log, pi, e) <code>string_length_tool</code> <code>string_length(text)</code> Counts characters, words, lines <code>json_parse_tool</code> <code>parse_json(json_string)</code> Parses JSON strings"},{"location":"technical_reports/agents/#web-tools","title":"Web Tools","text":"Tool Function Description <code>web_search_tool</code> <code>web_search(query)</code> DuckDuckGo search (no API key) <code>fetch_url_tool</code> <code>fetch_url(url)</code> Fetch URL content <code>wikipedia_tool</code> <code>wikipedia(query, lang)</code> Wikipedia search (vi/en)"},{"location":"technical_reports/agents/#system-tools","title":"System Tools","text":"Tool Function Description <code>read_file_tool</code> <code>read_file(path)</code> Read file content <code>write_file_tool</code> <code>write_file(path, content)</code> Write to file <code>list_directory_tool</code> <code>list_directory(path)</code> List files/directories <code>shell_tool</code> <code>run_shell(command)</code> Execute shell command <code>python_tool</code> <code>run_python(code)</code> Execute Python code"},{"location":"technical_reports/agents/#usage-examples","title":"Usage Examples","text":""},{"location":"technical_reports/agents/#simple-agent-singleton","title":"Simple Agent (Singleton)","text":"<pre><code>from underthesea import agent\n\n# Basic conversation\nresponse = agent(\"Xin ch\u00e0o!\")\nprint(response)\n\n# With custom system prompt\nresponse = agent(\"NLP l\u00e0 g\u00ec?\", system_prompt=\"B\u1ea1n l\u00e0 chuy\u00ean gia NLP\")\n\n# Check history\nprint(agent.history)\n\n# Reset conversation\nagent.reset()\n</code></pre>"},{"location":"technical_reports/agents/#custom-agent-with-tools","title":"Custom Agent with Tools","text":"<pre><code>from underthesea.agent import Agent, Tool\n\ndef search_database(query: str) -&gt; list:\n    \"\"\"Search internal database.\"\"\"\n    return [{\"id\": 1, \"title\": f\"Result for {query}\"}]\n\ndef send_email(to: str, subject: str, body: str) -&gt; dict:\n    \"\"\"Send an email.\"\"\"\n    return {\"status\": \"sent\", \"to\": to}\n\nagent = Agent(\n    name=\"assistant\",\n    tools=[\n        Tool(search_database),\n        Tool(send_email, description=\"Send email to user\"),\n    ],\n    instruction=\"You are a helpful office assistant.\"\n)\n\nresponse = agent(\"Find documents about AI and email the results to john@example.com\")\n</code></pre>"},{"location":"technical_reports/agents/#using-default-tools","title":"Using Default Tools","text":"<pre><code>from underthesea.agent import Agent, default_tools, core_tools, web_tools\n\n# All tools\nfull_agent = Agent(name=\"full\", tools=default_tools)\n\n# Safe agent (no system access)\nsafe_agent = Agent(name=\"safe\", tools=core_tools)\n\n# Web-enabled agent\nweb_agent = Agent(name=\"web\", tools=core_tools + web_tools)\n\n# Use agent\nresponse = full_agent(\"What time is it and what's the weather in Hanoi?\")\n</code></pre>"},{"location":"technical_reports/agents/#direct-tool-usage","title":"Direct Tool Usage","text":"<pre><code>from underthesea.agent import calculator_tool, wikipedia_tool\n\n# Use tools without LLM\nresult = calculator_tool(expression=\"sqrt(144) + 2**10\")\nprint(result)  # {'expression': '...', 'result': 1036.0}\n\nwiki = wikipedia_tool(query=\"H\u00e0 N\u1ed9i\", lang=\"vi\")\nprint(wiki[\"summary\"])\n</code></pre>"},{"location":"technical_reports/agents/#comparison-with-other-frameworks","title":"Comparison with Other Frameworks","text":""},{"location":"technical_reports/agents/#feature-comparison","title":"Feature Comparison","text":"Feature underthesea LangChain OpenAI SDK CrewAI Phidata Setup Complexity Low Medium Low Medium Low Tool Definition Function + decorator Class-based Function Class-based Toolkit Multi-agent Manual LangGraph Built-in Built-in Built-in Memory In-memory Multiple Session Built-in Built-in MCP Support No Yes Yes No Yes Providers OpenAI, Azure 100+ OpenAI OpenAI+ 50+"},{"location":"technical_reports/agents/#tool-count-comparison","title":"Tool Count Comparison","text":"Framework Built-in Tools Tool Categories underthesea 12 Core, Web, System LangChain 50+ Search, Code, API, DB OpenAI SDK 5 hosted + local Search, File, Code, Computer CrewAI 30+ File, Web, Search, Document Phidata/Agno 45+ Search, Finance, News, DB, Media smolagents 10 Search, Web, Code, Speech Pydantic AI 7 Search, Code, Image, Memory"},{"location":"technical_reports/agents/#design-philosophy","title":"Design Philosophy","text":"Framework Philosophy underthesea Simple, Vietnamese NLP focused, minimal dependencies LangChain Comprehensive, composable chains, large ecosystem OpenAI SDK Official, production-ready, OpenAI optimized CrewAI Role-based multi-agent collaboration Phidata Performance-focused, 45+ pre-built toolkits smolagents Lightweight, HuggingFace integration"},{"location":"technical_reports/agents/#performance-considerations","title":"Performance Considerations","text":""},{"location":"technical_reports/agents/#latency-sources","title":"Latency Sources","text":"Source Typical Time Notes LLM API call 500ms - 5s Depends on model and prompt length Tool execution Variable Depends on tool (web search ~1s) First call +2-5s Client initialization"},{"location":"technical_reports/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Reuse Agent Instances: Avoid creating new agents per request</li> <li>Limit Tools: Only include necessary tools (reduces prompt size)</li> <li>Set max_iterations: Prevent infinite tool loops</li> <li>Use core_tools for Safety: Avoid system tools in production</li> </ol>"},{"location":"technical_reports/agents/#memory-usage","title":"Memory Usage","text":"Component Memory Agent instance ~1KB LLM client ~5KB Per tool ~500B History (per message) ~1KB"},{"location":"technical_reports/agents/#security-considerations","title":"Security Considerations","text":""},{"location":"technical_reports/agents/#tool-safety-levels","title":"Tool Safety Levels","text":"Level Tools Risk Safe <code>core_tools</code> No external access Network <code>web_tools</code> HTTP requests only System <code>system_tools</code> File/shell access"},{"location":"technical_reports/agents/#recommendations","title":"Recommendations","text":"<ol> <li>Production: Use only <code>core_tools</code> unless necessary</li> <li>Shell Tool: Blocks dangerous commands (rm -rf, mkfs, etc.)</li> <li>Python Tool: Runs in restricted globals</li> <li>File Tools: Validate paths before use</li> </ol>"},{"location":"technical_reports/agents/#blocked-shell-commands","title":"Blocked Shell Commands","text":"<pre><code>dangerous = [\"rm -rf\", \"mkfs\", \"dd if=\", \":(){\", \"fork bomb\", \"&gt; /dev/\"]\n</code></pre>"},{"location":"technical_reports/agents/#api-reference","title":"API Reference","text":""},{"location":"technical_reports/agents/#module-exports","title":"Module Exports","text":"<pre><code>from underthesea.agent import (\n    # Core\n    agent,              # Singleton agent instance\n    Agent,              # Agent class with tools\n    LLM,                # LLM client\n    Tool,               # Function-to-tool wrapper\n\n    # Tool collections\n    default_tools,      # All 12 tools\n    core_tools,         # 4 safe tools\n    web_tools,          # 3 web tools\n    system_tools,       # 5 system tools\n\n    # Individual tools\n    current_datetime_tool,\n    calculator_tool,\n    string_length_tool,\n    json_parse_tool,\n    web_search_tool,\n    fetch_url_tool,\n    wikipedia_tool,\n    read_file_tool,\n    write_file_tool,\n    list_directory_tool,\n    shell_tool,\n    python_tool,\n)\n</code></pre>"},{"location":"technical_reports/agents/#testing","title":"Testing","text":"<pre><code># Run all agent tests\nuv run python -m unittest discover tests.agent\n\n# Run specific test modules\nuv run python -m unittest tests.agent.test_agent\nuv run python -m unittest tests.agent.test_tools\nuv run python -m unittest tests.agent.test_llm\n\n# Lint\nruff check underthesea/agent/\n</code></pre>"},{"location":"technical_reports/agents/#changelog","title":"Changelog","text":""},{"location":"technical_reports/agents/#unreleased","title":"Unreleased","text":"<ul> <li>Add <code>Agent</code> class with custom tools support (GH-712)</li> <li>Add <code>Tool</code> class for function wrapping</li> <li>Add 12 default tools: calculator, datetime, web_search, wikipedia, shell, python, file operations</li> </ul>"},{"location":"technical_reports/agents/#v915-2026-01-29","title":"v9.1.5 (2026-01-29)","text":"<ul> <li>Add <code>agent</code> singleton and <code>LLM</code> client (GH-745)</li> <li>Support OpenAI and Azure OpenAI providers</li> </ul>"},{"location":"technical_reports/agents/#references","title":"References","text":""},{"location":"technical_reports/agents/#openai-function-calling","title":"OpenAI Function Calling","text":"<ul> <li>OpenAI Function Calling Guide</li> <li>OpenAI Tools API Reference</li> </ul>"},{"location":"technical_reports/agents/#popular-agent-frameworks","title":"Popular Agent Frameworks","text":"<ul> <li>LangChain - Comprehensive AI application framework</li> <li>OpenAI Agents SDK - Official OpenAI agent framework</li> <li>CrewAI - Role-based multi-agent framework</li> <li>Phidata/Agno - High-performance agent framework</li> <li>smolagents - Lightweight HuggingFace agents</li> <li>Pydantic AI - Type-safe agent framework</li> <li>Google ADK - Google's Agent Development Kit</li> <li>AWS Strands - AWS agent SDK</li> </ul>"},{"location":"technical_reports/agents/#related-documentation","title":"Related Documentation","text":"<ul> <li>underthesea Agent API</li> <li>Issue GH-712: Agents with LLMs</li> </ul>"},{"location":"technical_reports/agents/comparison/","title":"Agent Frameworks Comparison (2025)","text":"<p>This document provides a comprehensive comparison of popular AI agent frameworks and SDKs as of 2025.</p>"},{"location":"technical_reports/agents/comparison/#framework-overview","title":"Framework Overview","text":"Framework Organization GitHub Stars Language License LangChain LangChain 100k+ Python, JS MIT LangGraph LangChain 14k+ Python, JS MIT OpenAI Agents SDK OpenAI 11k+ Python, TS MIT AutoGen Microsoft 45k+ Python, .NET MIT Semantic Kernel Microsoft 25k+ Python, C#, Java MIT CrewAI CrewAI 25k+ Python MIT Phidata/Agno Phidata 18k+ Python MIT smolagents HuggingFace 15k+ Python Apache 2.0 Pydantic AI Pydantic 10k+ Python MIT Google ADK Google 8k+ Python, TS, Java Apache 2.0 LlamaIndex LlamaIndex 40k+ Python, TS MIT Haystack deepset 18k+ Python Apache 2.0 AWS Strands AWS 3k+ Python, TS Apache 2.0 Claude Agent SDK Anthropic New Python, TS MIT IBM BeeAI IBM/Linux Foundation 3k+ Python, TS Apache 2.0 Letta/MemGPT Letta 12k+ Python Apache 2.0 CAMEL-AI CAMEL-AI 6k+ Python Apache 2.0 Dify Dify 60k+ Python, TS Apache 2.0 Flowise Flowise (Workday) 35k+ JS/TS Apache 2.0 Langflow Langflow 40k+ Python MIT"},{"location":"technical_reports/agents/comparison/#built-in-tools-comparison","title":"Built-in Tools Comparison","text":""},{"location":"technical_reports/agents/comparison/#openai-agents-sdk","title":"OpenAI Agents SDK","text":"Category Tools Hosted <code>WebSearchTool</code>, <code>FileSearchTool</code>, <code>CodeInterpreterTool</code>, <code>ImageGenerationTool</code>, <code>HostedMCPTool</code> Local <code>ComputerTool</code>, <code>ShellTool</code>, <code>ApplyPatchTool</code>, <code>FunctionTool</code>"},{"location":"technical_reports/agents/comparison/#huggingface-smolagents","title":"HuggingFace smolagents","text":"Category Tools Search <code>DuckDuckGoSearchTool</code>, <code>GoogleSearchTool</code>, <code>ApiWebSearchTool</code>, <code>WebSearchTool</code>, <code>WikipediaSearchTool</code> Web <code>VisitWebpageTool</code> Code <code>PythonInterpreterTool</code> User <code>UserInputTool</code>, <code>FinalAnswerTool</code> Audio <code>SpeechToTextTool</code>"},{"location":"technical_reports/agents/comparison/#pydantic-ai","title":"Pydantic AI","text":"Tool Description <code>WebSearchTool</code> Search the web <code>WebFetchTool</code> Fetch web pages <code>CodeExecutionTool</code> Execute code in sandbox <code>ImageGenerationTool</code> Generate images <code>FileSearchTool</code> RAG/vector search <code>MemoryTool</code> Persistent memory <code>MCPServerTool</code> MCP integration"},{"location":"technical_reports/agents/comparison/#phidataagno-45-toolkits","title":"Phidata/Agno (45+ Toolkits)","text":"Category Tools Search DuckDuckGo, GoogleSearch, Exa, SearxNG, Serpapi, Tavily Finance YFinanceTools, OpenBB News Newspaper4k, HackerNews, Arxiv, Pubmed Database DuckDb, Postgres, SQL Code Python, Shell, Calculator Files File, CSV, Pandas Web Apify, Crawl4AI, Firecrawl, Spider, Website, JinaReader Media Dalle, YouTube, MLXTranscribe, ModelsLabs Communication Email, Slack, Resend Services GitHub, Jira, Twitter, Zendesk, CalCom, Wikipedia"},{"location":"technical_reports/agents/comparison/#crewai-30-tools","title":"CrewAI (30+ Tools)","text":"Category Tools File FileReadTool, DirectoryReadTool, DirectorySearchTool Web ScrapeWebsiteTool, WebsiteSearchTool, FirecrawlCrawlWebsiteTool Search SerperDevTool, EXASearchTool, BraveSearchTool Documents PDFSearchTool, DOCXSearchTool, TXTSearchTool, MDXSearchTool Data CSVSearchTool, JSONSearchTool, XMLSearchTool Code CodeInterpreterTool, CodeDocsSearchTool, GithubSearchTool Database PGSearchTool Media YoutubeVideoSearchTool, YoutubeChannelSearchTool, DALL-E Tool"},{"location":"technical_reports/agents/comparison/#langchain","title":"LangChain","text":"Category Tools Search DuckDuckGoSearchRun, GoogleSearchRun, BingSearchRun, WikipediaQueryRun, ArxivQueryRun Code PythonREPL, ShellTool Math LLMMathChain HTTP RequestsGetTool, RequestsPostTool APIs OpenWeatherMapQueryRun, NewsAPITool"},{"location":"technical_reports/agents/comparison/#underthesea-12-tools","title":"underthesea (12 Tools)","text":"Category Tools Core current_datetime_tool, calculator_tool, string_length_tool, json_parse_tool Web web_search_tool, fetch_url_tool, wikipedia_tool System read_file_tool, write_file_tool, list_directory_tool, shell_tool, python_tool"},{"location":"technical_reports/agents/comparison/#feature-matrix","title":"Feature Matrix","text":"Feature underthesea LangChain OpenAI SDK CrewAI Phidata smolagents Simple API \u2705 \u26a0\ufe0f \u2705 \u26a0\ufe0f \u2705 \u2705 Multi-agent \u274c \u2705 \u2705 \u2705 \u2705 \u2705 MCP Support \u274c \u2705 \u2705 \u274c \u2705 \u2705 Memory In-memory Multiple Session Built-in Built-in \u274c Streaming \u274c \u2705 \u2705 \u2705 \u2705 \u2705 Async \u274c \u2705 \u2705 \u2705 \u2705 \u2705 Type Safety \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f Visual Builder \u274c Flowise \u274c \u274c \u274c \u274c Tracing \u274c LangSmith Built-in \u274c Built-in \u274c Human-in-loop \u274c \u2705 \u2705 \u2705 \u2705 \u2705 <p>Legend: \u2705 Full support | \u26a0\ufe0f Partial | \u274c Not supported</p>"},{"location":"technical_reports/agents/comparison/#provider-support","title":"Provider Support","text":"Framework OpenAI Azure Anthropic Google AWS Bedrock Local/Ollama underthesea \u2705 \u2705 \u274c \u274c \u274c \u274c LangChain \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 OpenAI SDK \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CrewAI \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Phidata \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 smolagents \u2705 \u274c \u2705 \u2705 \u274c \u2705 Pydantic AI \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Google ADK \u2705 \u274c \u2705 \u2705 \u274c \u2705"},{"location":"technical_reports/agents/comparison/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"technical_reports/agents/comparison/#simple-chatbot-qa","title":"Simple Chatbot / Q&amp;A","text":"Recommendation Frameworks Best underthesea, Pydantic AI, smolagents Good OpenAI SDK, Phidata Overkill LangChain, CrewAI, AutoGen"},{"location":"technical_reports/agents/comparison/#multi-agent-collaboration","title":"Multi-Agent Collaboration","text":"Recommendation Frameworks Best CrewAI, AutoGen, CAMEL-AI Good LangGraph, OpenAI SDK, Phidata Limited underthesea, smolagents"},{"location":"technical_reports/agents/comparison/#rag-document-qa","title":"RAG / Document Q&amp;A","text":"Recommendation Frameworks Best LlamaIndex, LangChain, Haystack Good CrewAI (with tools), Dify Limited underthesea, smolagents"},{"location":"technical_reports/agents/comparison/#code-generation-automation","title":"Code Generation / Automation","text":"Recommendation Frameworks Best Claude Agent SDK, OpenAI SDK Good LangChain, AutoGen Limited underthesea"},{"location":"technical_reports/agents/comparison/#enterprise-production","title":"Enterprise / Production","text":"Recommendation Frameworks Best Semantic Kernel, AWS Strands, IBM BeeAI Good LangChain + LangSmith, OpenAI SDK Prototype underthesea, Phidata, smolagents"},{"location":"technical_reports/agents/comparison/#visual-no-code","title":"Visual / No-Code","text":"Recommendation Frameworks Best Flowise, Dify, Langflow, n8n Code-first All others"},{"location":"technical_reports/agents/comparison/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"technical_reports/agents/comparison/#gaia-benchmark-general-ai-assistants","title":"GAIA Benchmark (General AI Assistants)","text":"Framework Score Notes CAMEL-AI OWL 58.18% #1 on leaderboard AutoGen ~55% Multi-agent OpenAI SDK ~50% With tools"},{"location":"technical_reports/agents/comparison/#agent-instantiation-speed","title":"Agent Instantiation Speed","text":"Framework Time Notes Phidata/Agno 2\u03bcs Claims 5000x faster than LangGraph smolagents ~10\u03bcs Lightweight LangGraph ~10ms Full features"},{"location":"technical_reports/agents/comparison/#memory-efficiency","title":"Memory Efficiency","text":"Framework Memory Notes Phidata/Agno Low Claims 50x more efficient smolagents Low Minimal dependencies LangChain High Large ecosystem"},{"location":"technical_reports/agents/comparison/#protocol-support","title":"Protocol Support","text":""},{"location":"technical_reports/agents/comparison/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"Framework MCP Client MCP Server Notes OpenAI SDK \u2705 \u274c HostedMCPTool LangChain \u2705 \u2705 Full support Pydantic AI \u2705 \u274c MCPServerTool Google ADK \u2705 \u2705 Native support Dify \u2705 \u2705 MCP Apps Langflow \u2705 \u2705 v1.7+ smolagents \u2705 \u274c Latest specs underthesea \u274c \u274c Not supported"},{"location":"technical_reports/agents/comparison/#a2a-agent-to-agent-protocol","title":"A2A (Agent-to-Agent Protocol)","text":"Framework Support Notes IBM BeeAI \u2705 ACP merged with A2A AWS Strands \u2705 Built-in Google ADK \u2705 Native Others \u274c Not yet"},{"location":"technical_reports/agents/comparison/#conclusion","title":"Conclusion","text":""},{"location":"technical_reports/agents/comparison/#when-to-use-underthesea-agent","title":"When to Use underthesea Agent","text":"<p>Good for: - Simple Vietnamese NLP chatbots - Quick prototyping - Minimal dependencies - OpenAI/Azure OpenAI only</p> <p>Not ideal for: - Multi-agent systems - Production at scale - Complex workflows - Multiple LLM providers</p>"},{"location":"technical_reports/agents/comparison/#migration-path","title":"Migration Path","text":"<p>If you outgrow underthesea agents:</p> <ol> <li>More providers \u2192 Pydantic AI, Phidata</li> <li>Multi-agent \u2192 CrewAI, AutoGen</li> <li>Production \u2192 LangChain + LangSmith, AWS Strands</li> <li>Visual \u2192 Flowise, Dify, Langflow</li> </ol>"},{"location":"technical_reports/agents/tools/","title":"Default Tools Reference","text":"<p>This document provides detailed documentation for all built-in tools in the underthesea agent module.</p>"},{"location":"technical_reports/agents/tools/#tool-collections","title":"Tool Collections","text":"<pre><code>from underthesea.agent import (\n    default_tools,   # All 12 tools\n    core_tools,      # 4 safe tools\n    web_tools,       # 3 web tools\n    system_tools,    # 5 system tools\n)\n</code></pre> Collection Tools Safety Level <code>core_tools</code> 4 Safe - No external access <code>web_tools</code> 3 Network - HTTP requests <code>system_tools</code> 5 System - File/shell access <code>default_tools</code> 12 All of the above"},{"location":"technical_reports/agents/tools/#core-tools","title":"Core Tools","text":""},{"location":"technical_reports/agents/tools/#current_datetime_tool","title":"current_datetime_tool","text":"<p>Get the current date, time, and weekday.</p> <p>Function: <code>get_current_datetime() -&gt; dict</code></p> <p>Parameters: None</p> <p>Returns: <pre><code>{\n    \"datetime\": \"2025-01-30T10:30:00.123456\",  # ISO format\n    \"date\": \"2025-01-30\",                       # YYYY-MM-DD\n    \"time\": \"10:30:00\",                         # HH:MM:SS\n    \"weekday\": \"Thursday\",                      # Day name\n    \"timestamp\": 1738236600                     # Unix timestamp\n}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import current_datetime_tool\n\nresult = current_datetime_tool()\nprint(f\"Today is {result['weekday']}, {result['date']}\")\n</code></pre></p>"},{"location":"technical_reports/agents/tools/#calculator_tool","title":"calculator_tool","text":"<p>Evaluate mathematical expressions safely.</p> <p>Function: <code>calculator(expression: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>expression</code> | <code>str</code> | Yes | Math expression to evaluate |</p> <p>Supported Operations: | Category | Operations | |----------|------------| | Arithmetic | <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>**</code> (power), <code>%</code> (modulo) | | Functions | <code>sqrt</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>log</code>, <code>log10</code>, <code>exp</code> | | Rounding | <code>abs</code>, <code>round</code>, <code>floor</code>, <code>ceil</code> | | Aggregation | <code>min</code>, <code>max</code>, <code>sum</code>, <code>pow</code> | | Constants | <code>pi</code> (3.14159...), <code>e</code> (2.71828...) |</p> <p>Returns: <pre><code># Success\n{\"expression\": \"sqrt(16) + 2\", \"result\": 6.0}\n\n# Error\n{\"expression\": \"invalid\", \"error\": \"name 'invalid' is not defined\"}\n</code></pre></p> <p>Examples: <pre><code>from underthesea.agent import calculator_tool\n\n# Basic arithmetic\ncalculator_tool(expression=\"2 + 3 * 4\")  # result: 14\n\n# Functions\ncalculator_tool(expression=\"sqrt(144)\")  # result: 12.0\ncalculator_tool(expression=\"sin(pi/2)\")  # result: 1.0\n\n# Complex expressions\ncalculator_tool(expression=\"log(e**2)\")  # result: 2.0\ncalculator_tool(expression=\"round(pi, 2)\")  # result: 3.14\n</code></pre></p> <p>Security: Uses restricted <code>eval</code> with only safe math functions allowed.</p>"},{"location":"technical_reports/agents/tools/#string_length_tool","title":"string_length_tool","text":"<p>Count characters, words, and lines in text.</p> <p>Function: <code>string_length(text: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>text</code> | <code>str</code> | Yes | Text to analyze |</p> <p>Returns: <pre><code>{\n    \"text\": \"Hello World...\",  # First 100 chars (truncated if longer)\n    \"characters\": 11,           # Total character count\n    \"words\": 2,                 # Word count (split by whitespace)\n    \"lines\": 1                  # Line count (newlines + 1)\n}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import string_length_tool\n\nresult = string_length_tool(text=\"Hello World\\nThis is a test\")\n# {'text': 'Hello World\\nThis is a test', 'characters': 27, 'words': 5, 'lines': 2}\n</code></pre></p>"},{"location":"technical_reports/agents/tools/#json_parse_tool","title":"json_parse_tool","text":"<p>Parse a JSON string into a Python object.</p> <p>Function: <code>parse_json(json_string: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>json_string</code> | <code>str</code> | Yes | JSON string to parse |</p> <p>Returns: <pre><code># Success\n{\"success\": True, \"data\": &lt;parsed object&gt;}\n\n# Error\n{\"success\": False, \"error\": \"Expecting property name: line 1 column 2\"}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import json_parse_tool\n\n# Valid JSON\nresult = json_parse_tool(json_string='{\"name\": \"test\", \"value\": 123}')\n# {'success': True, 'data': {'name': 'test', 'value': 123}}\n\n# Invalid JSON\nresult = json_parse_tool(json_string='not json')\n# {'success': False, 'error': '...'}\n</code></pre></p>"},{"location":"technical_reports/agents/tools/#web-tools","title":"Web Tools","text":""},{"location":"technical_reports/agents/tools/#web_search_tool","title":"web_search_tool","text":"<p>Search the web using DuckDuckGo (no API key required).</p> <p>Function: <code>web_search(query: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>query</code> | <code>str</code> | Yes | Search query |</p> <p>Returns: <pre><code>{\n    \"query\": \"python tutorial\",\n    \"results\": [\n        {\"title\": \"Python Tutorial\", \"url\": \"https://...\"},\n        {\"title\": \"Learn Python\", \"url\": \"https://...\"},\n        # Up to 5 results\n    ]\n}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import web_search_tool\n\nresult = web_search_tool(query=\"Vietnamese NLP\")\nfor r in result[\"results\"]:\n    print(f\"{r['title']}: {r['url']}\")\n</code></pre></p> <p>Notes: - Uses DuckDuckGo HTML search - Returns up to 5 results - 10 second timeout - No API key required</p>"},{"location":"technical_reports/agents/tools/#fetch_url_tool","title":"fetch_url_tool","text":"<p>Fetch and read content from a URL.</p> <p>Function: <code>fetch_url(url: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>url</code> | <code>str</code> | Yes | URL to fetch |</p> <p>Returns: <pre><code># Success\n{\n    \"url\": \"https://example.com\",\n    \"status\": 200,\n    \"content\": \"&lt;html&gt;...\"  # Up to 10,000 chars\n}\n\n# Error\n{\"url\": \"https://...\", \"error\": \"HTTP Error 404: Not Found\"}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import fetch_url_tool\n\nresult = fetch_url_tool(url=\"https://example.com\")\nif \"error\" not in result:\n    print(f\"Status: {result['status']}\")\n    print(f\"Content length: {len(result['content'])}\")\n</code></pre></p> <p>Notes: - 10 second timeout - Content truncated to 10,000 characters - UTF-8 decoding with error handling</p>"},{"location":"technical_reports/agents/tools/#wikipedia_tool","title":"wikipedia_tool","text":"<p>Search Wikipedia and get article summary.</p> <p>Function: <code>wikipedia_search(query: str, lang: str = \"vi\") -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Default | Description | |------|------|----------|---------|-------------| | <code>query</code> | <code>str</code> | Yes | - | Search query / article title | | <code>lang</code> | <code>str</code> | No | <code>\"vi\"</code> | Language code (<code>vi</code>, <code>en</code>, etc.) |</p> <p>Returns: <pre><code># Success\n{\n    \"title\": \"H\u00e0 N\u1ed9i\",\n    \"summary\": \"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a Vi\u1ec7t Nam...\",\n    \"url\": \"https://vi.wikipedia.org/wiki/H\u00e0_N\u1ed9i\"\n}\n\n# Not found\n{\"query\": \"xyz123\", \"error\": \"HTTP Error 404: Not Found\"}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import wikipedia_tool\n\n# Vietnamese Wikipedia\nresult = wikipedia_tool(query=\"H\u00e0 N\u1ed9i\", lang=\"vi\")\nprint(result[\"summary\"])\n\n# English Wikipedia\nresult = wikipedia_tool(query=\"Vietnam\", lang=\"en\")\nprint(result[\"summary\"])\n</code></pre></p> <p>Notes: - Uses Wikipedia REST API - Returns article summary (not full text) - 10 second timeout</p>"},{"location":"technical_reports/agents/tools/#system-tools","title":"System Tools","text":""},{"location":"technical_reports/agents/tools/#read_file_tool","title":"read_file_tool","text":"<p>Read content from a file.</p> <p>Function: <code>read_file(file_path: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>file_path</code> | <code>str</code> | Yes | Path to file |</p> <p>Returns: <pre><code># Success\n{\"file_path\": \"/path/to/file.txt\", \"content\": \"file contents...\"}\n\n# Error\n{\"file_path\": \"/path/to/file.txt\", \"error\": \"No such file or directory\"}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import read_file_tool\n\nresult = read_file_tool(file_path=\"README.md\")\nif \"error\" not in result:\n    print(result[\"content\"])\n</code></pre></p> <p>Notes: - UTF-8 encoding - Content truncated to 10,000 characters</p>"},{"location":"technical_reports/agents/tools/#write_file_tool","title":"write_file_tool","text":"<p>Write content to a file.</p> <p>Function: <code>write_file(file_path: str, content: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>file_path</code> | <code>str</code> | Yes | Path to file | | <code>content</code> | <code>str</code> | Yes | Content to write |</p> <p>Returns: <pre><code># Success\n{\"file_path\": \"/path/to/file.txt\", \"success\": True, \"bytes_written\": 123}\n\n# Error\n{\"file_path\": \"/path/to/file.txt\", \"error\": \"Permission denied\"}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import write_file_tool\n\nresult = write_file_tool(file_path=\"output.txt\", content=\"Hello World\")\nprint(f\"Wrote {result['bytes_written']} bytes\")\n</code></pre></p> <p>Notes: - Creates file if not exists - Overwrites existing content - UTF-8 encoding</p>"},{"location":"technical_reports/agents/tools/#list_directory_tool","title":"list_directory_tool","text":"<p>List files and directories in a path.</p> <p>Function: <code>list_directory(path: str = \".\") -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Default | Description | |------|------|----------|---------|-------------| | <code>path</code> | <code>str</code> | No | <code>\".\"</code> | Directory path |</p> <p>Returns: <pre><code>{\n    \"path\": \"/home/user\",\n    \"directories\": [\"Documents\", \"Downloads\"],  # Sorted\n    \"files\": [\"file.txt\", \"notes.md\"]           # Sorted\n}\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import list_directory_tool\n\nresult = list_directory_tool(path=\".\")\nprint(f\"Directories: {result['directories']}\")\nprint(f\"Files: {result['files']}\")\n</code></pre></p>"},{"location":"technical_reports/agents/tools/#shell_tool","title":"shell_tool","text":"<p>Run a shell command and return the output.</p> <p>Function: <code>run_shell_command(command: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>command</code> | <code>str</code> | Yes | Shell command to execute |</p> <p>Returns: <pre><code># Success\n{\n    \"command\": \"ls -la\",\n    \"output\": \"total 16\\ndrwxr-xr-x...\",\n    \"return_code\": 0\n}\n\n# Error\n{\"command\": \"invalid_cmd\", \"output\": \"command not found\", \"return_code\": 127}\n\n# Blocked\n{\"command\": \"rm -rf /\", \"error\": \"Command blocked for safety\"}\n</code></pre></p> <p>Blocked Commands: <pre><code>[\"rm -rf\", \"mkfs\", \"dd if=\", \":(){\", \"fork bomb\", \"&gt; /dev/\"]\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import shell_tool\n\nresult = shell_tool(command=\"pwd\")\nprint(result[\"output\"])\n\nresult = shell_tool(command=\"git status\")\nprint(result[\"output\"])\n</code></pre></p> <p>Notes: - 30 second timeout - Output truncated to 5,000 characters - Dangerous commands blocked</p>"},{"location":"technical_reports/agents/tools/#python_tool","title":"python_tool","text":"<p>Execute Python code in a restricted environment.</p> <p>Function: <code>run_python(code: str) -&gt; dict</code></p> <p>Parameters: | Name | Type | Required | Description | |------|------|----------|-------------| | <code>code</code> | <code>str</code> | Yes | Python code to execute |</p> <p>Returns: <pre><code># Success\n{\"code\": \"print(2 + 2)\", \"output\": \"4\"}\n\n# Error\n{\"code\": \"import os\", \"error\": \"name 'os' is not defined\"}\n</code></pre></p> <p>Allowed Built-ins: <pre><code>print, len, range, str, int, float, list, dict, set, tuple, bool,\nabs, sum, min, max, round, sorted, reversed, enumerate, zip, map, filter, any, all\n</code></pre></p> <p>Example: <pre><code>from underthesea.agent import python_tool\n\n# Simple calculation\nresult = python_tool(code=\"print(sum(range(10)))\")\n# {'code': '...', 'output': '45'}\n\n# Multiple operations\nresult = python_tool(code=\"\"\"\nfor i in range(3):\n    print(f\"Number: {i}\")\n\"\"\")\n# {'code': '...', 'output': 'Number: 0\\nNumber: 1\\nNumber: 2'}\n</code></pre></p> <p>Notes: - Captures stdout via <code>print()</code> - Restricted globals (no imports, no file access) - Use for safe calculations only</p>"},{"location":"technical_reports/agents/tools/#creating-custom-tools","title":"Creating Custom Tools","text":""},{"location":"technical_reports/agents/tools/#basic-tool","title":"Basic Tool","text":"<pre><code>from underthesea.agent import Agent, Tool\n\ndef my_tool(param1: str, param2: int = 10) -&gt; dict:\n    \"\"\"Tool description shown to the LLM.\"\"\"\n    return {\"result\": f\"{param1} x {param2}\"}\n\ntool = Tool(my_tool)\nagent = Agent(name=\"my_agent\", tools=[tool])\n</code></pre>"},{"location":"technical_reports/agents/tools/#with-custom-namedescription","title":"With Custom Name/Description","text":"<pre><code>tool = Tool(\n    my_tool,\n    name=\"custom_name\",\n    description=\"Custom description for the LLM\"\n)\n</code></pre>"},{"location":"technical_reports/agents/tools/#tool-schema","title":"Tool Schema","text":"<pre><code>tool = Tool(my_tool)\n\n# View OpenAI format\nprint(tool.to_openai_tool())\n# {\n#     \"type\": \"function\",\n#     \"function\": {\n#         \"name\": \"my_tool\",\n#         \"description\": \"Tool description shown to the LLM.\",\n#         \"parameters\": {\n#             \"type\": \"object\",\n#             \"properties\": {\n#                 \"param1\": {\"type\": \"string\", \"description\": \"Parameter param1\"},\n#                 \"param2\": {\"type\": \"integer\", \"description\": \"Parameter param2\"}\n#             },\n#             \"required\": [\"param1\"]\n#         }\n#     }\n# }\n</code></pre>"},{"location":"technical_reports/agents/tools/#direct-execution","title":"Direct Execution","text":"<pre><code># Via Tool object\nresult = tool(param1=\"test\", param2=5)\n\n# Via execute (returns JSON string)\nresult_json = tool.execute({\"param1\": \"test\", \"param2\": 5})\n</code></pre>"}]}