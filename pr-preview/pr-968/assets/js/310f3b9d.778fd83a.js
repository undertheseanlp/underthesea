"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[2629],{3228(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>a,frontMatter:()=>l,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"technical-reports/audio/text-to-speech","title":"Voice","description":"This document provides a technical overview of the AI models used in the underthesea voice (text-to-speech) module.","source":"@site/versioned_docs/version-9.2/technical-reports/audio/text-to-speech.md","sourceDirName":"technical-reports/audio","slug":"/technical-reports/audio/text-to-speech","permalink":"/docs/technical-reports/audio/text-to-speech","draft":false,"unlisted":false,"editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/versioned_docs/version-9.2/technical-reports/audio/text-to-speech.md","tags":[],"version":"9.2","frontMatter":{},"sidebar":"technicalReportsSidebar","previous":{"title":"Machine Translation","permalink":"/docs/technical-reports/translate"},"next":{"title":"Agents","permalink":"/docs/technical-reports/agents/"}}');var r=i(4848),t=i(8453);const l={},d="Voice",c={},o=[{value:"Overview",id:"overview",level:2},{value:"Installation",id:"installation",level:2},{value:"Model Architecture",id:"model-architecture",level:2},{value:"1. Duration Model",id:"1-duration-model",level:3},{value:"2. Acoustic Model",id:"2-acoustic-model",level:3},{value:"3. HiFi-GAN Vocoder",id:"3-hifi-gan-vocoder",level:3},{value:"Audio Configuration",id:"audio-configuration",level:2},{value:"Text Processing Pipeline",id:"text-processing-pipeline",level:2},{value:"1. Text Normalization",id:"1-text-normalization",level:3},{value:"2. Phoneme Conversion",id:"2-phoneme-conversion",level:3},{value:"Model Files",id:"model-files",level:2},{value:"Framework Dependencies",id:"framework-dependencies",level:2},{value:"Usage Example",id:"usage-example",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Limitations",id:"limitations",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"voice",children:"Voice"})}),"\n",(0,r.jsx)(n.p,{children:"This document provides a technical overview of the AI models used in the underthesea voice (text-to-speech) module."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["The voice module implements a neural text-to-speech (TTS) system for Vietnamese. It is based on ",(0,r.jsx)(n.a,{href:"https://github.com/ntt123/vietTTS",children:"VietTTS"})," by NTT123 and uses a two-stage architecture:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text-to-Mel"}),": Converts text/phonemes to mel-spectrogram"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mel-to-Wave (Vocoder)"}),": Converts mel-spectrogram to audio waveform"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Text \u2192 [Text Normalization] \u2192 [Duration Model] \u2192 [Acoustic Model] \u2192 Mel \u2192 [HiFi-GAN] \u2192 Audio\n"})}),"\n",(0,r.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'pip install "underthesea[voice]"\nunderthesea download-model VIET_TTS_V0_4_1\n'})}),"\n",(0,r.jsx)(n.h2,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"1-duration-model",children:"1. Duration Model"}),"\n",(0,r.jsx)(n.p,{children:"The Duration Model predicts the duration (in seconds) for each phoneme in the input sequence."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Token Encoder"}),(0,r.jsx)(n.td,{children:"Embedding + 3\xd7 Conv1D + Bidirectional LSTM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Projection"}),(0,r.jsx)(n.td,{children:"Linear \u2192 GELU \u2192 Linear \u2192 Softplus"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Vocabulary Size"}),(0,r.jsx)(n.td,{children:"256"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LSTM Dimension"}),(0,r.jsx)(n.td,{children:"256"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dropout Rate"}),(0,r.jsx)(n.td,{children:"0.5"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," Phoneme sequence with lengths\n",(0,r.jsx)(n.strong,{children:"Output:"})," Duration for each phoneme (in seconds)"]}),"\n",(0,r.jsx)(n.h3,{id:"2-acoustic-model",children:"2. Acoustic Model"}),"\n",(0,r.jsx)(n.p,{children:"The Acoustic Model generates mel-spectrograms from phonemes and their predicted durations."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Token Encoder"}),(0,r.jsx)(n.td,{children:"Embedding + 3\xd7 Conv1D + Bidirectional LSTM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Upsampling"}),(0,r.jsx)(n.td,{children:"Gaussian attention-based upsampling"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PreNet"}),(0,r.jsx)(n.td,{children:"2\xd7 Linear (256 dim) with dropout"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Decoder"}),(0,r.jsx)(n.td,{children:"2\xd7 LSTM with skip connections"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PostNet"}),(0,r.jsx)(n.td,{children:"5\xd7 Conv1D with batch normalization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Projection"}),(0,r.jsx)(n.td,{children:"Linear to mel dimension"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Encoder Dimension"}),(0,r.jsx)(n.td,{children:"256"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Decoder Dimension"}),(0,r.jsx)(n.td,{children:"512"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PostNet Dimension"}),(0,r.jsx)(n.td,{children:"512"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mel Dimension"}),(0,r.jsx)(n.td,{children:"80"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gaussian Upsampling"}),": Uses soft attention to upsample encoder outputs to match target frame length"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autoregressive Decoder"}),": Generates mel frames sequentially with teacher forcing during training"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Zoneout Regularization"}),": Applies zoneout to LSTM states during training for better generalization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PostNet Refinement"}),": Residual convolutional network refines the predicted mel-spectrogram"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-hifi-gan-vocoder",children:"3. HiFi-GAN Vocoder"}),"\n",(0,r.jsx)(n.p,{children:"The vocoder converts mel-spectrograms to raw audio waveforms using the HiFi-GAN architecture."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conv Pre"}),(0,r.jsx)(n.td,{children:"Conv1D (7 kernel)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Upsampling"}),(0,r.jsx)(n.td,{children:"Multiple Conv1DTranspose layers"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Multi-Receptive Field Fusion (MRF)"}),(0,r.jsx)(n.td,{children:"ResBlocks with varying kernel sizes and dilations"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conv Post"}),(0,r.jsx)(n.td,{children:"Conv1D (7 kernel) + Tanh"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Scale Upsampling"}),": Progressive upsampling from mel frame rate to audio sample rate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Receptive Field Fusion"}),": Combines outputs from residual blocks with different receptive fields"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Leaky ReLU Activation"}),": Uses leaky ReLU with slope 0.1 throughout"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ResBlock Types:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ResBlock1"}),": 3 dilated convolutions (dilation: 1, 3, 5) with residual connections"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ResBlock2"}),": 2 dilated convolutions (dilation: 1, 3) with residual connections"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"audio-configuration",children:"Audio Configuration"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sample Rate"}),(0,r.jsx)(n.td,{children:"16,000 Hz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FFT Size"}),(0,r.jsx)(n.td,{children:"1,024"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mel Channels"}),(0,r.jsx)(n.td,{children:"80"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Frequency Range"}),(0,r.jsx)(n.td,{children:"0 - 8,000 Hz"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"text-processing-pipeline",children:"Text Processing Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"1-text-normalization",children:"1. Text Normalization"}),"\n",(0,r.jsx)(n.p,{children:"The input text is normalized before synthesis:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Normalization steps:\n1. Unicode NFKC normalization\n2. Lowercase conversion\n3. Punctuation \u2192 silence markers\n4. Multiple spaces \u2192 single space\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-phoneme-conversion",children:"2. Phoneme Conversion"}),"\n",(0,r.jsx)(n.p,{children:"Text is converted to phonemes using a lexicon lookup:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Vietnamese characters are mapped to phoneme sequences"}),"\n",(0,r.jsxs)(n.li,{children:["Special tokens: ",(0,r.jsx)(n.code,{children:"sil"})," (silence), ",(0,r.jsx)(n.code,{children:"sp"})," (short pause), ",(0,r.jsx)(n.code,{children:" "})," (word boundary)"]}),"\n",(0,r.jsx)(n.li,{children:"Unknown words are processed character-by-character"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-files",children:"Model Files"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"VIET_TTS_V0_4_1"})," model package includes:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"File"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"lexicon.txt"})}),(0,r.jsx)(n.td,{children:"Word-to-phoneme mapping"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"duration_latest_ckpt.pickle"})}),(0,r.jsx)(n.td,{children:"Duration model weights"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"acoustic_latest_ckpt.pickle"})}),(0,r.jsx)(n.td,{children:"Acoustic model weights"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"hk_hifi.pickle"})}),(0,r.jsx)(n.td,{children:"HiFi-GAN vocoder weights"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"config.json"})}),(0,r.jsx)(n.td,{children:"HiFi-GAN configuration"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"framework-dependencies",children:"Framework Dependencies"}),"\n",(0,r.jsx)(n.p,{children:"The voice module uses JAX ecosystem:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Library"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"JAX"}),(0,r.jsx)(n.td,{children:"Numerical computation and automatic differentiation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"JAXlib"}),(0,r.jsx)(n.td,{children:"JAX backend (CPU/GPU/TPU support)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"dm-haiku"}),(0,r.jsx)(n.td,{children:"Neural network library for JAX"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Optax"}),(0,r.jsx)(n.td,{children:"Gradient processing and optimization"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"usage-example",children:"Usage Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from underthesea.pipeline.tts import tts\n\n# Basic usage\ntts("Xin ch\xe0o Vi\u1ec7t Nam")  # Creates sound.wav\n\n# Custom output file\ntts("H\xe0 N\u1ed9i l\xe0 th\u1ee7 \u0111\xf4", outfile="output.wav")\n\n# With playback\ntts("\u0110\xe2y l\xe0 m\u1ed9t v\xed d\u1ee5", play=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"First Call Latency"}),": Model loading on first call may take several seconds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"JAX Compilation"}),": JIT compilation occurs on first inference, subsequent calls are faster"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text Length"}),": Maximum recommended text length is 500 characters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory Usage"}),": GPU memory usage depends on input text length"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Single Speaker"}),": Current model supports only one voice"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vietnamese Only"}),": Designed specifically for Vietnamese language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prosody"}),": Limited control over prosody and emotion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Not optimized for real-time streaming"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/ntt123/vietTTS",children:"VietTTS"})," - Original implementation by NTT123"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2010.05646",children:"HiFi-GAN"})," - Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2010.04301",children:"Non-Attentive Tacotron"})," - Robust and Controllable Neural TTS Synthesis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/deepmind/dm-haiku",children:"dm-haiku"})," - JAX neural network library by DeepMind"]}),"\n"]})]})}function a(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>d});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);