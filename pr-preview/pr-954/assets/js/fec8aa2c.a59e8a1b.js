"use strict";(globalThis.webpackChunkunderthesea_docs=globalThis.webpackChunkunderthesea_docs||[]).push([[6818],{9052(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>o,frontMatter:()=>t,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"technical-reports/dependency-parsing","title":"Dependency Parsing Technical Report","description":"Overview","source":"@site/docs/technical-reports/dependency-parsing.md","sourceDirName":"technical-reports","slug":"/technical-reports/dependency-parsing","permalink":"/docs/technical-reports/dependency-parsing","draft":false,"unlisted":false,"editUrl":"https://github.com/undertheseanlp/underthesea/tree/main/docusaurus/docs/technical-reports/dependency-parsing.md","tags":[],"version":"current","frontMatter":{},"sidebar":"technicalReportsSidebar","previous":{"title":"Sentence Tokenization Technical Report","permalink":"/docs/technical-reports/sent-tokenize"},"next":{"title":"Language Identification Technical Report","permalink":"/docs/technical-reports/language-identification"}}');var d=i(4848),s=i(8453);const t={},l="Dependency Parsing Technical Report",c={},h=[{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Model: Biaffine Dependency Parser",id:"model-biaffine-dependency-parser",level:3},{value:"Components",id:"components",level:3},{value:"Default Hyperparameters",id:"default-hyperparameters",level:3},{value:"File Structure",id:"file-structure",level:2},{value:"Performance",id:"performance",level:2},{value:"Benchmark Results on VLSP2020-DP",id:"benchmark-results-on-vlsp2020-dp",level:3},{value:"Metrics Description",id:"metrics-description",level:3},{value:"Training History",id:"training-history",level:3},{value:"Comparison with Other Methods",id:"comparison-with-other-methods",level:3},{value:"Pretrained Models",id:"pretrained-models",level:2},{value:"Training",id:"training",level:2},{value:"Dataset",id:"dataset",level:3},{value:"Training Script",id:"training-script",level:3},{value:"Training Parameters",id:"training-parameters",level:3},{value:"Usage",id:"usage",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Output Format",id:"output-format",level:3},{value:"Visualization",id:"visualization",level:3},{value:"PyTorch v2 Compatibility (GH-706)",id:"pytorch-v2-compatibility-gh-706",level:2},{value:"Issue",id:"issue",level:3},{value:"Fixes Applied",id:"fixes-applied",level:3},{value:"1. torch.load with weights_only=False",id:"1-torchload-with-weights_onlyfalse",level:4},{value:"2. Replace deprecated apply_permutation",id:"2-replace-deprecated-apply_permutation",level:4},{value:"3. Fix non-tuple sequence indexing",id:"3-fix-non-tuple-sequence-indexing",level:4},{value:"4. Additional Fixes",id:"4-additional-fixes",level:4},{value:"Testing",id:"testing",level:3},{value:"Dependency Relations",id:"dependency-relations",level:2},{value:"References",id:"references",level:2},{value:"Changelog",id:"changelog",level:2},{value:"Version 9.1.3 (PR #871)",id:"version-913-pr-871",level:3}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"dependency-parsing-technical-report",children:"Dependency Parsing Technical Report"})}),"\n",(0,d.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,d.jsx)(n.p,{children:"The dependency parsing module in Underthesea provides Vietnamese dependency parsing using a Biaffine Neural Dependency Parser. This document describes the architecture, training process, and recent updates for PyTorch v2 compatibility."}),"\n",(0,d.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,d.jsx)(n.h3,{id:"model-biaffine-dependency-parser",children:"Model: Biaffine Dependency Parser"}),"\n",(0,d.jsx)(n.p,{children:"The implementation is based on the Deep Biaffine Attention architecture proposed by Dozat and Manning (2017)."}),"\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"Reference:"})," Timothy Dozat and Christopher D. Manning. 2017. ",(0,d.jsx)(n.a,{href:"https://openreview.net/forum?id=Hk95PK9le",children:"Deep Biaffine Attention for Neural Dependency Parsing"})]}),"\n",(0,d.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"DependencyParser\n\u251c\u2500\u2500 Embeddings\n\u2502   \u251c\u2500\u2500 word_embed: nn.Embedding (word embeddings)\n\u2502   \u251c\u2500\u2500 feat_embed: CharLSTM | BertEmbedding | nn.Embedding (feature embeddings)\n\u2502   \u2514\u2500\u2500 embed_dropout: IndependentDropout\n\u251c\u2500\u2500 Encoder\n\u2502   \u251c\u2500\u2500 lstm: BiLSTM (3-layer bidirectional LSTM)\n\u2502   \u2514\u2500\u2500 lstm_dropout: SharedDropout\n\u251c\u2500\u2500 MLP Layers\n\u2502   \u251c\u2500\u2500 mlp_arc_d/h: MLP (arc head/dependent)\n\u2502   \u2514\u2500\u2500 mlp_rel_d/h: MLP (relation head/dependent)\n\u2514\u2500\u2500 Biaffine Attention\n    \u251c\u2500\u2500 arc_attn: Biaffine (arc scoring)\n    \u2514\u2500\u2500 rel_attn: Biaffine (relation scoring)\n"})}),"\n",(0,d.jsx)(n.h3,{id:"default-hyperparameters",children:"Default Hyperparameters"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Parameter"}),(0,d.jsx)(n.th,{children:"Value"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_embed"}),(0,d.jsx)(n.td,{children:"50"}),(0,d.jsx)(n.td,{children:"Word embedding dimension"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_feat_embed"}),(0,d.jsx)(n.td,{children:"100"}),(0,d.jsx)(n.td,{children:"Feature embedding dimension"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_char_embed"}),(0,d.jsx)(n.td,{children:"50"}),(0,d.jsx)(n.td,{children:"Character embedding dimension"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_lstm_hidden"}),(0,d.jsx)(n.td,{children:"400"}),(0,d.jsx)(n.td,{children:"BiLSTM hidden size"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_lstm_layers"}),(0,d.jsx)(n.td,{children:"3"}),(0,d.jsx)(n.td,{children:"Number of BiLSTM layers"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_mlp_arc"}),(0,d.jsx)(n.td,{children:"500"}),(0,d.jsx)(n.td,{children:"Arc MLP output size"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"n_mlp_rel"}),(0,d.jsx)(n.td,{children:"100"}),(0,d.jsx)(n.td,{children:"Relation MLP output size"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"embed_dropout"}),(0,d.jsx)(n.td,{children:"0.33"}),(0,d.jsx)(n.td,{children:"Embedding dropout"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"lstm_dropout"}),(0,d.jsx)(n.td,{children:"0.33"}),(0,d.jsx)(n.td,{children:"LSTM dropout"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"mlp_dropout"}),(0,d.jsx)(n.td,{children:"0.33"}),(0,d.jsx)(n.td,{children:"MLP dropout"})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"file-structure",children:"File Structure"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"underthesea/\n\u251c\u2500\u2500 pipeline/dependency_parse/\n\u2502   \u251c\u2500\u2500 __init__.py          # Main API: dependency_parse()\n\u2502   \u2514\u2500\u2500 visualize.py         # Visualization utilities\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 dependency_parser.py # DependencyParser model\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 base.py              # BiLSTM, MLP, Biaffine, CharLSTM\n\u2502   \u251c\u2500\u2500 bert.py              # BERT embeddings\n\u2502   \u2514\u2500\u2500 embeddings.py        # FieldEmbeddings, CharacterEmbeddings\n\u251c\u2500\u2500 trainers/\n\u2502   \u2514\u2500\u2500 dependency_parser_trainer.py\n\u251c\u2500\u2500 transforms/\n\u2502   \u2514\u2500\u2500 conll.py             # CoNLL format handling\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 sp_field.py          # Field, SubwordField\n    \u251c\u2500\u2500 sp_vocab.py          # Vocabulary\n    \u251c\u2500\u2500 sp_data.py           # Dataset\n    \u251c\u2500\u2500 sp_metric.py         # AttachmentMetric (UAS, LAS)\n    \u2514\u2500\u2500 sp_alg.py            # Eisner, MST algorithms\n"})}),"\n",(0,d.jsx)(n.h2,{id:"performance",children:"Performance"}),"\n",(0,d.jsx)(n.h3,{id:"benchmark-results-on-vlsp2020-dp",children:"Benchmark Results on VLSP2020-DP"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Model"}),(0,d.jsx)(n.th,{children:"UAS"}),(0,d.jsx)(n.th,{children:"LAS"}),(0,d.jsx)(n.th,{children:"UCM"}),(0,d.jsx)(n.th,{children:"LCM"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"MaltParser (baseline)"}),(0,d.jsx)(n.td,{children:"75.41%"}),(0,d.jsx)(n.td,{children:"66.11%"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"-"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Biaffine Attention (v1)"}),(0,d.jsx)(n.td,{children:"87.28%"}),(0,d.jsx)(n.td,{children:"72.63%"}),(0,d.jsx)(n.td,{children:"30.67%"}),(0,d.jsx)(n.td,{children:"6.98%"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.strong,{children:"vi-dp-v1a1 (current)"})}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.strong,{children:"87.10%"})}),(0,d.jsx)(n.td,{children:(0,d.jsx)(n.strong,{children:"80.00%"})}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,d.jsx)(n.h3,{id:"metrics-description",children:"Metrics Description"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Metric"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsxs)(n.td,{children:[(0,d.jsx)(n.strong,{children:"UAS"})," (Unlabeled Attachment Score)"]}),(0,d.jsx)(n.td,{children:"Percentage of tokens with correct head"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsxs)(n.td,{children:[(0,d.jsx)(n.strong,{children:"LAS"})," (Labeled Attachment Score)"]}),(0,d.jsx)(n.td,{children:"Percentage of tokens with correct head AND relation label"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsxs)(n.td,{children:[(0,d.jsx)(n.strong,{children:"UCM"})," (Unlabeled Complete Match)"]}),(0,d.jsx)(n.td,{children:"Percentage of sentences with ALL heads correct"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsxs)(n.td,{children:[(0,d.jsx)(n.strong,{children:"LCM"})," (Labeled Complete Match)"]}),(0,d.jsx)(n.td,{children:"Percentage of sentences with ALL heads and labels correct"})]})]})]}),"\n",(0,d.jsx)(n.h3,{id:"training-history",children:"Training History"}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"MaltParser Baseline (2020)"})}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"Metric     | Precision |    Recall |  F1 Score\n-----------+-----------+-----------+-----------\nUAS        |     75.41 |     75.41 |     75.41\nLAS        |     66.11 |     66.11 |     66.11\nCLAS       |     62.70 |     62.17 |     62.43\n"})}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"Biaffine Attention v1 (240 epochs, 2020)"})}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"2020-11-29 23:05:58 Epoch 240 saved\ndev:   - UCM: 30.67% LCM:  6.98% UAS: 87.28% LAS: 72.63%\ntest:  - UCM: 30.67% LCM:  6.98% UAS: 87.28% LAS: 72.63%\nTraining time: 33m 46s, 5.96s/epoch\n"})}),"\n",(0,d.jsx)(n.h3,{id:"comparison-with-other-methods",children:"Comparison with Other Methods"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Method"}),(0,d.jsx)(n.th,{children:"Year"}),(0,d.jsx)(n.th,{children:"UAS"}),(0,d.jsx)(n.th,{children:"LAS"}),(0,d.jsx)(n.th,{children:"Notes"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"MST Parser (N.L. Minh et al.)"}),(0,d.jsx)(n.td,{children:"2008"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"450 sentences corpus"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"MaltParser (N.T. Luong et al.)"}),(0,d.jsx)(n.td,{children:"2013"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"Vietnamese treebank"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"MaltParser (baseline)"}),(0,d.jsx)(n.td,{children:"2020"}),(0,d.jsx)(n.td,{children:"75.41%"}),(0,d.jsx)(n.td,{children:"66.11%"}),(0,d.jsx)(n.td,{children:"VLSP2020-DP"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Biaffine Attention"}),(0,d.jsx)(n.td,{children:"2020"}),(0,d.jsx)(n.td,{children:"87.28%"}),(0,d.jsx)(n.td,{children:"72.63%"}),(0,d.jsx)(n.td,{children:"VLSP2020-DP"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"NLP@UIT (VLSP2019 winner)"}),(0,d.jsx)(n.td,{children:"2019"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"-"}),(0,d.jsx)(n.td,{children:"Ensemble model"})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"pretrained-models",children:"Pretrained Models"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Model"}),(0,d.jsx)(n.th,{children:"Description"}),(0,d.jsx)(n.th,{children:"UAS"}),(0,d.jsx)(n.th,{children:"LAS"}),(0,d.jsx)(n.th,{children:"Source"})]})}),(0,d.jsx)(n.tbody,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"vi-dp-v1a1"}),(0,d.jsx)(n.td,{children:"Default Vietnamese dependency parser"}),(0,d.jsx)(n.td,{children:"87.10%"}),(0,d.jsx)(n.td,{children:"80.00%"}),(0,d.jsx)(n.td,{children:"Trained on VLSP2020-DP"})]})})]}),"\n",(0,d.jsx)(n.p,{children:"Models are automatically downloaded from the Underthesea resources repository."}),"\n",(0,d.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,d.jsx)(n.h3,{id:"dataset",children:"Dataset"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"VLSP2020-DP"}),": Vietnamese dependency parsing dataset from VLSP 2020 shared task"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"VLSP2020_DP_SAMPLE"}),": Small sample dataset for testing (auto-downloadable)"]}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"training-script",children:"Training Script"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:'from underthesea.datasets.vlsp2020_dp import VLSP2020_DP_SAMPLE\nfrom underthesea.models.dependency_parser import DependencyParser\nfrom underthesea.modules.embeddings import FieldEmbeddings, CharacterEmbeddings\nfrom underthesea.trainers.dependency_parser_trainer import DependencyParserTrainer\n\n# Load corpus\ncorpus = VLSP2020_DP_SAMPLE()\n\n# Initialize parser\nembeddings = [FieldEmbeddings(), CharacterEmbeddings()]\nparser = DependencyParser(embeddings=embeddings, init_pre_train=True)\n\n# Train\ntrainer = DependencyParserTrainer(parser, corpus)\ntrainer.train(\n    base_path="path/to/save/model",\n    max_epochs=100,\n    lr=2e-3,\n    mu=0.9,\n    batch_size=5000\n)\n'})}),"\n",(0,d.jsx)(n.h3,{id:"training-parameters",children:"Training Parameters"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Parameter"}),(0,d.jsx)(n.th,{children:"Default"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"lr"}),(0,d.jsx)(n.td,{children:"2e-3"}),(0,d.jsx)(n.td,{children:"Learning rate"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"mu"}),(0,d.jsx)(n.td,{children:"0.9"}),(0,d.jsx)(n.td,{children:"Adam beta1"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"nu"}),(0,d.jsx)(n.td,{children:"0.9"}),(0,d.jsx)(n.td,{children:"Adam beta2"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"epsilon"}),(0,d.jsx)(n.td,{children:"1e-12"}),(0,d.jsx)(n.td,{children:"Adam epsilon"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"clip"}),(0,d.jsx)(n.td,{children:"5.0"}),(0,d.jsx)(n.td,{children:"Gradient clipping"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"decay"}),(0,d.jsx)(n.td,{children:"0.75"}),(0,d.jsx)(n.td,{children:"LR decay rate"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"decay_steps"}),(0,d.jsx)(n.td,{children:"5000"}),(0,d.jsx)(n.td,{children:"LR decay steps"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"patience"}),(0,d.jsx)(n.td,{children:"100"}),(0,d.jsx)(n.td,{children:"Early stopping patience"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"batch_size"}),(0,d.jsx)(n.td,{children:"5000"}),(0,d.jsx)(n.td,{children:"Batch size (tokens)"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"buckets"}),(0,d.jsx)(n.td,{children:"1000"}),(0,d.jsx)(n.td,{children:"Number of length buckets"})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,d.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:"from underthesea import dependency_parse\n\nresult = dependency_parse(\"T\xf4i l\xe0 sinh vi\xean Vi\u1ec7t Nam\")\n# Output: [('T\xf4i', 3, 'nsubj'), ('l\xe0', 3, 'cop'), ('sinh vi\xean', 0, 'root'), ('Vi\u1ec7t Nam', 3, 'compound')]\n"})}),"\n",(0,d.jsx)(n.h3,{id:"output-format",children:"Output Format"}),"\n",(0,d.jsx)(n.p,{children:"Each tuple contains:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"word"}),": The token"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"head"}),": Index of the head token (0 = root)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"relation"}),": Dependency relation label"]}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"visualization",children:"Visualization"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:'from underthesea.pipeline.dependency_parse import render, display\n\n# Render as SVG\nsvg = render("T\xf4i y\xeau Vi\u1ec7t Nam")\n\n# Display in notebook\ndisplay("T\xf4i y\xeau Vi\u1ec7t Nam")\n'})}),"\n",(0,d.jsx)(n.h2,{id:"pytorch-v2-compatibility-gh-706",children:"PyTorch v2 Compatibility (GH-706)"}),"\n",(0,d.jsx)(n.h3,{id:"issue",children:"Issue"}),"\n",(0,d.jsx)(n.p,{children:"The dependency parsing module was incompatible with PyTorch v2.0+ due to:"}),"\n",(0,d.jsxs)(n.ol,{children:["\n",(0,d.jsxs)(n.li,{children:["Deprecated ",(0,d.jsx)(n.code,{children:"weights_only"})," parameter in ",(0,d.jsx)(n.code,{children:"torch.load"})]}),"\n",(0,d.jsxs)(n.li,{children:["Deprecated ",(0,d.jsx)(n.code,{children:"apply_permutation"})," function"]}),"\n",(0,d.jsx)(n.li,{children:"Deprecated non-tuple sequence indexing"}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"fixes-applied",children:"Fixes Applied"}),"\n",(0,d.jsx)(n.h4,{id:"1-torchload-with-weights_onlyfalse",children:"1. torch.load with weights_only=False"}),"\n",(0,d.jsxs)(n.p,{children:["PyTorch 2.0+ requires explicit ",(0,d.jsx)(n.code,{children:"weights_only"})," parameter. Since models contain pickled Python objects (transforms, vocabularies), we use ",(0,d.jsx)(n.code,{children:"weights_only=False"}),":"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:"# Before\nstate = torch.load(path)\n\n# After\nstate = torch.load(path, map_location='cpu', weights_only=False)\n"})}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"Files modified:"})}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.code,{children:"underthesea/models/dependency_parser.py"})}),"\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.code,{children:"underthesea/models/model.py"})}),"\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.code,{children:"underthesea/modules/nn.py"})}),"\n"]}),"\n",(0,d.jsx)(n.h4,{id:"2-replace-deprecated-apply_permutation",children:"2. Replace deprecated apply_permutation"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:"# Before\nfrom torch.nn.modules.rnn import apply_permutation\nh = apply_permutation(hx[0], permutation)\n\n# After\nh = hx[0].index_select(0, permutation)\n"})}),"\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"File modified:"})," ",(0,d.jsx)(n.code,{children:"underthesea/modules/base.py"})]}),"\n",(0,d.jsx)(n.h4,{id:"3-fix-non-tuple-sequence-indexing",children:"3. Fix non-tuple sequence indexing"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:"# Before\nout_tensor[i][[slice(0, i) for i in tensor.size()]] = tensor\n\n# After\nout_tensor[i][tuple(slice(0, s) for s in tensor.size())] = tensor\n"})}),"\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"File modified:"})," ",(0,d.jsx)(n.code,{children:"underthesea/utils/sp_fn.py"})]}),"\n",(0,d.jsx)(n.h4,{id:"4-additional-fixes",children:"4. Additional Fixes"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Added ",(0,d.jsx)(n.code,{children:"self.bert"})," attribute storage in ",(0,d.jsx)(n.code,{children:"DependencyParser.__init__"})," for training compatibility"]}),"\n",(0,d.jsxs)(n.li,{children:["Fixed ",(0,d.jsx)(n.code,{children:"n_rels"})," parameter in ",(0,d.jsx)(n.code,{children:"_init_model_with_state_dict"})," (was incorrectly using ",(0,d.jsx)(n.code,{children:"n_feats"}),")"]}),"\n",(0,d.jsxs)(n.li,{children:["Fixed device import in ",(0,d.jsx)(n.code,{children:"DependencyParserTrainer"})]}),"\n",(0,d.jsx)(n.li,{children:"Added first-epoch model saving to ensure training always produces a model"}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"testing",children:"Testing"}),"\n",(0,d.jsx)(n.p,{children:"CI test added to verify training works with PyTorch v2:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-bash",children:"# Run training test\ntox -e train-dep\n\n# Or directly\npython -m unittest tests.pipeline.dependency_parse.test_train\n"})}),"\n",(0,d.jsx)(n.h2,{id:"dependency-relations",children:"Dependency Relations"}),"\n",(0,d.jsx)(n.p,{children:"Common Vietnamese dependency relations:"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Relation"}),(0,d.jsx)(n.th,{children:"Description"}),(0,d.jsx)(n.th,{children:"Example"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"root"}),(0,d.jsx)(n.td,{children:"Root of sentence"}),(0,d.jsx)(n.td,{children:"Main verb"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"nsubj"}),(0,d.jsx)(n.td,{children:"Nominal subject"}),(0,d.jsx)(n.td,{children:"T\xf4i (I) \u2192 \u0103n (eat)"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"obj"}),(0,d.jsx)(n.td,{children:"Direct object"}),(0,d.jsx)(n.td,{children:"c\u01a1m (rice) \u2190 \u0103n (eat)"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"cop"}),(0,d.jsx)(n.td,{children:"Copula"}),(0,d.jsx)(n.td,{children:"l\xe0 (is) \u2192 noun"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"compound"}),(0,d.jsx)(n.td,{children:"Compound"}),(0,d.jsx)(n.td,{children:"Vi\u1ec7t Nam \u2190 sinh vi\xean"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"nmod"}),(0,d.jsx)(n.td,{children:"Nominal modifier"}),(0,d.jsx)(n.td,{children:"c\u1ee7a (of) relations"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"amod"}),(0,d.jsx)(n.td,{children:"Adjectival modifier"}),(0,d.jsx)(n.td,{children:"\u0111\u1eb9p (beautiful) \u2192 noun"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"advmod"}),(0,d.jsx)(n.td,{children:"Adverbial modifier"}),(0,d.jsx)(n.td,{children:"r\u1ea5t (very) \u2192 adj"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"punct"}),(0,d.jsx)(n.td,{children:"Punctuation"}),(0,d.jsx)(n.td,{children:". , ! ?"})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,d.jsxs)(n.ol,{children:["\n",(0,d.jsx)(n.li,{children:"Dozat, T., & Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. ICLR 2017."}),"\n",(0,d.jsx)(n.li,{children:"VLSP 2020 Shared Task: Vietnamese Dependency Parsing"}),"\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.a,{href:"https://github.com/undertheseanlp/underthesea",children:"Underthesea GitHub Repository"})}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"changelog",children:"Changelog"}),"\n",(0,d.jsx)(n.h3,{id:"version-913-pr-871",children:"Version 9.1.3 (PR #871)"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:"Added PyTorch v2.0+ support"}),"\n",(0,d.jsx)(n.li,{children:"Fixed deprecated API usage"}),"\n",(0,d.jsx)(n.li,{children:"Added training CI test (train-dep)"}),"\n",(0,d.jsx)(n.li,{children:"Re-enabled dependency_parse tests in CI"}),"\n"]})]})}function o(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(a,{...e})}):a(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>l});var r=i(6540);const d={},s=r.createContext(d);function t(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);